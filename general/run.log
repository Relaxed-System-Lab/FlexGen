2023-10-26 10:10:55,886 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpuu89spmq
2023-10-26 10:10:55,887 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpuu89spmq/_remote_module_non_scriptable.py
2023-10-26 10:10:56,280 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-26 10:10:56,363 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-26 10:10:57,741 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-26 10:10:58,021 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-26 10:10:58,021 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-26 10:10:58,021 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-26 10:10:58,021 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-26 10:10:58,770 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-26 10:10:58,848 [model.py:111 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-26 10:10:58,848 [model.py:60 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-125m
2023-10-26 10:10:58,888 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-26 10:10:58,965 [model.py:68 in __init__] INFO - tied_params: [['lm_head.weight', 'model.decoder.embed_tokens.weight']]
2023-10-26 10:10:58,968 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-26 10:10:58,969 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-26 10:10:58,969 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-26 10:10:58,970 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-26 10:10:58,971 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-26 10:10:58,972 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-26 10:10:58,973 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-26 10:10:58,974 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-26 10:10:58,975 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-26 10:10:58,976 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-26 10:10:58,977 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-26 10:10:58,978 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-26 10:10:58,979 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-26 10:10:58,980 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-26 10:10:58,981 [model.py:211 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-26 10:10:58,981 [model.py:211 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-26 10:10:58,981 [model.py:215 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-26 10:10:58,984 [model.py:221 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-26 10:10:59,021 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-26 10:10:59,163 [model.py:298 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-26 10:10:59,163 [model.py:298 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-26 10:10:59,163 [model.py:298 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-26 10:10:59,164 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-26 10:10:59,164 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-26 10:10:59,164 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-26 10:10:59,164 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-26 10:10:59,164 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-26 10:10:59,164 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-26 10:10:59,164 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-26 10:10:59,164 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-26 10:10:59,164 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-26 10:10:59,165 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-26 10:10:59,165 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-26 10:10:59,165 [model.py:298 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-26 10:10:59,165 [model.py:298 in to_test_forward] DEBUG - lm_head to test forward
2023-10-26 10:10:59,168 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:10:59,254 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_tokens to meta


2023-10-26 10:10:59,256 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:10:59,259 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_positions to meta


2023-10-26 10:10:59,259 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:10:59,288 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.0 to meta


2023-10-26 10:10:59,290 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:10:59,312 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.1 to meta


2023-10-26 10:10:59,313 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:10:59,333 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.2 to meta


2023-10-26 10:10:59,334 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:10:59,352 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.3 to meta


2023-10-26 10:10:59,354 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:10:59,371 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.4 to meta


2023-10-26 10:10:59,373 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:10:59,390 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.5 to meta


2023-10-26 10:10:59,392 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:10:59,410 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.6 to meta


2023-10-26 10:10:59,411 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:10:59,431 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.7 to meta


2023-10-26 10:10:59,432 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:10:59,450 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.8 to meta


2023-10-26 10:10:59,451 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:10:59,469 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.9 to meta


2023-10-26 10:10:59,470 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:10:59,488 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.10 to meta


2023-10-26 10:10:59,489 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:10:59,507 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.11 to meta


2023-10-26 10:10:59,508 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:10:59,509 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.final_layer_norm to meta


2023-10-26 10:10:59,509 [model.py:264 in layer_cpu_load] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:10:59,600 [model.py:274 in layer_cpu_offload] DEBUG - offload_layer_weights: lm_head to meta


2023-10-26 10:10:59,602 [forward.py:28 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-26 10:10:59,603 [forward.py:28 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-26 10:10:59,603 [forward.py:28 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-26 10:10:59,603 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-26 10:10:59,603 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-26 10:10:59,603 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-26 10:10:59,603 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-26 10:10:59,603 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-26 10:10:59,603 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-26 10:10:59,604 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-26 10:10:59,604 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-26 10:10:59,604 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-26 10:10:59,604 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-26 10:10:59,604 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-26 10:10:59,604 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-26 10:10:59,604 [forward.py:28 in reset_forward] DEBUG - lm_head from test to old.
2023-10-26 10:10:59,613 [model.py:391 in init_all_weights] DEBUG - init all weights...
2023-10-26 10:10:59,747 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-26 10:10:59,747 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-26 10:10:59,748 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-26 10:10:59,748 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-26 10:10:59,748 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-26 10:10:59,748 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-26 10:10:59,748 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-26 10:10:59,748 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-26 10:10:59,748 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-26 10:10:59,748 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-26 10:10:59,748 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-26 10:10:59,749 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-26 10:10:59,749 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-26 10:10:59,749 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-26 10:10:59,749 [forward.py:95 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-26 10:10:59,749 [forward.py:95 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-26 10:10:59,790 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-26 10:10:59,927 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:00,016 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:00,021 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 9])",)
2023-10-26 10:11:00,021 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:00,022 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-26 10:11:00,023 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-26 10:11:00,024 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-26 10:11:00,024 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-26 10:11:00,025 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-26 10:11:00,025 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:00,026 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:00,031 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:00,051 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 9])", "<class 'int'>: 0")
2023-10-26 10:11:00,051 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:00,052 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-26 10:11:00,053 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-26 10:11:00,053 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-26 10:11:00,054 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-26 10:11:00,054 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-26 10:11:00,054 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:00,059 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:00,068 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:00,081 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-26 10:11:00,082 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:00,087 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,091 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,098 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,101 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,101 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-26 10:11:00,101 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:00,102 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:00,113 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:00,130 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-26 10:11:00,131 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:00,137 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,141 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,144 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,147 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,148 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-26 10:11:00,148 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:00,149 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:00,159 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:00,177 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-26 10:11:00,177 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:00,182 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,186 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,189 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,192 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,192 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-26 10:11:00,192 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:00,193 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:00,204 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:00,221 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-26 10:11:00,221 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:00,226 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,230 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,233 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,237 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,237 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-26 10:11:00,237 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:00,238 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:00,249 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:00,263 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-26 10:11:00,263 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:00,271 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,275 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,279 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,282 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,282 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-26 10:11:00,283 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:00,283 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:00,298 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:00,311 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-26 10:11:00,311 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:00,315 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,319 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,322 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,325 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,325 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-26 10:11:00,325 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:00,326 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:00,336 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:00,345 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-26 10:11:00,346 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:00,350 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,353 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,356 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,360 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,360 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-26 10:11:00,360 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:00,361 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:00,375 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:00,388 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-26 10:11:00,388 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:00,392 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,396 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,399 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,402 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,402 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-26 10:11:00,402 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:00,403 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:00,413 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:00,424 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-26 10:11:00,424 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:00,429 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,433 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,441 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,445 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,445 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-26 10:11:00,445 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:00,446 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:00,457 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:00,468 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-26 10:11:00,468 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:00,479 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,483 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,487 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,491 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,492 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-26 10:11:00,492 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:00,493 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:00,504 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:00,515 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-26 10:11:00,515 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:00,520 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,525 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,529 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,533 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,533 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-26 10:11:00,534 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:00,535 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:00,546 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:00,547 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-26 10:11:00,547 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:00,573 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,579 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,583 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,586 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 9, 64])"))
2023-10-26 10:11:00,587 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"))
2023-10-26 10:11:00,587 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:00,588 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:00,588 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:00,676 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-26 10:11:00,676 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:00,677 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-26 10:11:00,678 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-26 10:11:00,679 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-26 10:11:00,680 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 768])
2023-10-26 10:11:00,680 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])
2023-10-26 10:11:00,680 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:00,680 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:00,767 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:00,851 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 9, 768])",)
2023-10-26 10:11:00,851 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:00,864 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-26 10:11:00,877 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-26 10:11:00,890 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-26 10:11:00,904 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 9, 50272])
2023-10-26 10:11:00,907 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 9, 50272])
2023-10-26 10:11:00,907 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:00,909 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:00,994 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:00,996 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:00,996 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:00,997 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:00,997 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:00,998 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:00,999 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:00,999 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:00,999 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:01,000 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:01,002 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:01,014 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 10])", "<class 'int'>: 9")
2023-10-26 10:11:01,014 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:01,015 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,015 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,016 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,017 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,017 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:01,017 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:01,020 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:01,033 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:01,046 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,046 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:01,051 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,055 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,058 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,062 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,062 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-26 10:11:01,062 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:01,063 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:01,073 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:01,089 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,089 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:01,094 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,098 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,101 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,105 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,105 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-26 10:11:01,105 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:01,106 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:01,115 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:01,125 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,125 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:01,130 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,133 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,137 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,140 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,140 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-26 10:11:01,140 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:01,141 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:01,151 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:01,168 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,168 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:01,172 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,177 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,180 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,184 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,184 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-26 10:11:01,184 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:01,185 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:01,196 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:01,211 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,211 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:01,216 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,220 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,223 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,226 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,226 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-26 10:11:01,226 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:01,227 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:01,237 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:01,246 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,246 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:01,251 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,255 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,258 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,261 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,261 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-26 10:11:01,262 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:01,262 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:01,273 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:01,282 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,283 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:01,287 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,292 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,301 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,305 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,305 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-26 10:11:01,305 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:01,306 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:01,316 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:01,326 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,327 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:01,331 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,335 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,338 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,342 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,342 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-26 10:11:01,342 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:01,343 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:01,354 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:01,365 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,365 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:01,369 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,373 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,376 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,380 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,380 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-26 10:11:01,380 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:01,381 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:01,392 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:01,401 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,401 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:01,412 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,415 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,419 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,422 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,422 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-26 10:11:01,422 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:01,423 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:01,432 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:01,441 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,441 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:01,446 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,449 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,452 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,455 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,456 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-26 10:11:01,456 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:01,457 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:01,466 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:01,467 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,467 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:01,495 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,525 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,555 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,558 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 10, 64])"))
2023-10-26 10:11:01,559 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"))
2023-10-26 10:11:01,559 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:01,560 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:01,561 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:01,647 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,647 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:01,648 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,649 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,650 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,651 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,651 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:01,651 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:01,651 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:01,738 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:01,824 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,824 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:01,831 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:01,839 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:01,847 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:01,854 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:01,856 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:01,856 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:01,858 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:01,943 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:01,945 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:01,945 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:01,946 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,947 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,947 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,948 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,948 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:01,948 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:01,949 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:01,951 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:01,961 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 11])", "<class 'int'>: 10")
2023-10-26 10:11:01,961 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:01,961 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,962 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,963 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,964 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:01,964 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:01,964 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:01,967 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:01,975 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:01,984 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:01,985 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:01,989 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:01,992 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:01,995 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:01,998 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:01,999 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-26 10:11:01,999 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:02,000 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:02,009 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:02,019 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,019 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,023 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,027 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,030 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,033 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,033 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-26 10:11:02,033 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:02,034 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:02,043 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:02,053 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,053 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,057 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,061 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,064 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,067 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,068 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-26 10:11:02,068 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:02,068 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:02,078 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:02,087 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,088 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,092 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,095 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,098 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,102 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,102 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-26 10:11:02,102 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:02,103 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:02,112 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:02,125 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,126 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,130 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,134 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,137 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,140 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,140 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-26 10:11:02,141 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:02,141 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:02,151 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:02,160 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,160 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,165 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,168 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,171 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,174 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,175 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-26 10:11:02,175 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:02,176 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:02,186 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:02,195 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,195 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,199 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,203 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,206 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,209 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,210 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-26 10:11:02,210 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:02,211 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:02,220 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:02,230 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,230 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,234 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,239 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,242 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,245 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,246 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-26 10:11:02,246 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:02,247 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:02,256 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:02,265 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,265 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,270 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,273 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,277 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,280 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,280 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-26 10:11:02,280 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:02,281 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:02,291 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:02,300 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,301 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,305 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,309 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,314 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,317 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,317 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-26 10:11:02,317 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:02,318 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:02,328 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:02,338 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,338 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,342 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,346 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,349 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,353 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,353 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-26 10:11:02,354 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:02,354 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:02,364 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:02,365 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,365 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,369 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,373 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,376 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,380 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 11, 64])"))
2023-10-26 10:11:02,380 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"))
2023-10-26 10:11:02,380 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:02,381 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:02,382 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:02,465 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,465 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:02,466 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:02,467 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:02,468 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:02,468 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:02,468 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:02,468 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:02,469 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:02,554 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:02,636 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,637 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:02,644 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:02,654 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:02,661 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:02,668 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:02,670 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:02,670 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:02,672 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:02,759 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:02,761 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:02,761 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:02,762 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:02,762 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:02,763 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:02,763 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:02,763 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:02,764 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:02,765 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:02,769 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:02,786 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 12])", "<class 'int'>: 11")
2023-10-26 10:11:02,786 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:02,787 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:02,788 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:02,788 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:02,789 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:02,789 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:02,789 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:02,792 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:02,804 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:02,818 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,818 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,825 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,831 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,834 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,837 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,837 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-26 10:11:02,838 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:02,838 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:02,848 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:02,863 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,864 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,868 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,871 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,874 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,878 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,878 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-26 10:11:02,878 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:02,879 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:02,889 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:02,905 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,906 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,910 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,914 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,918 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,921 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,921 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-26 10:11:02,921 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:02,922 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:02,932 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:02,949 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,949 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,953 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,957 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,960 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,963 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,963 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-26 10:11:02,963 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:02,964 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:02,974 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:02,987 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:02,987 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:02,991 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,995 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:02,998 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,001 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,001 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-26 10:11:03,001 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:03,002 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:03,015 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:03,028 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,028 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,034 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,038 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,041 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,044 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,044 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-26 10:11:03,044 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:03,045 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:03,055 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:03,065 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,065 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,069 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,073 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,076 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,080 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,080 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-26 10:11:03,080 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:03,081 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:03,091 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:03,100 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,100 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,105 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,108 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,111 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,114 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,115 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-26 10:11:03,115 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:03,116 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:03,125 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:03,134 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,134 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,139 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,142 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,145 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,149 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,149 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-26 10:11:03,149 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:03,150 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:03,160 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:03,170 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,170 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,175 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,179 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,182 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,186 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,186 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-26 10:11:03,186 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:03,187 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:03,197 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:03,206 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,207 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,211 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,214 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,217 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,221 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,221 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-26 10:11:03,221 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:03,222 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:03,231 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:03,232 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,232 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,236 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,240 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,243 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,246 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 12, 64])"))
2023-10-26 10:11:03,246 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"))
2023-10-26 10:11:03,246 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:03,247 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:03,248 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:03,331 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,331 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:03,332 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:03,333 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:03,334 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:03,335 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:03,335 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:03,335 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:03,335 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:03,420 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:03,503 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,503 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:03,511 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:03,518 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:03,526 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:03,533 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:03,535 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:03,535 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:03,537 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:03,621 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:03,623 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:03,623 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:03,624 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:03,625 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:03,625 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:03,626 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:03,626 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:03,626 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:03,627 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:03,629 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:03,638 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 13])", "<class 'int'>: 12")
2023-10-26 10:11:03,638 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:03,639 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:03,639 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:03,640 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:03,641 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:03,641 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:03,641 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:03,644 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:03,655 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:03,668 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,669 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,673 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,676 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,680 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,683 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,683 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-26 10:11:03,683 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:03,684 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:03,693 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:03,710 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,710 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,714 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,719 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,722 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,731 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,731 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-26 10:11:03,731 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:03,732 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:03,741 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:03,751 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,751 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,755 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,759 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,762 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,765 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,765 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-26 10:11:03,765 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:03,766 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:03,776 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:03,791 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,792 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,796 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,800 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,822 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,837 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,837 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-26 10:11:03,838 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:03,838 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:03,849 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:03,861 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,861 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,865 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,869 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,872 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,875 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,875 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-26 10:11:03,876 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:03,876 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:03,886 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:03,895 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,895 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,900 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,903 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,906 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,909 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,910 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-26 10:11:03,910 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:03,911 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:03,920 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:03,929 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,930 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,934 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,937 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,940 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,944 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,944 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-26 10:11:03,944 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:03,945 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:03,954 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:03,963 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,964 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:03,968 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,972 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,975 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,978 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:03,978 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-26 10:11:03,978 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:03,979 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:03,989 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:03,998 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:03,998 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,002 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,006 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,010 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,013 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,013 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-26 10:11:04,013 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:04,014 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:04,024 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:04,034 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,034 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,038 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,044 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,049 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,053 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,053 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-26 10:11:04,053 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:04,054 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:04,063 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:04,072 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,073 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,077 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,081 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,084 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,087 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,087 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-26 10:11:04,087 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:04,088 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:04,098 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:04,099 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,099 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,103 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,107 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,110 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,113 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 13, 64])"))
2023-10-26 10:11:04,114 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"))
2023-10-26 10:11:04,114 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:04,115 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:04,115 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:04,208 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,209 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:04,210 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:04,210 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:04,211 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:04,212 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:04,212 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:04,212 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:04,213 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:04,303 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:04,385 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,386 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:04,393 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:04,403 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:04,413 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:04,421 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:04,423 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:04,423 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:04,425 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:04,509 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:04,511 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:04,511 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:04,512 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:04,512 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:04,513 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:04,514 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:04,514 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:04,514 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:04,515 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:04,516 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:04,526 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 14])", "<class 'int'>: 13")
2023-10-26 10:11:04,526 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:04,527 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:04,527 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:04,528 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:04,529 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:04,529 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:04,529 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:04,532 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:04,540 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:04,550 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,550 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,555 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,558 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,562 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,565 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,565 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-26 10:11:04,565 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:04,566 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:04,576 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:04,585 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,586 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,590 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,594 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,597 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,600 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,600 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-26 10:11:04,601 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:04,601 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:04,611 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:04,621 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,621 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,625 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,633 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,637 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,640 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,640 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-26 10:11:04,640 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:04,641 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:04,651 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:04,660 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,661 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,665 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,668 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,672 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,675 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,675 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-26 10:11:04,675 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:04,676 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:04,686 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:04,699 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,699 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,704 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,707 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,710 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,714 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,714 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-26 10:11:04,714 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:04,715 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:04,724 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:04,734 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,734 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,744 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,747 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,750 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,753 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,754 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-26 10:11:04,754 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:04,754 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:04,764 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:04,773 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,773 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,778 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,781 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,784 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,787 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,788 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-26 10:11:04,788 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:04,789 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:04,802 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:04,815 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,815 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,819 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,823 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,826 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,829 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,830 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-26 10:11:04,830 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:04,831 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:04,840 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:04,849 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,850 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,857 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,861 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,864 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,867 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,867 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-26 10:11:04,868 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:04,868 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:04,881 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:04,894 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,895 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,899 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,908 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,911 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,915 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,915 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-26 10:11:04,915 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:04,916 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:04,925 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:04,935 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,935 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,939 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,943 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,946 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,949 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,949 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-26 10:11:04,949 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:04,950 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:04,960 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:04,961 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:04,961 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:04,965 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,968 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,972 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,975 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 14, 64])"))
2023-10-26 10:11:04,975 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"))
2023-10-26 10:11:04,975 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:04,976 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:04,977 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:05,061 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,061 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:05,062 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,063 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,064 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,065 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,065 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:05,065 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:05,065 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:05,150 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:05,233 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,233 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:05,241 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:05,248 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:05,255 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:05,263 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:05,264 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:05,265 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:05,267 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:05,352 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:05,354 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:05,354 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:05,355 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,355 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,356 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,357 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,357 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:05,357 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:05,358 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:05,359 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:05,369 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 15])", "<class 'int'>: 14")
2023-10-26 10:11:05,369 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:05,370 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,371 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,371 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,372 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,372 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:05,372 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:05,375 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:05,387 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:05,401 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,401 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:05,406 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,409 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,413 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,416 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,416 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-26 10:11:05,417 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:05,417 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:05,427 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:05,437 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,437 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:05,441 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,445 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,449 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,452 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,452 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-26 10:11:05,452 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:05,453 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:05,462 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:05,472 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,473 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:05,477 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,480 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,484 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,487 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,487 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-26 10:11:05,487 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:05,488 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:05,499 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:05,509 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,509 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:05,514 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,517 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,521 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,524 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,524 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-26 10:11:05,524 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:05,525 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:05,535 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:05,545 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,545 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:05,549 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,553 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,556 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,560 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,560 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-26 10:11:05,560 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:05,562 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:05,575 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:05,588 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,589 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:05,593 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,597 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,600 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,604 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,604 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-26 10:11:05,604 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:05,605 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:05,615 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:05,625 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,625 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:05,629 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,633 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,636 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,640 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,640 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-26 10:11:05,640 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:05,641 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:05,651 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:05,661 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,661 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:05,665 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,669 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,672 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,677 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,677 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-26 10:11:05,677 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:05,678 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:05,688 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:05,698 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,698 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:05,702 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,706 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,709 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,713 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,713 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-26 10:11:05,714 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:05,714 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:05,725 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:05,735 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,735 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:05,739 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,743 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,747 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,750 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,750 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-26 10:11:05,750 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:05,751 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:05,761 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:05,771 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,771 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:05,775 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,779 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,782 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,786 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,786 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-26 10:11:05,786 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:05,787 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:05,797 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:05,798 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,798 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:05,802 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,806 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,809 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,812 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 15, 64])"))
2023-10-26 10:11:05,812 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"))
2023-10-26 10:11:05,812 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:05,813 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:05,814 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:05,898 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:05,898 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:05,899 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,900 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,901 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,901 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:05,901 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:05,901 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:05,902 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:05,987 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:06,071 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,071 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:06,078 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:06,085 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:06,092 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:06,099 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:06,101 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:06,101 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:06,104 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:06,190 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:06,192 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:06,192 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:06,193 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:06,193 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:06,194 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:06,195 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:06,195 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:06,195 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:06,196 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:06,198 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:06,207 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 16])", "<class 'int'>: 15")
2023-10-26 10:11:06,208 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:06,208 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:06,209 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:06,209 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:06,210 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:06,210 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:06,210 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:06,213 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:06,221 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:06,234 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,235 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:06,239 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,242 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,246 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,249 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,249 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-26 10:11:06,249 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:06,250 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:06,260 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:06,270 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,270 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:06,275 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,278 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,281 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,285 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,285 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-26 10:11:06,285 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:06,286 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:06,295 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:06,305 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,306 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:06,310 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,317 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,320 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,323 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,324 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-26 10:11:06,324 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:06,325 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:06,335 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:06,345 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,345 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:06,349 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,353 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,356 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,360 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,360 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-26 10:11:06,360 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:06,361 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:06,371 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:06,380 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,381 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:06,385 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,391 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,395 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,398 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,398 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-26 10:11:06,398 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:06,399 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:06,409 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:06,422 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,422 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:06,427 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,431 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,434 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,437 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,437 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-26 10:11:06,437 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:06,438 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:06,448 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:06,457 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,457 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:06,463 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,466 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,470 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,473 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,473 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-26 10:11:06,473 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:06,474 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:06,484 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:06,494 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,494 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:06,498 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,502 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,505 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,508 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,508 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-26 10:11:06,509 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:06,509 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:06,519 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:06,529 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,529 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:06,553 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,560 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,564 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,569 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,570 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-26 10:11:06,570 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:06,571 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:06,581 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:06,590 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,591 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:06,595 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,604 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,613 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,637 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,637 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-26 10:11:06,637 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:06,638 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:06,649 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:06,660 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,660 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:06,665 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,669 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,673 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,677 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,677 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-26 10:11:06,678 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:06,678 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:06,689 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:06,689 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,690 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:06,694 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,698 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,701 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,705 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 16, 64])"))
2023-10-26 10:11:06,705 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"))
2023-10-26 10:11:06,705 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:06,706 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:06,706 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:06,792 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,793 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:06,794 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:06,794 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:06,795 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:06,796 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:06,796 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:06,796 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:06,797 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:06,884 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:06,969 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:06,970 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:06,981 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:06,992 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:07,001 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:07,010 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:07,012 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:07,012 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:07,016 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:07,101 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:07,104 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:07,104 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:07,104 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:07,105 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:07,106 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:07,106 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:07,107 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:07,107 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:07,108 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:07,109 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:07,119 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 17])", "<class 'int'>: 16")
2023-10-26 10:11:07,119 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:07,120 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:07,121 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:07,122 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:07,122 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:07,123 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:07,123 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:07,126 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:07,135 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:07,148 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:07,149 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:07,154 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,158 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,162 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,166 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,167 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-26 10:11:07,167 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:07,168 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:07,178 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:07,188 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:07,188 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:07,193 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,210 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,214 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,218 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,219 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-26 10:11:07,219 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:07,220 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:07,229 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:07,239 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:07,240 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:07,245 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,249 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,253 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,257 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,257 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-26 10:11:07,258 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:07,259 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:07,269 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:07,279 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:07,279 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:07,284 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,289 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,293 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,297 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,297 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-26 10:11:07,297 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:07,298 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:07,307 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:07,318 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:07,318 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:07,323 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,328 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,333 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,348 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,348 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-26 10:11:07,348 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:07,349 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:07,359 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:07,373 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:07,373 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:07,378 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,382 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,387 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,391 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,391 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-26 10:11:07,391 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:07,392 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:07,402 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:07,412 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:07,412 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:07,417 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,422 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,426 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,430 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,430 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-26 10:11:07,430 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:07,431 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:07,441 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:07,458 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:07,458 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:07,463 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,466 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,470 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,474 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,474 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-26 10:11:07,474 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:07,475 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:07,485 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:07,494 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:07,495 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:07,499 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,503 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,506 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,510 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,510 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-26 10:11:07,510 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:07,511 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:07,521 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:07,530 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:07,530 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:07,535 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,539 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,542 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,546 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,546 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-26 10:11:07,546 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:07,547 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:07,557 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:07,566 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:07,567 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:07,571 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,575 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,578 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,581 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,582 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-26 10:11:07,582 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:07,583 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:07,592 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:07,593 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:07,593 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:07,598 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,601 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,605 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,608 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 17, 64])"))
2023-10-26 10:11:07,608 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"))
2023-10-26 10:11:07,609 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:07,609 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:07,610 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:07,695 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:07,695 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:07,696 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:07,697 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:07,698 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:07,699 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:07,699 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:07,699 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:07,699 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:07,786 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:07,870 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:07,871 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:07,878 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:07,889 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:07,903 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:07,917 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:07,922 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:07,922 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:07,925 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:08,012 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:08,014 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:08,014 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:08,015 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,016 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,016 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,017 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,017 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:08,017 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:08,018 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:08,020 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:08,030 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 18])", "<class 'int'>: 17")
2023-10-26 10:11:08,030 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:08,031 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,032 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,032 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,033 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,033 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:08,033 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:08,036 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:08,046 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:08,059 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,060 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:08,064 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,068 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,071 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,075 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,075 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-26 10:11:08,075 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:08,076 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:08,086 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:08,096 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,096 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:08,101 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,105 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,108 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,111 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,111 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-26 10:11:08,111 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:08,112 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:08,122 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:08,132 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,132 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:08,136 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,140 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,143 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,148 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,148 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-26 10:11:08,148 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:08,149 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:08,159 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:08,174 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,174 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:08,189 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,195 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,198 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,202 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,202 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-26 10:11:08,202 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:08,203 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:08,213 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:08,223 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,223 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:08,227 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,231 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,234 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,237 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,237 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-26 10:11:08,237 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:08,239 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:08,248 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:08,261 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,261 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:08,270 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,273 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,277 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,280 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,280 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-26 10:11:08,280 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:08,281 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:08,291 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:08,301 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,301 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:08,305 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,309 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,313 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,319 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,319 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-26 10:11:08,319 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:08,320 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:08,329 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:08,339 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,339 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:08,344 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,347 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,351 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,354 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,354 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-26 10:11:08,354 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:08,355 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:08,365 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:08,374 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,375 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:08,379 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,383 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,386 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,390 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,390 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-26 10:11:08,390 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:08,391 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:08,402 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:08,419 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,420 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:08,424 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,428 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,432 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,435 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,435 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-26 10:11:08,436 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:08,436 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:08,446 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:08,456 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,456 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:08,461 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,465 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,468 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,472 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,472 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-26 10:11:08,472 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:08,473 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:08,482 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:08,483 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,483 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:08,488 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,491 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,495 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,498 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 18, 64])"))
2023-10-26 10:11:08,498 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"))
2023-10-26 10:11:08,498 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:08,499 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:08,500 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:08,587 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,587 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:08,588 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,589 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,590 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,591 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,591 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:08,591 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:08,591 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:08,679 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:08,764 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,765 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:08,772 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:08,781 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:08,788 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:08,800 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:08,812 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:08,812 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:08,816 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:08,907 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:08,910 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:08,910 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:08,911 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,911 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,912 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,913 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,913 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:08,913 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:08,914 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:08,916 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:08,926 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 19])", "<class 'int'>: 18")
2023-10-26 10:11:08,927 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:08,928 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,928 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,929 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,930 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:08,930 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:08,930 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:08,933 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:08,943 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:08,957 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,957 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:08,963 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:08,967 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:08,970 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:08,974 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:08,974 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-26 10:11:08,974 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:08,975 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:08,986 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:08,996 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:08,996 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:09,001 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,005 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,009 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,012 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,012 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-26 10:11:09,012 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:09,013 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:09,024 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:09,034 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,034 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:09,039 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,043 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,047 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,050 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,051 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-26 10:11:09,051 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:09,052 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:09,062 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:09,073 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,073 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:09,078 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,082 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,086 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,090 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,090 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-26 10:11:09,090 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:09,091 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:09,101 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:09,111 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,111 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:09,116 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,120 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,124 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,127 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,127 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-26 10:11:09,127 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:09,129 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:09,139 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:09,153 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,154 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:09,158 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,162 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,166 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,170 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,171 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-26 10:11:09,171 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:09,171 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:09,182 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:09,192 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,192 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:09,197 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,201 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,205 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,209 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,209 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-26 10:11:09,209 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:09,210 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:09,220 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:09,231 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,231 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:09,236 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,240 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,243 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,247 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,247 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-26 10:11:09,247 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:09,248 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:09,258 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:09,269 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,269 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:09,274 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,278 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,282 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,286 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,286 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-26 10:11:09,286 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:09,287 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:09,297 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:09,308 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,308 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:09,313 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,317 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,321 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,325 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,325 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-26 10:11:09,326 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:09,326 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:09,337 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:09,347 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,348 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:09,353 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,357 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,361 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,364 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,365 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-26 10:11:09,365 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:09,366 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:09,377 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:09,377 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,378 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:09,382 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,386 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,390 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,394 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 19, 64])"))
2023-10-26 10:11:09,394 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"))
2023-10-26 10:11:09,394 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:09,395 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:09,396 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:09,484 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,484 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:09,485 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:09,486 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:09,487 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:09,488 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:09,488 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:09,488 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:09,489 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:09,576 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:09,662 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,662 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:09,670 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:09,681 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:09,688 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:09,696 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:09,697 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:09,698 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:09,700 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:09,787 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:09,789 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:09,789 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:09,790 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:09,790 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:09,791 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:09,792 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:09,792 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:09,792 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:09,793 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:09,797 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:09,810 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 20])", "<class 'int'>: 19")
2023-10-26 10:11:09,810 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:09,811 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:09,812 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:09,813 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:09,813 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:09,813 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:09,814 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:09,817 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:09,829 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:09,843 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,844 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:09,860 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:09,863 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:09,866 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:09,896 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:09,896 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-26 10:11:09,897 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:09,897 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:09,908 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:09,924 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,925 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:09,952 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:09,956 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:09,959 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:09,963 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:09,963 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-26 10:11:09,963 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:09,964 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:09,973 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:09,982 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:09,983 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:09,987 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:09,991 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:09,995 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:09,999 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:09,999 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-26 10:11:09,999 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:10,000 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:10,010 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:10,019 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,019 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,024 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,028 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,032 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,035 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,035 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-26 10:11:10,035 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:10,036 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:10,046 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:10,055 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,055 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,060 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,064 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,068 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,071 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,071 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-26 10:11:10,071 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:10,073 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:10,086 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:10,098 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,099 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,104 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,108 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,111 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,115 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,115 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-26 10:11:10,115 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:10,116 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:10,126 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:10,135 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,136 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,140 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,144 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,147 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,151 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,151 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-26 10:11:10,151 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:10,152 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:10,162 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:10,171 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,172 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,176 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,180 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,183 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,187 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,187 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-26 10:11:10,187 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:10,188 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:10,197 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:10,206 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,206 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,210 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,214 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,217 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,222 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,222 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-26 10:11:10,222 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:10,223 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:10,233 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:10,243 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,243 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,247 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,251 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,254 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,258 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,258 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-26 10:11:10,258 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:10,259 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:10,269 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:10,278 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,278 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,283 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,286 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,290 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,293 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,293 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-26 10:11:10,293 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:10,294 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:10,304 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:10,304 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,305 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,333 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,337 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,341 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,344 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 20, 64])"))
2023-10-26 10:11:10,344 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"))
2023-10-26 10:11:10,344 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:10,345 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:10,346 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:10,430 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,430 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:10,431 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:10,432 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:10,433 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:10,434 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:10,434 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:10,434 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:10,434 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:10,519 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:10,603 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,603 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:10,612 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:10,621 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:10,630 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:10,639 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:10,640 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:10,641 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:10,643 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:10,735 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:10,737 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:10,737 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:10,738 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:10,738 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:10,739 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:10,740 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:10,740 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:10,740 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:10,741 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:10,742 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:10,752 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 21])", "<class 'int'>: 20")
2023-10-26 10:11:10,752 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:10,753 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:10,754 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:10,754 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:10,755 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:10,755 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:10,755 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:10,758 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:10,767 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:10,780 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,780 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,784 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,788 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,792 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,797 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,797 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-26 10:11:10,797 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:10,798 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:10,807 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:10,816 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,817 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,823 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,827 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,830 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,833 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,833 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-26 10:11:10,834 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:10,834 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:10,844 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:10,853 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,853 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,858 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,861 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,865 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,868 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,868 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-26 10:11:10,868 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:10,869 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:10,879 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:10,895 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,896 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,900 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,903 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,907 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,910 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,910 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-26 10:11:10,910 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:10,911 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:10,921 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:10,930 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,931 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,935 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,938 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,942 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,945 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,945 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-26 10:11:10,945 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:10,946 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:10,956 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:10,968 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:10,969 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:10,975 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,983 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,986 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,989 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:10,990 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-26 10:11:10,990 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:10,991 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:11,000 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:11,010 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,010 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,014 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,018 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,021 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,024 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,024 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-26 10:11:11,024 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:11,025 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:11,035 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:11,044 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,044 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,048 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,052 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,055 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,058 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,058 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-26 10:11:11,058 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:11,059 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:11,069 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:11,078 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,078 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,083 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,087 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,092 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,095 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,095 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-26 10:11:11,095 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:11,096 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:11,105 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:11,118 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,118 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,122 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,126 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,130 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,133 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,133 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-26 10:11:11,133 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:11,134 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:11,143 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:11,152 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,152 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,157 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,161 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,164 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,167 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,167 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-26 10:11:11,168 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:11,168 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:11,178 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:11,179 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,179 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,185 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,188 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,192 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,195 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 21, 64])"))
2023-10-26 10:11:11,195 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"))
2023-10-26 10:11:11,195 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:11,196 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:11,197 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:11,279 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,280 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:11,281 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:11,281 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:11,282 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:11,283 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:11,283 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:11,283 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:11,283 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:11,367 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:11,449 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,449 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:11,456 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:11,463 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:11,471 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:11,483 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:11,485 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:11,485 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:11,487 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:11,571 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:11,573 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:11,573 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:11,573 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:11,574 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:11,575 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:11,575 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:11,575 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:11,576 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:11,577 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:11,578 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:11,587 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 22])", "<class 'int'>: 21")
2023-10-26 10:11:11,587 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:11,588 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:11,589 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:11,590 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:11,590 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:11,590 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:11,591 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:11,593 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:11,602 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:11,615 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,616 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,620 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,625 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,628 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,632 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,632 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-26 10:11:11,632 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:11,633 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:11,642 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:11,652 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,652 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,657 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,661 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,664 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,668 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,668 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-26 10:11:11,668 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:11,669 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:11,678 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:11,687 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,688 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,692 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,696 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,699 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,702 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,702 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-26 10:11:11,703 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:11,703 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:11,713 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:11,729 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,730 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,734 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,737 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,741 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,744 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,744 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-26 10:11:11,744 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:11,745 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:11,755 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:11,765 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,765 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,769 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,773 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,776 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,780 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,780 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-26 10:11:11,780 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:11,781 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:11,791 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:11,804 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,804 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,808 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,812 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,815 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,818 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,818 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-26 10:11:11,819 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:11,819 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:11,829 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:11,839 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,839 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,843 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,847 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,850 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,854 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,854 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-26 10:11:11,854 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:11,855 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:11,865 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:11,874 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,875 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,879 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,883 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,886 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,890 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,890 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-26 10:11:11,890 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:11,891 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:11,901 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:11,910 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,910 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,914 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,918 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,922 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,925 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,925 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-26 10:11:11,926 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:11,926 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:11,936 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:11,945 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,945 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,956 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,961 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,965 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,968 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,968 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-26 10:11:11,968 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:11,969 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:11,979 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:11,988 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:11,988 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:11,993 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:11,997 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:12,000 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:12,004 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:12,004 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-26 10:11:12,004 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:12,005 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:12,015 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:12,016 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,016 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:12,020 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:12,024 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:12,027 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:12,031 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 22, 64])"))
2023-10-26 10:11:12,031 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"))
2023-10-26 10:11:12,031 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:12,032 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:12,033 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:12,116 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,117 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:12,118 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:12,118 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:12,119 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:12,120 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:12,120 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:12,120 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:12,121 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:12,207 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:12,290 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,290 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:12,298 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:12,305 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:12,312 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:12,320 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:12,322 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:12,322 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:12,324 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:12,409 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:12,411 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:12,411 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:12,412 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:12,413 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:12,413 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:12,414 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:12,414 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:12,414 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:12,415 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:12,417 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:12,433 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 23])", "<class 'int'>: 22")
2023-10-26 10:11:12,433 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:12,434 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:12,434 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:12,435 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:12,436 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:12,436 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:12,436 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:12,439 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:12,451 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:12,465 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,465 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:12,469 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,473 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,478 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,481 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,482 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-26 10:11:12,482 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:12,483 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:12,493 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:12,509 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,510 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:12,514 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,518 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,521 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,525 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,525 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-26 10:11:12,525 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:12,526 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:12,536 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:12,554 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,554 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:12,558 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,562 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,566 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,569 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,569 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-26 10:11:12,569 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:12,570 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:12,581 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:12,597 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,598 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:12,602 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,606 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,609 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,613 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,613 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-26 10:11:12,613 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:12,614 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:12,624 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:12,637 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,637 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:12,641 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,645 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,649 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,653 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,653 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-26 10:11:12,653 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:12,654 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:12,668 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:12,681 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,681 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:12,686 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,689 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,693 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,696 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,697 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-26 10:11:12,697 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:12,697 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:12,708 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:12,717 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,718 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:12,729 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,733 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,737 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,744 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,744 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-26 10:11:12,744 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:12,745 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:12,759 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:12,772 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,772 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:12,777 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,781 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,784 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,788 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,788 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-26 10:11:12,788 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:12,789 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:12,798 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:12,808 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,809 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:12,813 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,817 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,821 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,824 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,824 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-26 10:11:12,824 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:12,825 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:12,836 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:12,853 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,854 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:12,859 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,864 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,867 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,871 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,871 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-26 10:11:12,871 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:12,872 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:12,882 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:12,892 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,893 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:12,908 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,911 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,915 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,919 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,919 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-26 10:11:12,919 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:12,920 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:12,930 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:12,931 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:12,931 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:12,935 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,939 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,954 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,958 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 23, 64])"))
2023-10-26 10:11:12,958 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"))
2023-10-26 10:11:12,958 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:12,959 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:12,960 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:13,055 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,055 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:13,056 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,057 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,057 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,058 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,058 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:13,059 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:13,059 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:13,149 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:13,235 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,236 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:13,245 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:13,254 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:13,263 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:13,272 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:13,274 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:13,274 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:13,276 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:13,363 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:13,366 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:13,366 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:13,367 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,367 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,368 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,369 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,369 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:13,369 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:13,370 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:13,372 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:13,382 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 24])", "<class 'int'>: 23")
2023-10-26 10:11:13,382 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:13,383 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,384 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,384 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,385 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,385 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:13,385 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:13,389 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:13,398 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:13,413 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,413 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:13,419 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,423 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,427 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,430 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,430 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-26 10:11:13,431 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:13,431 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:13,442 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:13,451 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,452 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:13,456 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,460 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,464 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,469 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,469 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-26 10:11:13,469 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:13,470 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:13,480 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:13,490 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,490 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:13,495 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,499 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,503 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,507 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,507 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-26 10:11:13,507 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:13,508 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:13,519 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:13,529 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,530 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:13,534 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,538 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,542 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,545 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,546 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-26 10:11:13,546 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:13,547 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:13,557 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:13,566 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,567 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:13,571 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,575 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,579 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,583 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,583 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-26 10:11:13,583 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:13,584 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:13,594 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:13,607 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,607 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:13,612 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,616 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,619 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,624 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,624 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-26 10:11:13,624 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:13,625 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:13,635 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:13,645 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,645 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:13,649 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,654 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,657 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,661 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,661 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-26 10:11:13,661 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:13,662 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:13,672 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:13,682 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,682 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:13,687 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,690 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,694 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,697 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,698 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-26 10:11:13,698 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:13,699 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:13,708 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:13,718 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,718 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:13,723 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,727 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,730 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,734 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,734 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-26 10:11:13,734 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:13,735 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:13,745 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:13,755 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,755 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:13,760 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,764 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,767 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,771 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,771 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-26 10:11:13,771 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:13,772 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:13,782 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:13,792 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,792 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:13,796 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,800 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,806 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,831 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,831 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-26 10:11:13,831 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:13,832 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:13,843 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:13,844 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,844 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:13,864 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,867 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,871 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,874 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 24, 64])"))
2023-10-26 10:11:13,875 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"))
2023-10-26 10:11:13,875 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:13,876 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:13,876 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:13,961 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:13,962 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:13,963 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,963 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,964 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,965 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:13,965 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:13,965 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:13,966 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:14,052 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:14,140 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:14,141 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:14,151 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:14,161 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:14,168 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:14,174 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:14,175 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:14,176 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:14,178 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:14,257 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:14,259 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:14,259 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:14,259 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:14,260 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:14,260 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:14,261 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:14,261 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:14,261 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:14,262 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:14,265 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:14,281 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 25])", "<class 'int'>: 24")
2023-10-26 10:11:14,281 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:14,281 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:14,282 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:14,283 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:14,283 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:14,283 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:14,284 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:14,286 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:14,293 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:14,304 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:14,304 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:14,308 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,312 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,315 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,318 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,318 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-26 10:11:14,318 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:14,319 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:14,327 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:14,342 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:14,342 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:14,346 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,350 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,353 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,356 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,356 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-26 10:11:14,356 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:14,357 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:14,365 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:14,380 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:14,380 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:14,384 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,387 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,391 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,394 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,394 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-26 10:11:14,394 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:14,395 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:14,403 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:14,418 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:14,418 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:14,422 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,426 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,429 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,432 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,432 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-26 10:11:14,432 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:14,433 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:14,441 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:14,453 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:14,454 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:14,458 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,461 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,464 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,468 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,468 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-26 10:11:14,468 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:14,469 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:14,481 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:14,493 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:14,493 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:14,497 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,501 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,504 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,507 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,507 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-26 10:11:14,507 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:14,508 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:14,516 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:14,525 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:14,525 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:14,529 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,532 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,537 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,540 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,540 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-26 10:11:14,541 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:14,541 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:14,553 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:14,565 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:14,565 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:14,569 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,593 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,597 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,602 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,602 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-26 10:11:14,602 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:14,603 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:14,613 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:14,622 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:14,623 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:14,627 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,631 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,634 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,638 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,638 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-26 10:11:14,638 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:14,639 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:14,649 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:14,660 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:14,661 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:14,665 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,670 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,673 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,676 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,677 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-26 10:11:14,677 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:14,678 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:14,687 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:14,696 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:14,696 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:14,700 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,705 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,708 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,711 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,711 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-26 10:11:14,712 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:14,712 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:14,726 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:14,727 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:14,727 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:14,731 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,736 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,739 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,742 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 25, 64])"))
2023-10-26 10:11:14,742 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"))
2023-10-26 10:11:14,742 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:14,743 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:14,744 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:14,827 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:14,827 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:14,828 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:14,829 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:14,830 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:14,831 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:14,831 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:14,831 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:14,831 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:14,916 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:15,001 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,001 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:15,009 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:15,016 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:15,023 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:15,031 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:15,033 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:15,033 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:15,035 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:15,118 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:15,120 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:15,120 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:15,121 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,122 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,122 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,123 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,123 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:15,123 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:15,124 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:15,126 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:15,138 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 26])", "<class 'int'>: 25")
2023-10-26 10:11:15,138 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:15,139 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,140 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,141 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,141 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,141 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:15,142 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:15,145 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:15,156 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:15,169 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,170 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:15,174 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,178 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,182 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,185 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,185 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-26 10:11:15,185 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:15,186 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:15,196 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:15,212 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,212 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:15,216 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,222 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,226 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,230 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,230 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-26 10:11:15,230 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:15,231 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:15,241 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:15,251 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,251 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:15,255 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,259 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,267 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,270 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,270 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-26 10:11:15,270 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:15,271 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:15,280 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:15,289 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,290 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:15,294 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,298 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,302 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,306 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,306 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-26 10:11:15,306 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:15,307 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:15,315 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:15,324 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,324 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:15,328 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,332 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,336 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,339 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,340 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-26 10:11:15,340 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:15,341 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:15,354 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:15,367 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,367 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:15,372 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,375 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,379 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,382 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,383 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-26 10:11:15,383 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:15,384 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:15,392 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:15,401 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,401 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:15,405 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,409 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,413 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,416 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,416 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-26 10:11:15,417 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:15,417 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:15,425 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:15,434 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,435 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:15,439 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,443 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,446 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,449 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,449 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-26 10:11:15,450 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:15,450 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:15,459 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:15,468 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,468 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:15,473 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,476 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,480 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,483 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,483 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-26 10:11:15,484 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:15,484 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:15,492 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:15,502 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,502 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:15,507 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,511 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,514 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,518 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,518 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-26 10:11:15,518 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:15,519 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:15,528 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:15,537 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,537 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:15,542 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,546 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,549 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,553 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,553 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-26 10:11:15,553 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:15,554 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:15,563 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:15,563 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,564 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:15,568 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,571 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,576 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,579 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 26, 64])"))
2023-10-26 10:11:15,579 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"))
2023-10-26 10:11:15,579 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:15,580 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:15,581 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:15,661 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,661 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:15,662 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,663 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,663 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,664 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,664 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:15,665 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:15,665 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:15,746 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:15,826 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,827 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:15,834 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:15,840 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:15,846 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:15,853 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:15,854 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:15,855 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:15,857 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:15,938 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:15,940 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:15,940 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:15,941 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,942 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,942 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,943 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,943 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:15,943 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:15,944 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:15,945 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:15,954 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 27])", "<class 'int'>: 26")
2023-10-26 10:11:15,955 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:15,955 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,956 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,957 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,957 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:15,957 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:15,957 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:15,960 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:15,969 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:15,982 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:15,982 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:15,987 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:15,990 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:15,994 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:15,997 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:15,997 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-26 10:11:15,997 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:15,998 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:16,006 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:16,018 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,018 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,024 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,028 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,031 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,035 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,035 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-26 10:11:16,035 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:16,036 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:16,044 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:16,054 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,054 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,058 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,062 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,065 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,069 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,069 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-26 10:11:16,069 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:16,070 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:16,078 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:16,087 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,087 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,091 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,095 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,099 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,102 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,102 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-26 10:11:16,102 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:16,103 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:16,111 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:16,120 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,120 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,124 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,128 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,131 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,135 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,135 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-26 10:11:16,135 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:16,136 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:16,148 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:16,161 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,161 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,165 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,169 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,173 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,176 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,176 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-26 10:11:16,176 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:16,177 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:16,185 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:16,194 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,194 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,198 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,202 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,206 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,210 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,210 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-26 10:11:16,210 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:16,211 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:16,220 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:16,229 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,229 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,233 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,237 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,253 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,257 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,257 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-26 10:11:16,257 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:16,258 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:16,267 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:16,277 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,277 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,282 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,286 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,289 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,293 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,293 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-26 10:11:16,293 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:16,294 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:16,304 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:16,313 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,313 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,318 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,323 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,327 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,330 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,330 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-26 10:11:16,330 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:16,331 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:16,341 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:16,350 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,350 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,355 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,359 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,363 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,366 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,367 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-26 10:11:16,367 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:16,368 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:16,377 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:16,378 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,378 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,383 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,387 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,390 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,394 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 27, 64])"))
2023-10-26 10:11:16,394 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"))
2023-10-26 10:11:16,395 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:16,395 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:16,396 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:16,481 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,482 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:16,483 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:16,484 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:16,485 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:16,485 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:16,485 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:16,486 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:16,486 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:16,573 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:16,659 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,660 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:16,667 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:16,678 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:16,685 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:16,691 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:16,693 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:16,693 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:16,695 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:16,779 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:16,781 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:16,781 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:16,782 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:16,783 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:16,783 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:16,784 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:16,784 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:16,784 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:16,786 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:16,787 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:16,796 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 28])", "<class 'int'>: 27")
2023-10-26 10:11:16,797 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:16,797 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:16,798 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:16,799 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:16,800 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:16,800 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:16,800 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:16,803 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:16,811 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:16,820 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,821 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,825 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,829 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,833 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,836 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,836 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-26 10:11:16,836 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:16,837 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:16,847 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:16,856 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,856 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,860 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,864 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,867 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,871 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,871 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-26 10:11:16,871 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:16,872 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:16,880 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:16,889 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,890 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,894 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,898 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,902 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,905 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,905 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-26 10:11:16,905 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:16,906 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:16,915 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:16,924 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,925 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,929 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,933 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,937 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,940 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,940 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-26 10:11:16,940 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:16,941 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:16,950 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:16,963 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,964 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:16,968 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,972 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,975 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,979 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:16,979 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-26 10:11:16,979 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:16,980 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:16,989 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:16,998 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:16,998 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,003 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,007 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,010 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,014 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,014 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-26 10:11:17,014 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:17,015 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:17,024 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:17,034 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,034 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,039 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,043 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,046 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,050 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,050 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-26 10:11:17,050 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:17,051 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:17,061 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:17,078 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,078 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,083 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,087 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,090 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,094 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,094 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-26 10:11:17,095 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:17,095 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:17,104 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:17,114 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,114 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,119 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,123 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,126 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,130 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,130 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-26 10:11:17,130 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:17,131 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:17,140 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:17,149 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,150 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,154 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,158 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,162 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,165 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,165 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-26 10:11:17,165 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:17,166 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:17,175 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:17,185 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,185 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,193 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,203 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,206 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,210 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,210 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-26 10:11:17,210 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:17,211 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:17,221 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:17,222 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,222 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,227 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,231 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,234 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,238 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 28, 64])"))
2023-10-26 10:11:17,238 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"))
2023-10-26 10:11:17,238 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:17,239 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:17,239 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:17,326 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,326 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:17,327 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:17,328 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:17,329 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:17,329 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:17,330 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:17,330 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:17,330 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:17,418 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:17,502 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,502 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:17,512 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:17,521 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:17,531 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:17,540 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:17,541 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:17,542 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:17,544 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:17,630 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:17,632 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:17,632 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:17,633 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:17,634 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:17,634 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:17,635 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:17,635 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:17,635 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:17,637 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:17,638 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:17,648 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 29])", "<class 'int'>: 28")
2023-10-26 10:11:17,648 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:17,649 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:17,650 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:17,650 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:17,651 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:17,651 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:17,651 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:17,654 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:17,667 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:17,681 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,681 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,686 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,690 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,693 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,697 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,697 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-26 10:11:17,698 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:17,698 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:17,709 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:17,726 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,726 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,730 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,735 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,738 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,741 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,742 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-26 10:11:17,742 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:17,743 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:17,752 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:17,762 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,762 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,767 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,771 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,774 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,780 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,780 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-26 10:11:17,780 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:17,781 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:17,792 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:17,808 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,809 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,813 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,817 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,821 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,825 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,825 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-26 10:11:17,825 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:17,826 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:17,837 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:17,850 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,851 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,856 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,859 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,863 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,867 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,867 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-26 10:11:17,867 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:17,868 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:17,878 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:17,888 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,888 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,893 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,897 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,901 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,904 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,904 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-26 10:11:17,905 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:17,905 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:17,916 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:17,925 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,925 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,930 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,934 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,938 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,941 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,942 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-26 10:11:17,942 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:17,943 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:17,953 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:17,963 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:17,963 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:17,968 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,971 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,975 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,978 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:17,979 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-26 10:11:17,979 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:17,980 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:17,990 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:17,999 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,000 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:18,004 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,008 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,012 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,016 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,016 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-26 10:11:18,016 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:18,017 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:18,028 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:18,037 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,038 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:18,042 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,047 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,050 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,054 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,054 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-26 10:11:18,054 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:18,055 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:18,065 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:18,075 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,075 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:18,080 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,084 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,088 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,091 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,092 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-26 10:11:18,092 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:18,093 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:18,106 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:18,107 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,107 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:18,134 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,138 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,143 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,147 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 29, 64])"))
2023-10-26 10:11:18,147 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"))
2023-10-26 10:11:18,147 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:18,148 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:18,148 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:18,235 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,235 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:18,236 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:18,237 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:18,238 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:18,239 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:18,239 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:18,239 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:18,239 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:18,326 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:18,411 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,411 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:18,421 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:18,431 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:18,442 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:18,452 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:18,454 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:18,454 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:18,457 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:18,549 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:18,552 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:18,552 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:18,553 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:18,553 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:18,554 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:18,554 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:18,555 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:18,555 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:18,556 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:18,557 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:18,567 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 30])", "<class 'int'>: 29")
2023-10-26 10:11:18,567 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:18,568 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:18,569 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:18,569 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:18,570 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:18,570 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:18,570 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:18,573 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:18,582 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:18,595 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,595 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:18,600 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,604 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,607 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,611 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,612 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-26 10:11:18,612 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:18,613 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:18,623 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:18,633 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,633 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:18,646 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,650 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,654 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,657 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,658 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-26 10:11:18,658 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:18,659 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:18,668 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:18,679 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,679 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:18,684 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,688 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,691 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,695 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,695 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-26 10:11:18,695 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:18,696 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:18,707 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:18,724 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,724 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:18,729 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,733 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,736 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,740 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,740 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-26 10:11:18,740 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:18,741 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:18,751 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:18,762 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,762 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:18,767 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,770 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,774 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,778 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,778 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-26 10:11:18,778 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:18,779 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:18,789 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:18,803 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,803 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:18,831 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,835 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,841 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,853 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,854 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-26 10:11:18,854 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:18,855 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:18,866 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:18,876 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,877 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:18,883 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,886 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,890 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,894 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,894 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-26 10:11:18,894 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:18,895 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:18,906 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:18,916 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,916 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:18,921 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,925 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,929 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,932 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,932 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-26 10:11:18,933 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:18,933 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:18,944 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:18,953 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,954 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:18,958 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,962 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,966 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,969 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,969 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-26 10:11:18,969 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:18,970 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:18,981 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:18,990 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:18,990 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:18,995 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:18,999 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:19,003 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:19,006 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:19,006 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-26 10:11:19,006 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:19,007 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:19,017 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:19,026 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,027 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:19,031 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:19,035 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:19,039 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:19,042 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:19,042 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-26 10:11:19,043 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:19,044 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:19,054 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:19,054 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,054 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:19,059 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:19,063 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:19,081 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:19,085 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 30, 64])"))
2023-10-26 10:11:19,085 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"))
2023-10-26 10:11:19,085 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:19,086 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:19,087 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:19,172 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,172 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:19,173 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:19,174 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:19,174 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:19,175 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:19,175 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:19,175 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:19,176 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:19,262 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:19,346 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,346 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:19,353 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:19,361 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:19,369 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:19,376 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:19,378 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:19,378 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:19,380 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:19,466 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:19,468 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:19,468 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:19,469 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:19,470 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:19,470 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:19,471 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:19,471 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:19,471 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:19,472 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:19,474 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:19,483 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 31])", "<class 'int'>: 30")
2023-10-26 10:11:19,483 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:19,484 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:19,485 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:19,486 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:19,486 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:19,487 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:19,487 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:19,489 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:19,498 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:19,511 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,511 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:19,516 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,520 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,523 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,527 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,527 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-26 10:11:19,528 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:19,528 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:19,538 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:19,548 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,548 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:19,553 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,557 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,560 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,564 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,564 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-26 10:11:19,564 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:19,565 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:19,575 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:19,585 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,585 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:19,600 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,603 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,607 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,611 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,611 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-26 10:11:19,611 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:19,612 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:19,622 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:19,632 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,632 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:19,637 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,641 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,644 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,648 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,648 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-26 10:11:19,649 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:19,650 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:19,663 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:19,673 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,673 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:19,678 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,682 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,685 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,689 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,689 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-26 10:11:19,690 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:19,690 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:19,700 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:19,709 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,710 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:19,714 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,718 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,722 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,725 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,726 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-26 10:11:19,726 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:19,726 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:19,737 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:19,747 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,747 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:19,752 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,755 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,759 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,763 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,763 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-26 10:11:19,763 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:19,764 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:19,774 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:19,791 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,791 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:19,796 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,799 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,803 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,806 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,806 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-26 10:11:19,807 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:19,807 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:19,818 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:19,827 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,828 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:19,832 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,838 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,841 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,847 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,847 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-26 10:11:19,847 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:19,848 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:19,858 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:19,868 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,869 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:19,873 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,877 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,881 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,885 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,885 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-26 10:11:19,886 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:19,886 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:19,896 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:19,906 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,906 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:19,910 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,914 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,918 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,921 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,922 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-26 10:11:19,922 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:19,923 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:19,933 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:19,933 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:19,933 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:19,938 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,942 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,945 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,949 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 31, 64])"))
2023-10-26 10:11:19,949 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"))
2023-10-26 10:11:19,949 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:19,950 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:19,950 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:20,036 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,036 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:20,037 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,038 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,039 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,039 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,039 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:20,040 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:20,040 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:20,126 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:20,210 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,211 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:20,219 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:20,226 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:20,233 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:20,240 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:20,242 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:20,242 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:20,244 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:20,331 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:20,333 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:20,333 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:20,334 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,334 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,335 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,336 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,336 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:20,336 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:20,337 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:20,338 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:20,348 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 32])", "<class 'int'>: 31")
2023-10-26 10:11:20,348 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:20,349 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,350 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,350 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,351 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,351 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:20,351 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:20,354 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:20,363 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:20,376 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,376 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:20,381 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,385 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,388 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,392 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,392 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-26 10:11:20,392 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:20,393 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:20,403 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:20,413 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,413 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:20,418 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,422 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,425 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,429 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,429 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-26 10:11:20,429 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:20,430 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:20,440 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:20,449 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,450 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:20,454 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,458 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,462 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,466 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,466 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-26 10:11:20,467 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:20,467 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:20,477 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:20,487 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,487 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:20,492 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,496 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,499 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,503 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,503 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-26 10:11:20,504 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:20,504 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:20,514 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:20,528 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,528 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:20,533 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,537 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,540 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,544 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,544 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-26 10:11:20,545 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:20,546 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:20,556 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:20,565 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,566 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:20,570 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,574 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,578 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,581 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,581 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-26 10:11:20,582 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:20,582 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:20,592 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:20,601 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,602 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:20,606 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,612 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,621 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,625 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,625 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-26 10:11:20,625 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:20,626 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:20,636 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:20,645 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,645 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:20,650 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,654 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,658 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,661 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,661 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-26 10:11:20,662 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:20,662 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:20,672 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:20,682 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,682 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:20,686 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,690 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,694 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,698 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,698 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-26 10:11:20,698 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:20,699 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:20,709 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:20,725 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,726 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:20,730 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,735 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,738 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,742 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,742 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-26 10:11:20,742 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:20,743 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:20,753 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:20,762 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,762 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:20,767 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,771 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,774 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,778 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,778 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-26 10:11:20,778 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:20,779 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:20,789 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:20,790 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,790 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:20,794 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,798 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,802 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,805 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 32, 64])"))
2023-10-26 10:11:20,806 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"))
2023-10-26 10:11:20,806 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:20,807 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:20,807 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:20,893 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:20,893 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:20,894 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,895 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,896 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,897 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:20,897 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:20,897 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:20,897 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:20,985 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:21,070 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:21,071 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:21,078 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:21,085 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:21,093 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:21,100 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:21,102 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:21,102 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:21,104 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:21,190 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:21,192 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:21,193 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:21,193 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:21,194 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:21,195 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:21,195 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:21,196 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:21,196 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:21,197 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:21,198 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:21,208 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 33])", "<class 'int'>: 32")
2023-10-26 10:11:21,208 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:21,209 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:21,210 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:21,211 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:21,211 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:21,211 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:21,212 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:21,215 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:21,223 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:21,236 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:21,236 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:21,241 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,245 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,249 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,253 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,253 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-26 10:11:21,253 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:21,254 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:21,264 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:21,274 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:21,274 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:21,279 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,283 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,287 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,291 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,291 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-26 10:11:21,291 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:21,292 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:21,305 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:21,320 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:21,321 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:21,328 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,337 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,343 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,347 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,348 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-26 10:11:21,348 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:21,349 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:21,360 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:21,377 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:21,378 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:21,382 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,386 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,390 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,394 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,394 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-26 10:11:21,394 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:21,395 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:21,406 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:21,416 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:21,417 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:21,421 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,425 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,429 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,433 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,433 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-26 10:11:21,433 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:21,435 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:21,445 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:21,459 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:21,460 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:21,464 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,470 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,473 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,477 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,477 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-26 10:11:21,477 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:21,478 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:21,489 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:21,499 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:21,499 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:21,504 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,508 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,511 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,536 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,536 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-26 10:11:21,537 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:21,537 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:21,548 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:21,558 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:21,558 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:21,579 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,586 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,591 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,599 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,600 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-26 10:11:21,600 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:21,601 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:21,612 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:21,622 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:21,622 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:21,627 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,632 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,635 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,639 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,639 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-26 10:11:21,639 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:21,640 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:21,651 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:21,661 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:21,661 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:21,666 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,670 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,673 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,678 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,678 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-26 10:11:21,678 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:21,679 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:21,689 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:21,699 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:21,699 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:21,705 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,709 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,712 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,720 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,720 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-26 10:11:21,720 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:21,722 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:21,733 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:21,734 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:21,734 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:21,740 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,747 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,751 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,755 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 33, 64])"))
2023-10-26 10:11:21,755 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"))
2023-10-26 10:11:21,755 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:21,756 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:21,756 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:21,842 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:21,843 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:21,844 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:21,845 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:21,845 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:21,846 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:21,846 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:21,846 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:21,847 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:21,933 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:22,019 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,019 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:22,027 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:22,034 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:22,041 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:22,049 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:22,051 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:22,051 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:22,053 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:22,140 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:22,142 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:22,142 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:22,143 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:22,144 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:22,144 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:22,145 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:22,145 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:22,145 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:22,146 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:22,151 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:22,164 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 34])", "<class 'int'>: 33")
2023-10-26 10:11:22,165 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:22,165 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:22,166 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:22,167 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:22,167 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:22,168 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:22,168 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:22,171 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:22,184 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:22,197 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,198 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:22,202 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,206 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,210 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,213 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,214 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-26 10:11:22,214 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:22,215 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:22,225 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:22,242 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,242 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:22,246 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,251 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,254 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,258 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,258 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-26 10:11:22,258 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:22,259 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:22,269 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:22,279 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,279 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:22,284 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,303 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,307 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,311 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,311 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-26 10:11:22,311 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:22,312 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:22,323 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:22,340 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,341 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:22,345 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,350 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,354 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,358 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,358 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-26 10:11:22,358 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:22,359 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:22,371 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:22,385 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,385 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:22,390 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,394 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,398 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,401 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,402 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-26 10:11:22,402 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:22,402 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:22,413 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:22,423 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,423 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:22,428 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,432 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,436 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,439 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,440 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-26 10:11:22,440 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:22,441 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:22,451 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:22,461 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,461 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:22,466 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,471 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,476 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,481 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,481 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-26 10:11:22,481 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:22,482 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:22,493 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:22,504 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,504 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:22,509 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,530 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,534 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,539 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,539 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-26 10:11:22,539 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:22,540 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:22,551 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:22,561 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,561 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:22,566 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,571 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,574 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,579 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,579 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-26 10:11:22,579 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:22,580 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:22,590 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:22,600 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,601 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:22,606 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,610 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,614 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,617 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,617 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-26 10:11:22,618 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:22,618 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:22,629 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:22,638 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,639 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:22,644 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,649 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,653 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,657 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,657 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-26 10:11:22,657 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:22,658 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:22,668 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:22,668 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,669 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:22,673 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,677 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,681 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,685 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 34, 64])"))
2023-10-26 10:11:22,685 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"))
2023-10-26 10:11:22,686 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:22,686 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:22,687 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:22,773 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,773 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:22,774 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:22,775 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:22,776 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:22,777 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:22,777 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:22,777 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:22,777 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:22,863 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:22,949 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:22,950 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:22,957 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:22,964 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:22,972 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:22,979 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:22,981 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:22,981 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:22,983 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:23,070 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:23,072 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:23,072 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:23,073 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,073 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,074 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,075 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,075 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:23,075 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:23,076 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:23,077 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:23,087 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 35])", "<class 'int'>: 34")
2023-10-26 10:11:23,087 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:23,088 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,089 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,090 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,090 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,090 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:23,091 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:23,093 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:23,102 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:23,117 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:23,117 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:23,125 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,140 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,144 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,149 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,149 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-26 10:11:23,149 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:23,150 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:23,160 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:23,170 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:23,170 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:23,175 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,179 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,184 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,188 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,188 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-26 10:11:23,188 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:23,189 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:23,199 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:23,209 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:23,209 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:23,214 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,218 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,222 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,225 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,226 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-26 10:11:23,226 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:23,227 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:23,237 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:23,254 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:23,254 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:23,259 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,263 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,266 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,270 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,270 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-26 10:11:23,270 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:23,271 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:23,281 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:23,290 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:23,291 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:23,295 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,299 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,303 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,307 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,307 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-26 10:11:23,308 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:23,309 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:23,318 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:23,332 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:23,332 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:23,337 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,341 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,345 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,348 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,349 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-26 10:11:23,349 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:23,350 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:23,360 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:23,370 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:23,370 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:23,375 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,379 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,382 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,386 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,386 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-26 10:11:23,386 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:23,387 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:23,400 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:23,414 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:23,414 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:23,419 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,426 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,429 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,434 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,434 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-26 10:11:23,434 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:23,435 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:23,445 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:23,455 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:23,455 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:23,460 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,464 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,468 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,471 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,472 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-26 10:11:23,472 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:23,472 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:23,486 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:23,499 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:23,500 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:23,504 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,509 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,512 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,516 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,516 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-26 10:11:23,517 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:23,517 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:23,528 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:23,537 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:23,537 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:23,542 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,546 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,550 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,554 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,554 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-26 10:11:23,554 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:23,555 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:23,569 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:23,570 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:23,570 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:23,575 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,579 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,583 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,586 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 35, 64])"))
2023-10-26 10:11:23,587 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"))
2023-10-26 10:11:23,587 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:23,588 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:23,588 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:23,675 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:23,675 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:23,676 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,677 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,678 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,679 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,679 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:23,679 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:23,679 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:23,773 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:23,867 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:23,867 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:23,875 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:23,886 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:23,893 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:23,900 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:23,902 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:23,902 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:23,904 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:23,991 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:23,993 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:23,993 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:23,994 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,994 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,995 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,996 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:23,996 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:23,996 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:23,997 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:23,998 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:24,011 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 36])", "<class 'int'>: 35")
2023-10-26 10:11:24,011 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:24,012 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,013 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,014 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,014 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,014 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:24,015 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:24,018 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:24,030 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:24,044 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,044 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:24,049 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,053 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,056 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,060 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,061 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-26 10:11:24,061 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:24,062 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:24,072 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:24,088 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,088 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:24,093 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,097 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,101 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,104 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,105 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-26 10:11:24,105 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:24,106 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:24,116 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:24,126 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,126 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:24,133 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,137 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,141 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,144 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,145 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-26 10:11:24,145 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:24,146 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:24,156 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:24,173 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,173 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:24,178 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,196 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,200 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,204 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,204 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-26 10:11:24,204 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:24,205 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:24,215 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:24,229 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,229 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:24,234 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,238 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,241 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,245 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,245 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-26 10:11:24,246 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:24,247 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:24,257 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:24,266 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,267 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:24,271 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,275 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,279 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,284 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,284 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-26 10:11:24,284 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:24,285 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:24,295 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:24,304 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,305 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:24,309 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,313 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,317 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,320 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,321 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-26 10:11:24,321 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:24,322 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:24,332 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:24,341 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,342 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:24,346 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,351 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,354 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,358 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,358 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-26 10:11:24,358 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:24,359 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:24,369 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:24,379 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,379 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:24,384 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,388 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,392 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,395 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,395 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-26 10:11:24,396 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:24,396 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:24,407 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:24,416 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,417 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:24,421 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,426 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,429 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,433 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,434 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-26 10:11:24,434 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:24,435 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:24,445 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:24,457 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,457 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:24,462 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,466 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,470 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,473 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,474 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-26 10:11:24,474 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:24,475 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:24,485 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:24,486 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,486 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:24,491 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,495 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,498 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,502 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 36, 64])"))
2023-10-26 10:11:24,502 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"))
2023-10-26 10:11:24,502 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:24,503 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:24,504 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:24,590 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,590 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:24,591 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,592 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,593 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,594 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,594 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:24,594 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:24,594 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:24,682 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:24,767 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,767 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:24,783 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:24,798 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:24,808 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:24,817 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:24,819 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:24,819 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:24,822 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:24,905 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:24,908 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:24,908 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:24,909 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,909 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,910 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,910 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,911 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:24,911 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:24,912 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:24,913 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:24,923 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 37])", "<class 'int'>: 36")
2023-10-26 10:11:24,923 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:24,924 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,925 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,926 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,926 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:24,926 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:24,927 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:24,930 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:24,938 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:24,951 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,951 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:24,956 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:24,960 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:24,963 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:24,971 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:24,972 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-26 10:11:24,972 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:24,972 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:24,982 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:24,992 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:24,992 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:24,997 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,001 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,005 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,008 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,009 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-26 10:11:25,009 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:25,010 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:25,020 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:25,030 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,030 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,035 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,039 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,043 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,047 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,047 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-26 10:11:25,047 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:25,048 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:25,058 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:25,069 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,070 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,074 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,078 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,082 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,085 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,086 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-26 10:11:25,086 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:25,087 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:25,097 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:25,106 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,107 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,111 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,115 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,119 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,123 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,123 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-26 10:11:25,123 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:25,125 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:25,135 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:25,149 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,149 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,154 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,158 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,162 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,165 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,166 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-26 10:11:25,166 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:25,167 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:25,176 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:25,186 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,186 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,191 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,195 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,202 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,206 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,206 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-26 10:11:25,206 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:25,207 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:25,217 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:25,227 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,227 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,238 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,242 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,245 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,249 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,249 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-26 10:11:25,249 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:25,250 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:25,259 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:25,268 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,268 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,273 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,277 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,280 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,284 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,285 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-26 10:11:25,285 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:25,285 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:25,294 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:25,304 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,304 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,309 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,313 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,317 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,321 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,321 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-26 10:11:25,321 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:25,322 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:25,331 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:25,340 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,340 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,345 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,349 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,352 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,356 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,356 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-26 10:11:25,356 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:25,357 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:25,367 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:25,368 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,368 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,372 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,376 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,380 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,383 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 37, 64])"))
2023-10-26 10:11:25,384 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"))
2023-10-26 10:11:25,384 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:25,385 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:25,385 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:25,468 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,468 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:25,469 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:25,470 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:25,471 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:25,471 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:25,472 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:25,472 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:25,472 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:25,558 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:25,640 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,640 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:25,648 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:25,656 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:25,663 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:25,670 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:25,672 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:25,672 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:25,674 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:25,757 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:25,759 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 1])",)
2023-10-26 10:11:25,759 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:25,760 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:25,761 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:25,761 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:25,762 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:25,762 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:25,762 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-26 10:11:25,763 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-26 10:11:25,764 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:25,774 [forward.py:52 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([4, 38])", "<class 'int'>: 37")
2023-10-26 10:11:25,774 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:25,775 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:25,775 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:25,776 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:25,777 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:25,777 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:25,777 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-26 10:11:25,780 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-26 10:11:25,788 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:25,800 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,800 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,805 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,809 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,812 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,816 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,816 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-26 10:11:25,816 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-26 10:11:25,817 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-26 10:11:25,826 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:25,840 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,840 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,844 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,848 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,852 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,855 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,855 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-26 10:11:25,856 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-26 10:11:25,856 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-26 10:11:25,866 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:25,875 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,876 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,880 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,884 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,888 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,892 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,892 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-26 10:11:25,893 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-26 10:11:25,893 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-26 10:11:25,904 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:25,920 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,920 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,925 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,929 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,934 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,938 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,938 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-26 10:11:25,938 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-26 10:11:25,939 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-26 10:11:25,949 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:25,961 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,961 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:25,966 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,970 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,974 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,977 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:25,977 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-26 10:11:25,977 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-26 10:11:25,978 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-26 10:11:25,988 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:25,998 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:25,998 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:26,003 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,006 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,010 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,014 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,014 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-26 10:11:26,014 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-26 10:11:26,015 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-26 10:11:26,025 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:26,034 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:26,034 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:26,039 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,043 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,046 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,050 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,050 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-26 10:11:26,050 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-26 10:11:26,051 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-26 10:11:26,060 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:26,070 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:26,070 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:26,075 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,079 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,082 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,086 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,086 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-26 10:11:26,086 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-26 10:11:26,087 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-26 10:11:26,096 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:26,105 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:26,105 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:26,110 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,114 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,118 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,121 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,122 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-26 10:11:26,122 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-26 10:11:26,123 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-26 10:11:26,132 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:26,141 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:26,141 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:26,146 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,150 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,154 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,157 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,157 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-26 10:11:26,157 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-26 10:11:26,158 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-26 10:11:26,167 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:26,177 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:26,177 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:26,181 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,185 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,189 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,192 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,193 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-26 10:11:26,193 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-26 10:11:26,193 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-26 10:11:26,203 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:26,203 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:26,204 [forward.py:53 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([4, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-26 10:11:26,208 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,212 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,215 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,219 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([1, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([1, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([1, 12, 38, 64])"))
2023-10-26 10:11:26,219 [forward.py:88 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 12, 38, 64])"))
2023-10-26 10:11:26,219 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-26 10:11:26,220 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-26 10:11:26,221 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:26,304 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:26,304 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:26,305 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:26,306 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:26,307 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:26,307 [forward.py:72 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 768])
2023-10-26 10:11:26,308 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])
2023-10-26 10:11:26,308 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-26 10:11:26,308 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-26 10:11:26,393 [model.py:373 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-26 10:11:26,477 [forward.py:52 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([4, 1, 768])",)
2023-10-26 10:11:26,477 [forward.py:53 in new_forward] DEBUG - kwargs: {}
2023-10-26 10:11:26,485 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:26,495 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:26,503 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:26,510 [forward.py:72 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([1, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([1, 1, 50272])
2023-10-26 10:11:26,512 [forward.py:88 in new_forward] DEBUG - outputs after concat: <class 'torch.Tensor'>: torch.Size([4, 1, 50272])
2023-10-26 10:11:26,512 [model.py:383 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-26 10:11:26,515 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-26 10:11:26,516 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-26 10:11:26,516 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-26 10:11:26,516 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-26 10:11:26,516 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-26 10:11:26,516 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-26 10:11:26,516 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-26 10:11:26,516 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-26 10:11:26,526 [forward.py:23 in reset_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-26 10:11:26,526 [forward.py:23 in reset_forward] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-26 10:11:26,526 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-26 10:11:26,526 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-26 10:11:26,526 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-26 10:11:26,526 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-26 10:11:26,527 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-26 10:11:26,527 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-26 10:11:26,527 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-26 10:11:26,527 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-26 10:11:26,527 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-26 10:11:26,527 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-26 10:11:26,528 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-26 10:11:26,528 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-26 10:11:26,528 [forward.py:23 in reset_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-26 10:11:26,528 [forward.py:23 in reset_forward] DEBUG - lm_head from flexgen to old.
