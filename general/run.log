2023-10-11 15:41:05,191 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmp4bhpuz7k
2023-10-11 15:41:05,191 [instantiator.py:76 in _write] INFO - Writing /tmp/tmp4bhpuz7k/_remote_module_non_scriptable.py
2023-10-11 15:41:05,611 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-11 15:41:05,669 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 15:41:07,162 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-11 15:41:07,441 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-11 15:41:07,442 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-11 15:41:07,442 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-11 15:41:07,442 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-11 15:41:08,255 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 15:41:08,344 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 15:41:08,392 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 15:41:08,495 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 15:41:08,495 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-11 15:41:08,502 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-11 15:41:08,503 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-11 15:41:08,503 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-11 15:41:08,504 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-11 15:41:08,505 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-11 15:41:08,506 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-11 15:41:08,507 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-11 15:41:08,508 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-11 15:41:08,509 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-11 15:41:08,510 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-11 15:41:08,511 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-11 15:41:08,512 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-11 15:41:08,513 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-11 15:41:08,514 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-11 15:41:08,515 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 15:41:08,515 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 15:41:08,515 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-11 15:41:08,517 [model.py:148 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-11 15:41:08,518 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-11 15:41:08,548 [forward.py:48 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-11 15:41:08,548 [forward.py:48 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-11 15:41:08,548 [forward.py:48 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-11 15:41:08,548 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-11 15:41:08,548 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-11 15:41:08,548 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-11 15:41:08,549 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-11 15:41:08,549 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-11 15:41:08,549 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-11 15:41:08,549 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-11 15:41:08,549 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-11 15:41:08,549 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-11 15:41:08,549 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-11 15:41:08,549 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-11 15:41:08,549 [forward.py:48 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-11 15:41:08,550 [forward.py:48 in to_test_forward] DEBUG - lm_head to test forward
2023-10-11 15:41:08,635 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 15:41:08,788 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:08,789 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:08,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:08,791 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:08,791 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:08,802 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:08,804 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:08,810 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:08,812 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:08,819 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:08,821 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:08,827 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:08,829 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:08,836 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:08,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:08,844 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:08,845 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:08,852 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:08,853 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:08,859 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:08,861 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:08,867 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:08,868 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:08,874 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:08,876 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:08,882 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:08,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:08,890 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:08,891 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:08,892 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:08,892 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:08,900 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:08,905 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-11 15:41:08,905 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:41:08,914 [forward.py:28 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-11 15:41:08,914 [forward.py:28 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-11 15:41:08,914 [forward.py:28 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-11 15:41:08,914 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-11 15:41:08,914 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-11 15:41:08,914 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-11 15:41:08,914 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-11 15:41:08,914 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-11 15:41:08,915 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-11 15:41:08,915 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-11 15:41:08,915 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-11 15:41:08,915 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-11 15:41:08,915 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-11 15:41:08,915 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-11 15:41:08,915 [forward.py:28 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-11 15:41:08,915 [forward.py:28 in reset_forward] DEBUG - lm_head from test to old.
2023-10-11 15:41:08,916 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-11 15:41:08,916 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-11 15:41:08,916 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-11 15:41:08,916 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-11 15:41:08,916 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-11 15:41:08,916 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-11 15:41:08,916 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-11 15:41:08,916 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-11 15:41:08,917 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-11 15:41:08,917 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-11 15:41:08,917 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-11 15:41:08,917 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-11 15:41:08,917 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-11 15:41:08,917 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-11 15:41:08,917 [forward.py:120 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-11 15:41:08,917 [forward.py:120 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-11 15:41:08,958 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 15:41:09,098 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:09,099 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:09,099 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])",)
2023-10-11 15:41:09,100 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:09,101 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:41:09,101 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:41:09,102 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:41:09,103 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:41:09,103 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])
2023-10-11 15:41:09,103 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:09,104 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:09,104 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:09,108 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])", "<class 'int'>: 0")
2023-10-11 15:41:09,108 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:09,109 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:41:09,110 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:41:09,110 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:41:09,111 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])", "<class 'int'>: 0"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:41:09,111 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])
2023-10-11 15:41:09,111 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:09,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:09,118 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:09,122 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:41:09,122 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,128 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,132 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,136 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,139 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,140 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:41:09,140 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:09,141 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:09,145 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:09,149 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:41:09,149 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,154 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,160 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,164 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,168 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,168 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:41:09,168 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:09,170 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:09,173 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:09,177 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:41:09,177 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,182 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,185 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,189 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,192 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,193 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:41:09,193 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:09,194 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:09,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:09,201 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:41:09,201 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,210 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,213 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,217 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,217 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:41:09,217 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:09,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:09,222 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:09,226 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:41:09,226 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,231 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,234 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,238 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,242 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,242 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:41:09,242 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:09,244 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:09,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:09,250 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:41:09,251 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,255 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,259 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,262 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,266 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,266 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:41:09,266 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:09,267 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:09,271 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:09,274 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:41:09,275 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,279 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,283 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,286 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,290 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,290 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:41:09,290 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:09,292 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:09,295 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:09,299 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:41:09,299 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,303 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,307 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,311 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,315 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,315 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:41:09,315 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:09,316 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:09,319 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:09,323 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:41:09,323 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,328 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,332 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,336 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,340 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,340 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:41:09,340 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:09,341 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:09,345 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:09,348 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:41:09,349 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,353 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,357 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,361 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,365 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,365 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:41:09,365 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:09,366 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:09,370 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:09,374 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:41:09,374 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,378 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,383 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,386 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,389 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,390 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:41:09,390 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:09,391 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:09,395 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:09,395 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:41:09,395 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,400 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,405 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,408 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,413 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 9, 64])"))
2023-10-11 15:41:09,413 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"))
2023-10-11 15:41:09,413 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:09,414 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:09,415 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:09,415 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:41:09,415 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:09,416 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:41:09,417 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:41:09,418 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:41:09,419 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 768])
2023-10-11 15:41:09,419 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])
2023-10-11 15:41:09,419 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:09,419 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:09,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:09,420 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 768])",)
2023-10-11 15:41:09,420 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:09,435 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 50272])
2023-10-11 15:41:09,449 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 50272])
2023-10-11 15:41:09,462 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 50272])
2023-10-11 15:41:09,477 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 50272])
2023-10-11 15:41:09,477 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 50272])
2023-10-11 15:41:09,477 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:09,488 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:09,489 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:09,489 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:09,489 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:09,490 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,491 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,491 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,492 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,492 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:09,492 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:09,493 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:09,493 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:09,496 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 10])", "<class 'int'>: 9")
2023-10-11 15:41:09,497 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:09,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,498 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,499 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,499 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 10])", "<class 'int'>: 9"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,499 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:09,500 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:09,502 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:09,506 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:09,509 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,509 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,517 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,520 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,524 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,527 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,527 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:41:09,527 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:09,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:09,533 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:09,536 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,537 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,541 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,545 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,552 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,552 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:41:09,552 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:09,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:09,557 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:09,561 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,561 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,565 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,569 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,572 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,576 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,576 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:41:09,576 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:09,578 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:09,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:09,585 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,585 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,594 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,597 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,601 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,605 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,605 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:41:09,605 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:09,606 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:09,610 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:09,614 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,614 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,618 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,623 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,626 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,629 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,630 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:41:09,630 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:09,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:09,635 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:09,638 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,638 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,643 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,647 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,650 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,654 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,654 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:41:09,654 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:09,655 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:09,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:09,662 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,662 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,671 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,674 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,678 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,678 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:41:09,678 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:09,680 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:09,683 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:09,686 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,687 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,695 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,698 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,702 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,702 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:41:09,702 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:09,703 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:09,707 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:09,710 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,711 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,715 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,722 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,726 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,726 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:41:09,726 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:09,728 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:09,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:09,735 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,735 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,739 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,743 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,746 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,750 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,750 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:41:09,750 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:09,751 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:09,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:09,759 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,759 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,763 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,767 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,770 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,774 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,774 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:41:09,774 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:09,776 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:09,779 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:09,780 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,780 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,785 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,789 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,793 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,797 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 9, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 10, 64])"))
2023-10-11 15:41:09,798 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"))
2023-10-11 15:41:09,798 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:09,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:09,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:09,800 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,800 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:09,802 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,805 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,808 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,811 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,811 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:09,812 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:09,812 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:09,813 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:09,813 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,813 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:09,826 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:09,838 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:09,846 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:09,854 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:09,854 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:09,854 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:09,862 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:09,862 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:09,863 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:09,863 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:09,864 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,864 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,866 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,866 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:09,866 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:09,866 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:09,867 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:09,870 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 11])", "<class 'int'>: 10")
2023-10-11 15:41:09,870 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:09,871 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,872 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,873 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,873 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 11])", "<class 'int'>: 10"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:09,873 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:09,873 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:09,876 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:09,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:09,884 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,884 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,889 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:09,919 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:09,940 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:09,944 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:09,944 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:41:09,944 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:09,946 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:09,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:09,954 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,954 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,959 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:09,963 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:09,966 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:09,970 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:09,970 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:41:09,971 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:09,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:09,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:09,979 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:09,980 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:09,984 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:09,988 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:09,992 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:09,996 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:09,996 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:41:09,997 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:09,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:10,002 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:10,005 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,005 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,010 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,014 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,017 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,021 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,021 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:41:10,021 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:10,023 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:10,026 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:10,030 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,030 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,035 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,039 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,043 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,046 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,047 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:41:10,047 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:10,048 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:10,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:10,055 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,055 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,060 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,064 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,068 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,072 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,072 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:41:10,073 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:10,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:10,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:10,081 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,081 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,116 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,120 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,124 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,128 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,128 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:41:10,128 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:10,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:10,133 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:10,136 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,137 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,142 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,147 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,150 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,154 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,154 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:41:10,154 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:10,155 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:10,159 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:10,163 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,163 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,168 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,171 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,175 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,179 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,179 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:41:10,179 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:10,181 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:10,184 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:10,187 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,188 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,192 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,196 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,200 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,203 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,204 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:41:10,204 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:10,205 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:10,208 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:10,213 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,213 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,231 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,239 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,244 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,247 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,248 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:41:10,248 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:10,249 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:10,253 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:10,253 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,253 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,258 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,262 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,266 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,270 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 10, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 11, 64])"))
2023-10-11 15:41:10,270 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"))
2023-10-11 15:41:10,271 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:10,272 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:10,272 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:10,273 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,273 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:10,274 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,275 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,276 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,276 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:10,277 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:10,277 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:10,277 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:10,278 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,278 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:10,287 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:10,295 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:10,302 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:10,310 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:10,310 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:10,310 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:10,315 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:10,316 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:10,316 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:10,317 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:10,317 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,318 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,318 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,319 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,319 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:10,320 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:10,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:10,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:10,324 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 12])", "<class 'int'>: 11")
2023-10-11 15:41:10,325 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:10,325 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,326 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,327 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,327 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 12])", "<class 'int'>: 11"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,328 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:10,328 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:10,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:10,334 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:10,338 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,338 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,343 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,348 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,353 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,356 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,357 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:41:10,357 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:10,358 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:10,362 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:10,366 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,366 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,371 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,375 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,378 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,382 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,382 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:41:10,382 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:10,384 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:10,387 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:10,391 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,391 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,396 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,400 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,404 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,408 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,408 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:41:10,408 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:10,410 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:10,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:10,416 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,417 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,421 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,426 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,430 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,434 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,434 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:41:10,434 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:10,435 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:10,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:10,443 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,443 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,448 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,452 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,456 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,460 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:41:10,460 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:10,461 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:10,465 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:10,468 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,469 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,474 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,478 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,481 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,485 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,485 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:41:10,485 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:10,487 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:10,490 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:10,494 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,494 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,499 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,503 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,507 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,511 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,511 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:41:10,511 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:10,513 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:10,517 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:10,520 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,521 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,526 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,530 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,534 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,538 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,538 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:41:10,538 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:10,539 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:10,543 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:10,547 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,547 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,552 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,556 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,560 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,564 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,564 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:41:10,564 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:10,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:10,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:10,572 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,572 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,577 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,581 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,585 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,610 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,610 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:41:10,610 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:10,611 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:10,615 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:10,619 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,619 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,629 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,633 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,636 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,640 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,640 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:41:10,640 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:10,642 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:10,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:10,647 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,647 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,651 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,656 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,660 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,664 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 11, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 12, 64])"))
2023-10-11 15:41:10,664 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"))
2023-10-11 15:41:10,664 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:10,666 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:10,666 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:10,667 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,667 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:10,668 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,669 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,669 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,670 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,670 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:10,671 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:10,671 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:10,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:10,672 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,672 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:10,681 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:10,689 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:10,697 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:10,705 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:10,705 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:10,706 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:10,712 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:10,712 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:10,713 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:10,713 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:10,714 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,715 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,715 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,716 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,716 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:10,716 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:10,717 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:10,717 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:10,721 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 13])", "<class 'int'>: 12")
2023-10-11 15:41:10,722 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:10,722 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,723 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,724 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,725 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 13])", "<class 'int'>: 12"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:10,725 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:10,725 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:10,728 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:10,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:10,735 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,735 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,741 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,745 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,749 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,755 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:41:10,755 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:10,757 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:10,762 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:10,766 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,766 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,772 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,780 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,784 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,784 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:41:10,784 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:10,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:10,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:10,794 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,794 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,799 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,803 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,806 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,811 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,811 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:41:10,811 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:10,813 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:10,816 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:10,823 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,823 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,844 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,853 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,861 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,866 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,866 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:41:10,866 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:10,868 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:10,872 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:10,876 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,876 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,882 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,887 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,892 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,897 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,897 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:41:10,897 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:10,899 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:10,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:10,906 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,906 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,912 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,918 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,922 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,927 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:41:10,927 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:10,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:10,932 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:10,936 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,936 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,947 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,952 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,957 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,957 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:41:10,957 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:10,959 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:10,962 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:10,966 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,966 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:10,972 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,977 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,981 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,986 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:10,986 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:41:10,986 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:10,988 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:10,991 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:10,995 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:10,995 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,001 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,011 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,016 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,016 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:41:11,017 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:11,018 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:11,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:11,026 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,026 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,037 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,042 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,046 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,047 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:41:11,047 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:11,048 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:11,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:11,055 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,056 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,061 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,066 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,070 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,074 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,074 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:41:11,074 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:11,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:11,079 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:11,080 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,080 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,085 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,120 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,128 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,132 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 12, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 13, 64])"))
2023-10-11 15:41:11,132 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"))
2023-10-11 15:41:11,132 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:11,134 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:11,135 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:11,135 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,135 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:11,136 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,137 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,137 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,138 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,138 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:11,139 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:11,139 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:11,140 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:11,140 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,140 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:11,149 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:11,157 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:11,165 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:11,172 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:11,172 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:11,173 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:11,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:11,179 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:11,179 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:11,180 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:11,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,181 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,182 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,182 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,182 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:11,183 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:11,183 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:11,184 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:11,187 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 14])", "<class 'int'>: 13")
2023-10-11 15:41:11,188 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:11,188 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,189 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,190 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,190 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 14])", "<class 'int'>: 13"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,191 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:11,191 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:11,194 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:11,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:11,201 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,201 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,211 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,215 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,219 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,219 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:41:11,219 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:11,220 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:11,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:11,227 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,228 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,234 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,238 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,242 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,246 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,246 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:41:11,246 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:11,248 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:11,251 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:11,254 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,255 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,260 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,264 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,268 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,272 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,272 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:41:11,272 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:11,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:11,277 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:11,280 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,280 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,285 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,289 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,293 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,297 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,297 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:41:11,298 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:11,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:11,302 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:11,306 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,306 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,311 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,315 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,319 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,336 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,336 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:41:11,336 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:11,338 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:11,341 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:11,345 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,345 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,350 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,354 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,357 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,361 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,361 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:41:11,361 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:11,363 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:11,366 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:11,370 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,370 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,375 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,379 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,384 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,387 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,388 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:41:11,388 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:11,389 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:11,393 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:11,396 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,396 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,401 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,405 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,409 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,413 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,413 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:41:11,413 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:11,414 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:11,418 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:11,421 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,422 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,426 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,430 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,434 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,438 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,438 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:41:11,438 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:11,440 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:11,443 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:11,447 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,447 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,452 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,456 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,463 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,463 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:41:11,464 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:11,465 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:11,468 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:11,472 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,472 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,477 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,481 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,485 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,489 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,490 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:41:11,490 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:11,491 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:11,494 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:11,495 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,495 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,500 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,504 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,508 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,512 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 13, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 14, 64])"))
2023-10-11 15:41:11,512 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"))
2023-10-11 15:41:11,512 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:11,514 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:11,514 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:11,515 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,515 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:11,516 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,516 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,517 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,518 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,519 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:11,519 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:11,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:11,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:11,520 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,520 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:11,533 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:11,541 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:11,549 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:11,556 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:11,557 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:11,557 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:11,563 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:11,563 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:11,564 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:11,564 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:11,565 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,566 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,566 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,567 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,567 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:11,568 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:11,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:11,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:11,573 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 15])", "<class 'int'>: 14")
2023-10-11 15:41:11,573 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:11,574 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,575 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,575 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,576 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 15])", "<class 'int'>: 14"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,576 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:11,576 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:11,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:11,582 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:11,586 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,586 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,612 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,620 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,624 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,624 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:41:11,624 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:11,626 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:11,629 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:11,632 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,633 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,638 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,642 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,649 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,649 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:41:11,650 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:11,651 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:11,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:11,658 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,658 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,663 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,671 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,675 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,675 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:41:11,675 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:11,677 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:11,680 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:11,683 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,684 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,689 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,693 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,701 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,701 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:41:11,701 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:11,703 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:11,706 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:11,710 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,710 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,715 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,723 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,727 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,727 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:41:11,727 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:11,729 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:11,732 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:11,736 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,736 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,742 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,746 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,750 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,754 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:41:11,754 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:11,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:11,758 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:11,762 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,762 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,768 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,772 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,775 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,779 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,779 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:41:11,780 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:11,781 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:11,784 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:11,788 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,788 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,793 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,797 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,801 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,805 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,805 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:41:11,805 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:11,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:11,809 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:11,813 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,813 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,831 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,835 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,839 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,842 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,843 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:41:11,843 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:11,844 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:11,847 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:11,851 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,851 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,856 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,861 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,869 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,869 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:41:11,869 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:11,870 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:11,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:11,877 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,877 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,882 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,886 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,890 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,894 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,894 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:41:11,894 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:11,895 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:11,899 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:11,899 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,899 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,905 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,908 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,912 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,917 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 14, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 15, 64])"))
2023-10-11 15:41:11,917 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"))
2023-10-11 15:41:11,917 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:11,918 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:11,919 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:11,919 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,919 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:11,920 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,921 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,922 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,922 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,922 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:11,923 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:11,923 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:11,923 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:11,924 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,924 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:11,933 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:11,940 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:11,948 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:11,955 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:11,955 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:11,955 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:11,961 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:11,961 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:11,962 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:11,962 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:11,962 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,963 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,964 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,964 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,964 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:11,964 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:11,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:11,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:11,969 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 16])", "<class 'int'>: 15")
2023-10-11 15:41:11,969 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:11,970 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,970 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,971 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,972 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 16])", "<class 'int'>: 15"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:11,972 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:11,972 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:11,975 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:11,978 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:11,982 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:11,982 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:11,987 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:11,991 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:11,995 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:11,998 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:11,999 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:41:11,999 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:12,000 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:12,003 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:12,007 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,007 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,012 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,021 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,025 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,029 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,029 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:41:12,029 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:12,031 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:12,034 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:12,038 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,038 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,043 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,047 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,051 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,055 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,055 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:41:12,055 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:12,057 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:12,060 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:12,064 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,064 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,069 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,073 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,103 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,110 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,110 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:41:12,110 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:12,112 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:12,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:12,119 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,120 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,125 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,129 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,133 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,137 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,137 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:41:12,137 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:12,139 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:12,142 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:12,146 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,146 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,151 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,161 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,164 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,168 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,169 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:41:12,169 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:12,170 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:12,173 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:12,177 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,177 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,182 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,186 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,189 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,193 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,193 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:41:12,193 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:12,195 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:12,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:12,201 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,202 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,207 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,211 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,218 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,218 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:41:12,218 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:12,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:12,222 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:12,226 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,226 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,233 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,237 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,241 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,244 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,245 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:41:12,245 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:12,246 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:12,249 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:12,253 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,253 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,258 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,262 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,265 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,269 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,269 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:41:12,269 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:12,270 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:12,274 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:12,277 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,278 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,283 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,286 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,304 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,308 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,308 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:41:12,308 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:12,309 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:12,313 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:12,313 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,314 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,337 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,341 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,345 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,349 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 15, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 16, 64])"))
2023-10-11 15:41:12,350 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"))
2023-10-11 15:41:12,350 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:12,351 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:12,352 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:12,353 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,353 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:12,354 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,355 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,356 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,357 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,357 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:12,358 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:12,358 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:12,359 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:12,359 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,359 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:12,369 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:12,377 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:12,385 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:12,392 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:12,393 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:12,393 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:12,401 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:12,401 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:12,402 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:12,402 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:12,403 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,404 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,404 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,405 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,405 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:12,405 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:12,406 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:12,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:12,411 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 17])", "<class 'int'>: 16")
2023-10-11 15:41:12,411 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:12,412 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,413 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,414 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,415 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 17])", "<class 'int'>: 16"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,415 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:12,415 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:12,419 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:12,422 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:12,426 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,426 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,431 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,435 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,439 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,443 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,443 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:41:12,443 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:12,445 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:12,448 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:12,452 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,452 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,457 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,461 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,465 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,470 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,470 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:41:12,470 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:12,472 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:12,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:12,479 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,479 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,484 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,488 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,492 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,496 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,496 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:41:12,496 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:12,498 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:12,501 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:12,505 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,505 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,510 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,515 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,519 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,522 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,523 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:41:12,523 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:12,524 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:12,527 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:12,531 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,532 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,544 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,552 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,552 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:41:12,552 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:12,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:12,557 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:12,561 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,561 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,590 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,595 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,599 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,603 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,603 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:41:12,603 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:12,605 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:12,608 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:12,612 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,612 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,617 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,621 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,625 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,629 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,630 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:41:12,630 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:12,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:12,634 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:12,638 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,638 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,643 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,649 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,653 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,657 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,657 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:41:12,657 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:12,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:12,662 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:12,666 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,666 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,671 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,675 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,680 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,684 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,684 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:41:12,684 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:12,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:12,689 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:12,692 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,692 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,701 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,705 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,709 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,709 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:41:12,709 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:12,711 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:12,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:12,718 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,718 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,724 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,728 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,732 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,736 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,737 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:41:12,737 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:12,738 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:12,741 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:12,742 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,742 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,748 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,751 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,755 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,760 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 16, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 17, 64])"))
2023-10-11 15:41:12,760 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"))
2023-10-11 15:41:12,760 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:12,761 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:12,762 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:12,762 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,762 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:12,765 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,766 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,766 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,767 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,767 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:12,768 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:12,768 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:12,768 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:12,769 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,769 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:12,778 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:12,785 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:12,798 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:12,810 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:12,811 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:12,811 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:12,818 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:12,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:12,819 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:12,819 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:12,820 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,821 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,821 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,822 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:12,822 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:12,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:12,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:12,827 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 18])", "<class 'int'>: 17")
2023-10-11 15:41:12,827 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:12,828 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,828 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,829 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,830 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 18])", "<class 'int'>: 17"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:12,830 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:12,830 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:12,833 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:12,836 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:12,840 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,840 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,845 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,849 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,853 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,856 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,857 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:41:12,857 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:12,858 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:12,862 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:12,865 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,865 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,871 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,875 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,879 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,883 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,883 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:41:12,883 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:12,885 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:12,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:12,891 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,892 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,896 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,900 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,904 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,908 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,908 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:41:12,909 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:12,910 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:12,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:12,917 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,917 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,923 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,931 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,935 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,935 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:41:12,935 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:12,937 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:12,940 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:12,944 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,944 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,949 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,953 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,957 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,962 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,962 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:41:12,962 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:12,964 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:12,967 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:12,970 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,971 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:12,976 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,980 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,984 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,987 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:12,987 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:41:12,988 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:12,989 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:12,992 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:12,996 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:12,996 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,001 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,013 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,017 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,017 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:41:13,017 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:13,019 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:13,023 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:13,026 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,026 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,031 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,036 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,039 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,043 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,044 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:41:13,044 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:13,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:13,048 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:13,052 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,052 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,067 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,082 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,090 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,097 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:41:13,097 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:13,099 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:13,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:13,106 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,106 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,112 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,116 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,120 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,124 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,125 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:41:13,125 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:13,126 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:13,129 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:13,133 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,134 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,138 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,143 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,146 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,150 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,151 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:41:13,151 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:13,152 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:13,156 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:13,157 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,157 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,162 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,166 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,170 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,176 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 17, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 18, 64])"))
2023-10-11 15:41:13,176 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"))
2023-10-11 15:41:13,176 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:13,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:13,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:13,179 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,179 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:13,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,181 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,182 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,182 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:13,182 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:13,183 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:13,183 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:13,183 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,184 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:13,194 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:13,201 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:13,209 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:13,217 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:13,218 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:13,218 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:13,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:13,225 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:13,225 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:13,225 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:13,226 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,227 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,228 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,228 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,228 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:13,229 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:13,229 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:13,229 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:13,233 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 19])", "<class 'int'>: 18")
2023-10-11 15:41:13,233 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:13,234 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,235 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,236 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,236 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 19])", "<class 'int'>: 18"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,236 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:13,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:13,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:13,243 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:13,247 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,247 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,256 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,281 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,286 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,290 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,290 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:41:13,291 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:13,292 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:13,295 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:13,299 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,299 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,316 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,321 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,326 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,330 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,330 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:41:13,330 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:13,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:13,335 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:13,339 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,339 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,344 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,350 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,359 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,363 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,363 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:41:13,364 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:13,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:13,368 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:13,372 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,373 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,378 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,385 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,390 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,395 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,395 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:41:13,395 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:13,396 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:13,400 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:13,404 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,404 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,410 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,414 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,418 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,424 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,425 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:41:13,425 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:13,426 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:13,430 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:13,434 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,434 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,440 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,449 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,453 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,453 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:41:13,453 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:13,454 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:13,459 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:13,463 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,463 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,468 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,473 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,477 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,482 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,482 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:41:13,482 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:13,484 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:13,487 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:13,491 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,491 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,498 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,505 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,509 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,514 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,514 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:41:13,514 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:13,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:13,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:13,523 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,523 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,529 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,535 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,545 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,545 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:41:13,545 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:13,547 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:13,550 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:13,554 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,554 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,560 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,590 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,595 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,604 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,605 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:41:13,605 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:13,606 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:13,610 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:13,614 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,614 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,619 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,624 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,628 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,633 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,633 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:41:13,633 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:13,635 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:13,638 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:13,639 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,639 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,649 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,654 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,658 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 18, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 19, 64])"))
2023-10-11 15:41:13,658 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"))
2023-10-11 15:41:13,659 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:13,660 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:13,661 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:13,661 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,661 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:13,662 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,663 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,664 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,665 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,665 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:13,665 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:13,666 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:13,666 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:13,666 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,667 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:13,676 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:13,685 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:13,693 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:13,700 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:13,701 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:13,701 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:13,707 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:13,707 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:13,708 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:13,708 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:13,708 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,709 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,710 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,710 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,710 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:13,711 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:13,711 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:13,712 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:13,716 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 20])", "<class 'int'>: 19")
2023-10-11 15:41:13,716 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:13,717 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,717 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,718 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 20])", "<class 'int'>: 19"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:13,719 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:13,719 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:13,723 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:13,726 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:13,731 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,731 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,738 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,744 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,749 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,753 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:41:13,754 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:13,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:13,759 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:13,763 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,763 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,768 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,773 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,777 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,782 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,782 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:41:13,782 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:13,783 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:13,787 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:13,792 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,792 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,798 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,802 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,820 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,825 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,825 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:41:13,825 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:13,826 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:13,830 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:13,834 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,834 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,839 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,843 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,847 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,851 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,851 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:41:13,851 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:13,853 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:13,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:13,860 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,860 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,870 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,873 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,877 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,877 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:41:13,877 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:13,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:13,882 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:13,885 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,886 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,891 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,894 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,898 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,902 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,902 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:41:13,902 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:13,904 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:13,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:13,911 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,911 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,915 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,920 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,923 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,927 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:41:13,927 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:13,929 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:13,932 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:13,936 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,936 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,941 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,945 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,949 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,954 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,954 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:41:13,954 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:13,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:13,958 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:13,962 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,963 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,968 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,972 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,976 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,980 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,980 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:41:13,980 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:13,982 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:13,985 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:13,988 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:13,989 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:13,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:13,998 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:14,002 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:14,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:14,006 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:41:14,006 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:14,007 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:14,010 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:14,014 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,014 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,022 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:14,026 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:14,030 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:14,033 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:14,034 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:41:14,034 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:14,035 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:14,038 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:14,039 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,039 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,045 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:14,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:14,054 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:14,058 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 19, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 20, 64])"))
2023-10-11 15:41:14,058 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"))
2023-10-11 15:41:14,058 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:14,059 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:14,060 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:14,060 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,061 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:14,063 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,064 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,065 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,066 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,066 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:14,066 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:14,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:14,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:14,067 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,067 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:14,086 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:14,097 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:14,107 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:14,114 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:14,115 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:14,115 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:14,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:14,121 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:14,121 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:14,121 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:14,122 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,123 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,123 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,124 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,124 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:14,124 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:14,125 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:14,125 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:14,129 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 21])", "<class 'int'>: 20")
2023-10-11 15:41:14,129 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:14,130 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,130 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,131 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,132 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 21])", "<class 'int'>: 20"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,132 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:14,132 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:14,135 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:14,139 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:14,142 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,143 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,148 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,152 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,156 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,163 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,163 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:41:14,163 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:14,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:14,168 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:14,171 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,171 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,176 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,180 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,184 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,188 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,188 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:41:14,188 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:14,190 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:14,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:14,197 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,197 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,202 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,210 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,214 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,214 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:41:14,214 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:14,215 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:14,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:14,222 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,222 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,228 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,233 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,237 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,241 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,242 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:41:14,242 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:14,243 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:14,246 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:14,250 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,250 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,255 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,260 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,264 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,268 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,269 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:41:14,269 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:14,270 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:14,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:14,277 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,277 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,282 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,286 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,290 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,294 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,294 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:41:14,294 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:14,295 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:14,298 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:14,302 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,302 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,322 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,326 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,331 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,335 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,335 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:41:14,335 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:14,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:14,340 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:14,343 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,343 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,348 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,352 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,356 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,361 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,361 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:41:14,361 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:14,362 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:14,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:14,369 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,369 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,375 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,379 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,383 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,387 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,387 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:41:14,387 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:14,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:14,392 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:14,395 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,396 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,401 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,405 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,409 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,413 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,413 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:41:14,413 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:14,414 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:14,418 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:14,421 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,421 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,429 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,433 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,436 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,440 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,440 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:41:14,441 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:14,442 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:14,445 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:14,446 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,446 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,451 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,455 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,459 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,463 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 20, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 21, 64])"))
2023-10-11 15:41:14,463 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"))
2023-10-11 15:41:14,464 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:14,465 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:14,465 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:14,466 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,466 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:14,466 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,467 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,468 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,469 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,469 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:14,469 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:14,470 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:14,470 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:14,471 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,471 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:14,479 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:14,487 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:14,495 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:14,502 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:14,502 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:14,502 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:14,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:14,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:14,508 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:14,509 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:14,509 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,510 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,510 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,511 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,511 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:14,511 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:14,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:14,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:14,516 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 22])", "<class 'int'>: 21")
2023-10-11 15:41:14,516 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:14,517 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,517 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,518 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,519 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 22])", "<class 'int'>: 21"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,519 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:14,519 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:14,522 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:14,525 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:14,529 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,529 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,535 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,539 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,544 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,548 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:41:14,548 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:14,549 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:14,553 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:14,556 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,556 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,561 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,566 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,570 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,604 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,605 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:41:14,605 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:14,606 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:14,610 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:14,613 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,613 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,619 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,623 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,627 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,631 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,631 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:41:14,631 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:14,632 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:14,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:14,639 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,639 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,649 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,654 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,658 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,658 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:41:14,658 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:14,660 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:14,663 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:14,666 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,667 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,672 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,676 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,680 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,685 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,685 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:41:14,685 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:14,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:14,692 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:14,696 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,696 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,701 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,707 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,712 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,716 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,716 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:41:14,716 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:14,717 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:14,721 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:14,725 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,725 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,730 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,735 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,739 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,742 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,743 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:41:14,743 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:14,744 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:14,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:14,752 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,752 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,757 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,761 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,765 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,769 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,770 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:41:14,770 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:14,771 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:14,774 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:14,778 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,778 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,783 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,787 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,791 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,797 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,797 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:41:14,797 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:14,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:14,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:14,806 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,806 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,823 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,827 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,832 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,836 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,836 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:41:14,836 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:14,837 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:14,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:14,844 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,845 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,851 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,855 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,859 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,863 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,864 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:41:14,864 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:14,865 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:14,869 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:14,869 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,869 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,875 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,879 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,883 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,888 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 21, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 22, 64])"))
2023-10-11 15:41:14,888 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"))
2023-10-11 15:41:14,888 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:14,889 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:14,890 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:14,890 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,891 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:14,891 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,892 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,893 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,894 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,894 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:14,894 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:14,895 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:14,895 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:14,895 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,896 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:14,909 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:14,918 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:14,925 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:14,934 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:14,934 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:14,934 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:14,939 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:14,940 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:14,940 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:14,940 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:14,941 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,942 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,943 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,943 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,943 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:14,943 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:14,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:14,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:14,948 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 23])", "<class 'int'>: 22")
2023-10-11 15:41:14,948 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:14,949 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,950 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,950 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 23])", "<class 'int'>: 22"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:14,951 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:14,951 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:14,954 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:14,958 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:14,961 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,961 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,967 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:14,971 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:14,975 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:14,979 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:14,980 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:41:14,980 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:14,981 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:14,985 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:14,988 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:14,989 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:14,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:14,998 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,003 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,013 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,013 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:41:15,013 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:15,015 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:15,018 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:15,021 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,022 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,028 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,032 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,037 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,041 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,041 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:41:15,041 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:15,042 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:15,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:15,049 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,050 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,055 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,059 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,063 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,067 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,067 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:41:15,067 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:15,069 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:15,072 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:15,076 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,076 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,098 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,104 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,109 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,113 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,113 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:41:15,113 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:15,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:15,118 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:15,122 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,122 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,127 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,131 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,136 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,140 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,140 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:41:15,140 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:15,141 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:15,144 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:15,148 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,148 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,154 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,158 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,162 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,167 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,167 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:41:15,167 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:15,169 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:15,172 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:15,175 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,176 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,182 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,186 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,190 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,194 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,195 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:41:15,195 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:15,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:15,199 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:15,203 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,203 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,209 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,213 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,218 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,223 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,223 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:41:15,223 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:15,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:15,228 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:15,231 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,232 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,237 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,242 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,246 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,250 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,251 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:41:15,251 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:15,252 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:15,255 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:15,259 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,259 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,265 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,269 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,274 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,278 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,278 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:41:15,278 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:15,280 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:15,283 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:15,284 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,284 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,289 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,294 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,298 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,302 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 22, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 23, 64])"))
2023-10-11 15:41:15,303 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"))
2023-10-11 15:41:15,303 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:15,304 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:15,304 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:15,305 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,305 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:15,307 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,310 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,315 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,319 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,319 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:15,319 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:15,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:15,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:15,321 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,321 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:15,332 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:15,340 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:15,348 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:15,355 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:15,356 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:15,356 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:15,362 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:15,362 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:15,363 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:15,363 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:15,363 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,364 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,365 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,366 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,366 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:15,366 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:15,366 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:15,367 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:15,371 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 24])", "<class 'int'>: 23")
2023-10-11 15:41:15,371 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:15,372 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,372 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,373 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,374 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 24])", "<class 'int'>: 23"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,374 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:15,374 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:15,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:15,380 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:15,384 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,384 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,389 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,394 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,398 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,402 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,402 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:41:15,402 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:15,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:15,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:15,411 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,411 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,416 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,423 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,427 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,431 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,432 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:41:15,432 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:15,433 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:15,436 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:15,440 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,440 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,446 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,450 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,454 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,458 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,459 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:41:15,459 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:15,460 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:15,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:15,467 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,467 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,472 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,477 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,482 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,486 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,486 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:41:15,486 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:15,487 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:15,491 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:15,494 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,495 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,500 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,510 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,515 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,521 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,521 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:41:15,521 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:15,523 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:15,526 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:15,530 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,530 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,536 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,544 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,548 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,549 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:41:15,549 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:15,550 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:15,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:15,558 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,558 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,563 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,568 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,591 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,595 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,595 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:41:15,595 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:15,597 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:15,600 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:15,604 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,604 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,609 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,614 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,618 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,623 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,623 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:41:15,623 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:15,624 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:15,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:15,631 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,631 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,636 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,641 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,649 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,649 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:41:15,650 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:15,651 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:15,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:15,658 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,658 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,663 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,671 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,689 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,689 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:41:15,689 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:15,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:15,694 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:15,698 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,698 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,711 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,715 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,719 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,725 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,725 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:41:15,725 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:15,726 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:15,730 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:15,730 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,731 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,736 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,740 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,744 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,749 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 23, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 24, 64])"))
2023-10-11 15:41:15,749 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"))
2023-10-11 15:41:15,749 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:15,750 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:15,751 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:15,752 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,752 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:15,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,755 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,755 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:15,755 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:15,756 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:15,756 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:15,757 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,757 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:15,768 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:15,780 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:15,790 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:15,800 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:15,801 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:15,801 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:15,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:15,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:15,820 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:15,820 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:15,821 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,821 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,823 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,823 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:15,823 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:15,824 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:15,824 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:15,828 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 25])", "<class 'int'>: 24")
2023-10-11 15:41:15,828 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:15,829 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,830 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,830 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,831 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 25])", "<class 'int'>: 24"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:15,831 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:15,832 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:15,835 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:15,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:15,842 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,842 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,849 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,854 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,860 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,865 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,865 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:41:15,866 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:15,868 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:15,871 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:15,875 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,875 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,882 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,888 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,894 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,904 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,904 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:41:15,904 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:15,905 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:15,909 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:15,913 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,914 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,930 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,939 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,950 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,955 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,955 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:41:15,955 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:15,957 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:15,960 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:15,964 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,964 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:15,970 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,976 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,980 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,985 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:15,985 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:41:15,985 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:15,986 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:15,990 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:15,994 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:15,994 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,000 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,014 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,019 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,023 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,024 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:41:16,024 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:16,026 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:16,029 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:16,033 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,033 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,039 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,045 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,051 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,057 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,057 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:41:16,057 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:16,058 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:16,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:16,066 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,066 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,090 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,094 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,099 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,108 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,109 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:41:16,109 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:16,111 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:16,114 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:16,117 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,118 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,123 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,127 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,132 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,136 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,136 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:41:16,136 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:16,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:16,140 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:16,144 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,144 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,149 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,153 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,157 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,162 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,162 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:41:16,162 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:16,163 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:16,167 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:16,170 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,171 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,176 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,181 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,185 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,189 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,189 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:41:16,189 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:16,191 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:16,194 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:16,198 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,198 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,203 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,207 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,211 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,216 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,216 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:41:16,216 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:16,218 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:16,221 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:16,221 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,221 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,227 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,231 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,236 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,240 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 24, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 25, 64])"))
2023-10-11 15:41:16,241 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"))
2023-10-11 15:41:16,241 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:16,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:16,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:16,243 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,243 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:16,244 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,244 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,245 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,246 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,246 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:16,246 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:16,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:16,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:16,248 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,248 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:16,258 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:16,266 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:16,274 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:16,282 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:16,283 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:16,283 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:16,289 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:16,290 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:16,291 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:16,291 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:16,292 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,292 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,293 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,294 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,294 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:16,294 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:16,295 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:16,295 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:16,299 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 26])", "<class 'int'>: 25")
2023-10-11 15:41:16,299 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:16,300 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,301 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,301 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,302 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 26])", "<class 'int'>: 25"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,302 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:16,302 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:16,305 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:16,308 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:16,312 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,313 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,328 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,332 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,337 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,341 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,342 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:41:16,342 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:16,343 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:16,346 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:16,350 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,350 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,356 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,360 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,364 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,368 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,368 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:41:16,368 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:16,369 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:16,372 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:16,376 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,376 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,381 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,386 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,391 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,395 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,395 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:41:16,396 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:16,397 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:16,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:16,406 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,406 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,411 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,417 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,422 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,426 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,426 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:41:16,427 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:16,428 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:16,431 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:16,435 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,435 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,441 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,445 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,450 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,454 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,454 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:41:16,454 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:16,456 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:16,459 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:16,462 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,463 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,468 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,473 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,477 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,482 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,482 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:41:16,483 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:16,484 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:16,488 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:16,492 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,492 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,498 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,502 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,507 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,522 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,523 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:41:16,523 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:16,524 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:16,528 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:16,532 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,532 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,537 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,542 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,547 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,551 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,551 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:41:16,551 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:16,552 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:16,556 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:16,560 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,560 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,565 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,570 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,589 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,594 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,595 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:41:16,595 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:16,597 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:16,600 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:16,604 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,604 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,611 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,617 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,622 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,627 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,628 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:41:16,628 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:16,629 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:16,633 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:16,638 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,638 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,650 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,656 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,661 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,661 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:41:16,661 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:16,663 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:16,667 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:16,667 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,667 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,673 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,677 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,682 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,687 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 25, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 26, 64])"))
2023-10-11 15:41:16,688 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"))
2023-10-11 15:41:16,688 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:16,689 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:16,690 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:16,690 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,690 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:16,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,692 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,692 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,693 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,693 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:16,694 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:16,694 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:16,694 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:16,695 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,695 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:16,706 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:16,715 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:16,725 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:16,733 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:16,734 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:16,734 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:16,742 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:16,742 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:16,743 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:16,743 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:16,744 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,745 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,745 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,746 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,746 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:16,746 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:16,747 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:16,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:16,752 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 27])", "<class 'int'>: 26")
2023-10-11 15:41:16,752 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:16,752 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,753 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,754 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 27])", "<class 'int'>: 26"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:16,754 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:16,755 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:16,757 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:16,761 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:16,766 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,766 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,776 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,780 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,785 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,785 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:41:16,785 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:16,787 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:16,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:16,794 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,794 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,800 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,804 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,809 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,828 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,828 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:41:16,828 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:16,830 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:16,833 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:16,837 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,837 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,843 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,847 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,851 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,855 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,855 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:41:16,856 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:16,857 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:16,861 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:16,864 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,865 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,870 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,875 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,879 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,883 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,883 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:41:16,884 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:16,885 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:16,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:16,892 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,893 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,898 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,902 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,907 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,912 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,912 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:41:16,912 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:16,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:16,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:16,921 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,921 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,932 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,937 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,941 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,941 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:41:16,941 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:16,943 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:16,948 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:16,953 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,953 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,960 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,965 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,971 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,978 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,978 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:41:16,978 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:16,980 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:16,983 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:16,987 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:16,987 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:16,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:16,998 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,003 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,007 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,007 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:41:17,008 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:17,009 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:17,012 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:17,017 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,017 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,023 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,027 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,031 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,035 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,036 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:41:17,036 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:17,037 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:17,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:17,045 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,045 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,050 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,055 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,059 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,069 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,069 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:41:17,070 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:17,071 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:17,075 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:17,079 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,079 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,101 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,108 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,112 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,112 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:41:17,113 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:17,114 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:17,118 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:17,119 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,119 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,125 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,129 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,133 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,370 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 26, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 27, 64])"))
2023-10-11 15:41:17,371 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"))
2023-10-11 15:41:17,371 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:17,372 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:17,373 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:17,373 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,373 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:17,374 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,375 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,376 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,377 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,377 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:17,377 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:17,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:17,378 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:17,378 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,378 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:17,388 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:17,396 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:17,403 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:17,411 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:17,411 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:17,411 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:17,418 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:17,418 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:17,419 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:17,419 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:17,420 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,420 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,421 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,421 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,421 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:17,422 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:17,422 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:17,422 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:17,426 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 28])", "<class 'int'>: 27")
2023-10-11 15:41:17,426 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:17,427 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,427 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,428 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,429 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 28])", "<class 'int'>: 27"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,429 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:17,429 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:17,432 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:17,435 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:17,439 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,439 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,445 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,449 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,453 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,457 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,458 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:41:17,458 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:17,459 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:17,462 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:17,466 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,466 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,471 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,475 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,479 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,484 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,485 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:41:17,485 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:17,486 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:17,489 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:17,493 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,494 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,499 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,523 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,527 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,532 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,532 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:41:17,532 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:17,534 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:17,537 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:17,541 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,541 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,547 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,551 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,556 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,560 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,560 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:41:17,560 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:17,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:17,565 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:17,568 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,569 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,574 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,578 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,583 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,587 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,587 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:41:17,587 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:17,588 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:17,592 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:17,595 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,596 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,601 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,606 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,610 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,614 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,614 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:41:17,615 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:17,616 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:17,619 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:17,623 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,623 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,629 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,633 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,638 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,642 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,642 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:41:17,642 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:17,644 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:17,647 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:17,651 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,651 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,656 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,661 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,665 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,669 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,670 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:41:17,670 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:17,671 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:17,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:17,679 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,679 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,684 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,695 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,700 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,700 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:41:17,700 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:17,702 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:17,705 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:17,709 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,709 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,714 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,718 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,723 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,727 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,727 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:41:17,727 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:17,729 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:17,732 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:17,736 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,737 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,742 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,746 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,750 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,755 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,755 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:41:17,755 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:17,756 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:17,760 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:17,760 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,760 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,784 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,788 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,792 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,796 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 27, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 28, 64])"))
2023-10-11 15:41:17,796 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"))
2023-10-11 15:41:17,796 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:17,797 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:17,798 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:17,798 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,798 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:17,799 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,800 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,801 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,802 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,802 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:17,802 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:17,803 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:17,803 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:17,803 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,804 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:17,823 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:17,836 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:17,843 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:17,851 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:17,852 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:17,852 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:17,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:17,860 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:17,860 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:17,860 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:17,861 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,862 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,863 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,863 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:17,863 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:17,864 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:17,864 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:17,868 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 29])", "<class 'int'>: 28")
2023-10-11 15:41:17,868 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:17,869 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,870 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,871 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,871 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 29])", "<class 'int'>: 28"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:17,871 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:17,872 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:17,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:17,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:17,882 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,882 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,887 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:17,892 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:17,896 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:17,900 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:17,901 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:41:17,901 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:17,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:17,906 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:17,910 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,911 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,916 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:17,928 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:17,938 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:17,943 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:17,943 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:41:17,944 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:17,946 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:17,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:17,954 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,954 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,960 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:17,965 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:17,969 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:17,974 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:17,974 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:41:17,974 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:17,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:17,979 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:17,983 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:17,984 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:17,989 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:17,997 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,001 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,006 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:41:18,007 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:18,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:18,014 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:18,018 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,018 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,024 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,028 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,033 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,037 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,037 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:41:18,037 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:18,039 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:18,043 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:18,047 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,047 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,052 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,057 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,062 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,066 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,067 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:41:18,067 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:18,068 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:18,072 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:18,076 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,076 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,081 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,086 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,090 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,110 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,111 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:41:18,111 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:18,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:18,119 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:18,124 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,125 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,130 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,135 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,140 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,145 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,145 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:41:18,146 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:18,147 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:18,151 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:18,155 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,155 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,160 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,165 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,170 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,174 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,175 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:41:18,175 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:18,177 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:18,180 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:18,184 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,185 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,190 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,195 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,199 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,203 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,203 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:41:18,203 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:18,205 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:18,208 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:18,212 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,213 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,221 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,226 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,231 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,235 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,236 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:41:18,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:18,237 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:18,241 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:18,241 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,242 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,272 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,277 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,281 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,286 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 28, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 29, 64])"))
2023-10-11 15:41:18,286 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"))
2023-10-11 15:41:18,286 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:18,287 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:18,288 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:18,288 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,288 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:18,289 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,290 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,291 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,292 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,292 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:18,292 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:18,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:18,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:18,293 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,294 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:18,303 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:18,311 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:18,318 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:18,327 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:18,327 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:18,327 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:18,334 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:18,334 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:18,335 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:18,335 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:18,336 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,337 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,338 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,339 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,339 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:18,340 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:18,340 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:18,341 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:18,346 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 30])", "<class 'int'>: 29")
2023-10-11 15:41:18,346 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:18,347 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,348 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,349 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,350 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 30])", "<class 'int'>: 29"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,350 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:18,350 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:18,354 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:18,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:18,364 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,364 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,370 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,375 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,381 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,387 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,387 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:41:18,387 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:18,389 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:18,393 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:18,397 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,397 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,403 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,408 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,430 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,434 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,435 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:41:18,435 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:18,436 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:18,440 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:18,444 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,444 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,449 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,453 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,458 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,462 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,462 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:41:18,463 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:18,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:18,468 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:18,471 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,472 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,488 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,492 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,496 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,500 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,501 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:41:18,501 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:18,502 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:18,506 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:18,510 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,510 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,515 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,519 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,524 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,528 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,529 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:41:18,529 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:18,530 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:18,534 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:18,538 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,538 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,544 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,551 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,556 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,561 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,561 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:41:18,562 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:18,563 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:18,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:18,570 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,570 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,575 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,581 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,585 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,591 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,591 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:41:18,591 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:18,593 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:18,596 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:18,600 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,600 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,605 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,610 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,614 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,619 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,619 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:41:18,619 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:18,621 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:18,624 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:18,628 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,628 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,633 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,639 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,644 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,649 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,649 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:41:18,649 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:18,650 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:18,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:18,658 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,658 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,663 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,667 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,672 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,676 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,676 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:41:18,677 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:18,678 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:18,681 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:18,686 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,686 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,691 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,696 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,700 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,704 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,705 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:41:18,705 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:18,706 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:18,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:18,710 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,711 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,716 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,720 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,725 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,729 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 29, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 30, 64])"))
2023-10-11 15:41:18,729 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"))
2023-10-11 15:41:18,729 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:18,730 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:18,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:18,731 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,732 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:18,732 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,736 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,738 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,741 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,742 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:18,743 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:18,743 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:18,743 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:18,744 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,744 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:18,758 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:18,770 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:18,779 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:18,787 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:18,788 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:18,788 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:18,795 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:18,796 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:18,796 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:18,797 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:18,797 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,798 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,799 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,800 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,800 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:18,800 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:18,801 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:18,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:18,806 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 31])", "<class 'int'>: 30")
2023-10-11 15:41:18,807 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:18,808 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,808 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,809 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,810 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 31])", "<class 'int'>: 30"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:18,810 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:18,810 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:18,813 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:18,816 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:18,820 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,821 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,826 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,843 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,848 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,853 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,853 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:41:18,853 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:18,855 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:18,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:18,863 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,863 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,868 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,874 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,879 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,884 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,884 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:41:18,884 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:18,886 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:18,890 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:18,894 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,894 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,899 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,903 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,908 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,912 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,912 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:41:18,912 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:18,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:18,918 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:18,922 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,922 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,927 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,931 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,936 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,940 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,940 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:41:18,940 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:18,942 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:18,946 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:18,950 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,950 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,955 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,960 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,964 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,969 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:18,969 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:41:18,969 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:18,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:18,974 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:18,978 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:18,979 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:18,997 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,001 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,007 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,011 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,012 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:41:19,012 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:19,013 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:19,017 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:19,021 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,022 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,027 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,031 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,036 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,040 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,040 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:41:19,040 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:19,042 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:19,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:19,050 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,050 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,057 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,063 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,067 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,072 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,072 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:41:19,072 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:19,073 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:19,078 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:19,082 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,082 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,088 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,093 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,097 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,101 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,102 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:41:19,102 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:19,104 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:19,107 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:19,111 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,111 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,117 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,121 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,126 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,132 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,132 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:41:19,132 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:19,133 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:19,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:19,142 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,142 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,147 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,152 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,156 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,160 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,161 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:41:19,161 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:19,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:19,166 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:19,167 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,167 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,172 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,177 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,181 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,186 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 30, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 31, 64])"))
2023-10-11 15:41:19,186 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"))
2023-10-11 15:41:19,186 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:19,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:19,188 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:19,188 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,189 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:19,189 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,190 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,191 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,192 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,192 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:19,192 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:19,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:19,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:19,193 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,194 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:19,202 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:19,210 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:19,218 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:19,225 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:19,225 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:19,225 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:19,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:19,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:19,232 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:19,233 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:19,233 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,234 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,234 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,235 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,235 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:19,235 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:19,236 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:19,236 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:19,241 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 32])", "<class 'int'>: 31")
2023-10-11 15:41:19,241 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:19,241 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,242 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,243 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,244 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 32])", "<class 'int'>: 31"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,244 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:19,244 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:19,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:19,251 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:19,255 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,255 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,266 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,272 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,277 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,281 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,282 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:41:19,282 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:19,284 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:19,287 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:19,291 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,291 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,296 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,313 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,318 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,323 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,323 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:41:19,323 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:19,325 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:19,328 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:19,332 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,333 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,338 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,342 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,348 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,352 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,353 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:41:19,353 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:19,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:19,359 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:19,363 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,364 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,369 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,374 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,380 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,384 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,385 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:41:19,385 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:19,386 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:19,390 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:19,394 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,395 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,400 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,405 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,409 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,413 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,414 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:41:19,414 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:19,415 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:19,419 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:19,423 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,423 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,429 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,433 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,438 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,442 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,442 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:41:19,442 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:19,443 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:19,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:19,451 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,451 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,456 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,461 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,465 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,470 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,470 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:41:19,471 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:19,472 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:19,476 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:19,480 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,481 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,493 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,497 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,503 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,508 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,508 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:41:19,509 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:19,510 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:19,514 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:19,518 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,518 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,524 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,528 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,534 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,540 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,540 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:41:19,540 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:19,542 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:19,546 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:19,550 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,551 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,557 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,562 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,568 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,573 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,573 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:41:19,573 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:19,574 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:19,578 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:19,582 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,583 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,588 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,593 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,604 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,609 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,610 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:41:19,610 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:19,611 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:19,615 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:19,615 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,616 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,621 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,626 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,631 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,636 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 31, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 32, 64])"))
2023-10-11 15:41:19,636 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"))
2023-10-11 15:41:19,637 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:19,638 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:19,639 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:19,639 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,639 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:19,640 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,641 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,642 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,643 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,643 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:19,643 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:19,644 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:19,644 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:19,644 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,645 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:19,655 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:19,665 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:19,676 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:19,689 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:19,689 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:19,689 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:19,696 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:19,696 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:19,697 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:19,697 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:19,698 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,698 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,699 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,700 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,700 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:19,700 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:19,700 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:19,701 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:19,704 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 33])", "<class 'int'>: 32")
2023-10-11 15:41:19,705 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:19,705 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,706 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,706 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,707 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 33])", "<class 'int'>: 32"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:19,707 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:19,707 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:19,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:19,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:19,718 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,718 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,724 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,728 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,734 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,755 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,755 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:41:19,755 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:19,757 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:19,761 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:19,765 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,765 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,771 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,779 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,783 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,792 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,792 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:41:19,792 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:19,794 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:19,798 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:19,802 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,802 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,808 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,813 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,819 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,824 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,825 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:41:19,825 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:19,826 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:19,830 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:19,834 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,834 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,840 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,845 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,849 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,854 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,854 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:41:19,854 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:19,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:19,860 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:19,864 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,864 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,870 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,874 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,878 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,882 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,883 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:41:19,883 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:19,884 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:19,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:19,892 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,892 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,901 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,907 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,911 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,916 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,916 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:41:19,916 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:19,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:19,921 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:19,925 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,925 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,930 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,934 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,939 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,943 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,943 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:41:19,943 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:19,945 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:19,949 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:19,952 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,953 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,958 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,964 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,969 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,974 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:19,974 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:41:19,974 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:19,975 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:19,979 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:19,983 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:19,984 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:19,996 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,001 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,010 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,010 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:41:20,011 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:20,013 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:20,016 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:20,021 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,021 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,027 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,031 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,036 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,040 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,040 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:41:20,040 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:20,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:20,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:20,049 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,049 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,055 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,059 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,064 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,069 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,069 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:41:20,069 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:20,071 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:20,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:20,075 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,075 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,081 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,085 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,090 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,094 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 32, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 33, 64])"))
2023-10-11 15:41:20,094 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"))
2023-10-11 15:41:20,094 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:20,096 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:20,097 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:20,098 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,098 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:20,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,101 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,102 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,102 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,103 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:20,103 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:20,103 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:20,104 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:20,104 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,104 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:20,114 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:20,122 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:20,130 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:20,138 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:20,138 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:20,139 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:20,144 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:20,145 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:20,145 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:20,145 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:20,146 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,147 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,147 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,148 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,148 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:20,148 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:20,149 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:20,149 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:20,154 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 34])", "<class 'int'>: 33")
2023-10-11 15:41:20,154 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:20,154 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,155 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,156 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,156 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 34])", "<class 'int'>: 33"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,156 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:20,157 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:20,160 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:20,165 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:20,169 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,169 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,175 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,197 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,204 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,210 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,210 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:41:20,210 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:20,212 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:20,215 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:20,219 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,219 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,225 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,229 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,234 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,240 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,240 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:41:20,241 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:20,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:20,246 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:20,250 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,250 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,268 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,272 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,279 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,284 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,284 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:41:20,284 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:20,286 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:20,289 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:20,294 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,294 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,299 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,305 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,309 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,313 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,314 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:41:20,314 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:20,315 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:20,319 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:20,323 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,324 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,329 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,334 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,339 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,343 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,343 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:41:20,344 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:20,345 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:20,349 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:20,353 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,353 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,360 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,365 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,370 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,375 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,375 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:41:20,375 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:20,376 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:20,380 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:20,385 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,385 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,391 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,395 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,407 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,411 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,412 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:41:20,412 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:20,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:20,417 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:20,422 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,422 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,428 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,432 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,437 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,441 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,441 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:41:20,442 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:20,443 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:20,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:20,451 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,452 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,457 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,462 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,467 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,472 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,472 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:41:20,472 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:20,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:20,477 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:20,482 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,482 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,496 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,500 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,506 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,511 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,511 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:41:20,511 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:20,513 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:20,516 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:20,521 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,521 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,527 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,531 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,538 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,542 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,542 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:41:20,543 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:20,544 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:20,548 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:20,549 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,549 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,554 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,559 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,563 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,567 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 33, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 34, 64])"))
2023-10-11 15:41:20,567 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"))
2023-10-11 15:41:20,568 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:20,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:20,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:20,570 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,570 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:20,571 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,572 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,572 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,573 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,573 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:20,574 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:20,574 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:20,574 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:20,575 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,575 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:20,583 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:20,591 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:20,598 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:20,606 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:20,606 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:20,606 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:20,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:20,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:20,614 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:20,614 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:20,614 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,615 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,616 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,617 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:20,617 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:20,617 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:20,618 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:20,622 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 35])", "<class 'int'>: 34")
2023-10-11 15:41:20,622 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:20,622 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,623 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,624 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,625 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 35])", "<class 'int'>: 34"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:20,625 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:20,625 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:20,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:20,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:20,635 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,635 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,640 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,645 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,650 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,655 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,655 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:41:20,655 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:20,657 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:20,660 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:20,664 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,665 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,670 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,675 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,693 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,697 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,697 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:41:20,698 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:20,699 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:20,703 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:20,707 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,707 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,712 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,717 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,722 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,726 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,726 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:41:20,727 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:20,728 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:20,732 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:20,736 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,736 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,741 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,761 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,765 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,769 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,770 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:41:20,770 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:20,771 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:20,775 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:20,779 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,779 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,784 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,788 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,793 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,797 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,797 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:41:20,797 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:20,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:20,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:20,806 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,806 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,811 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,815 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,820 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,824 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,824 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:41:20,824 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:20,826 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:20,829 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:20,833 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,833 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,840 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,844 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,848 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,852 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,852 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:41:20,852 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:20,854 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:20,857 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:20,861 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,862 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,867 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,871 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,875 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,880 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,880 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:41:20,880 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:20,881 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:20,885 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:20,888 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,889 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,894 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,899 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,904 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,908 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,909 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:41:20,909 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:20,910 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:20,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:20,918 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,918 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,923 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,928 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,932 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,937 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,937 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:41:20,937 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:20,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:20,942 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:20,946 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,946 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,951 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,955 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,960 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,964 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,964 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:41:20,964 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:20,966 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:20,969 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:20,970 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:20,970 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:20,984 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,990 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:20,995 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:21,000 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 34, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 35, 64])"))
2023-10-11 15:41:21,000 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"))
2023-10-11 15:41:21,000 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:21,001 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:21,002 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:21,002 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,003 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:21,003 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,004 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,005 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,006 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,006 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:21,006 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:21,007 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:21,007 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:21,007 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,008 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:21,016 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:21,023 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:21,031 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:21,039 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:21,039 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:21,039 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:21,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:21,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:21,046 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:21,047 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:21,047 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,048 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,049 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,049 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:21,050 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:21,050 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:21,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:21,055 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 36])", "<class 'int'>: 35")
2023-10-11 15:41:21,055 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:21,056 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,056 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,057 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,058 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 36])", "<class 'int'>: 35"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,058 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:21,058 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:21,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:21,064 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:21,068 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,068 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,074 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,079 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,083 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,088 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,088 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:41:21,088 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:21,089 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:21,093 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:21,097 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,097 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,103 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,107 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,111 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,116 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,116 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:41:21,116 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:21,118 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:21,122 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:21,126 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,126 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,131 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,136 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,141 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,145 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,145 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:41:21,145 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:21,147 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:21,151 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:21,154 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,154 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,160 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,164 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,171 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,175 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,175 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:41:21,175 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:21,177 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:21,181 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:21,185 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,185 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,202 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,206 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,211 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,215 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,216 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:41:21,216 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:21,217 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:21,221 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:21,225 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,225 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,231 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,235 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,262 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,267 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,267 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:41:21,268 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:21,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:21,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:21,277 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,277 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,284 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,288 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,293 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,297 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,298 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:41:21,298 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:21,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:21,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:21,307 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,307 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,313 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,317 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,321 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,326 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,326 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:41:21,327 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:21,328 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:21,332 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:21,336 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,336 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,341 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,346 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,351 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,355 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,356 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:41:21,356 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:21,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:21,361 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:21,365 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,365 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,372 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,376 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,381 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,386 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,386 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:41:21,386 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:21,387 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:21,391 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:21,395 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,395 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,401 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,405 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,410 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,414 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,415 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:41:21,415 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:21,416 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:21,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:21,421 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,421 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,426 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,430 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,435 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,439 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 35, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 36, 64])"))
2023-10-11 15:41:21,439 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"))
2023-10-11 15:41:21,440 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:21,441 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:21,442 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:21,442 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,442 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:21,443 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,444 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,445 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,445 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:21,446 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:21,446 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:21,446 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:21,447 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,447 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:21,456 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:21,464 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:21,474 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:21,492 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:21,492 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:21,492 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:21,507 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:21,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:21,508 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:21,509 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:21,509 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,510 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,510 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,511 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,511 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:21,511 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:21,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:21,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:21,516 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 37])", "<class 'int'>: 36")
2023-10-11 15:41:21,516 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:21,517 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,518 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,518 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,519 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 37])", "<class 'int'>: 36"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:21,519 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:21,519 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:21,522 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:21,526 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:21,529 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,530 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,550 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,605 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,631 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,638 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,638 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:41:21,638 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:21,640 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:21,643 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:21,647 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,648 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,653 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,659 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,664 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,668 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,668 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:41:21,669 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:21,670 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:21,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:21,677 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,677 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,684 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,688 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,693 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,711 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,711 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:41:21,711 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:21,713 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:21,717 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:21,722 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,722 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,730 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,736 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,742 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,748 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,749 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:41:21,749 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:21,750 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:21,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:21,758 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,759 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,779 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,784 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,789 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,793 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,793 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:41:21,793 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:21,795 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:21,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:21,803 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,803 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,808 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,813 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,818 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,822 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,822 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:41:21,822 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:21,824 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:21,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:21,832 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,832 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,838 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,843 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,847 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,852 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,852 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:41:21,852 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:21,854 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:21,857 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:21,862 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,862 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,867 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,872 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,889 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,893 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,893 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:41:21,893 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:21,894 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:21,898 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:21,902 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,902 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,908 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,912 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,920 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,924 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,924 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:41:21,925 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:21,926 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:21,929 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:21,934 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,934 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,940 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,944 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,950 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,979 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,979 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:41:21,980 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:21,981 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:21,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:21,988 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:21,988 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:21,994 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:21,998 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:22,003 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:22,007 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:22,007 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:41:22,007 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:22,009 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:22,013 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:22,013 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,013 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:22,019 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:22,037 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:22,042 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:22,047 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 36, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 37, 64])"))
2023-10-11 15:41:22,047 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"))
2023-10-11 15:41:22,047 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:22,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:22,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:22,050 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,050 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:22,051 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,052 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,053 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,053 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,054 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:22,054 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:22,054 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:22,055 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:22,055 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,055 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:22,065 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:22,073 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:22,081 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:22,088 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:22,089 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:22,089 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:22,095 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:22,096 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:22,097 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 15:41:22,097 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:22,098 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,099 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,099 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,100 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,100 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:22,100 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 15:41:22,101 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 15:41:22,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:22,106 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 38])", "<class 'int'>: 37")
2023-10-11 15:41:22,106 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:22,107 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,108 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,109 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,110 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 38])", "<class 'int'>: 37"), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,110 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:22,110 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 15:41:22,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 15:41:22,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:22,122 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,122 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:22,130 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,134 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,151 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,155 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,156 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:41:22,156 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 15:41:22,157 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 15:41:22,161 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:22,165 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,165 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:22,171 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,176 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,183 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,187 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,187 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:41:22,188 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 15:41:22,189 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 15:41:22,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:22,197 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,197 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:22,202 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,208 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,212 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,217 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,217 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:41:22,217 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 15:41:22,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 15:41:22,222 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:22,226 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,226 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:22,233 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,237 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,243 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,248 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,248 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:41:22,248 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 15:41:22,250 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 15:41:22,254 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:22,258 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,259 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:22,264 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,269 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,274 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,279 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,279 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:41:22,280 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 15:41:22,282 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 15:41:22,285 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:22,289 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,289 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:22,295 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,299 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,304 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,309 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,309 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:41:22,309 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 15:41:22,310 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 15:41:22,314 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:22,318 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,319 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:22,324 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,329 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,333 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,338 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,338 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:41:22,338 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 15:41:22,340 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 15:41:22,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:22,348 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,348 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:22,353 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,364 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,369 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,373 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,374 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:41:22,374 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 15:41:22,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 15:41:22,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:22,383 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,383 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:22,399 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,403 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,408 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,413 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,413 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:41:22,413 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 15:41:22,414 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 15:41:22,418 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:22,422 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,422 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:22,428 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,432 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,437 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,442 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,442 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:41:22,443 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 15:41:22,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 15:41:22,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:22,451 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,452 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:22,457 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,462 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,466 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,471 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,471 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:41:22,471 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 15:41:22,473 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 15:41:22,477 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:22,477 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,477 [forward.py:85 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 15:41:22,488 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,492 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,498 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,503 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])", "<class 'torch.Tensor'>: torch.Size([2, 12, 37, 64])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])", ("<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])", "<class 'mixtensor.MixTensor'>: torch.Size([2, 12, 38, 64])"))
2023-10-11 15:41:22,504 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 64])"))
2023-10-11 15:41:22,504 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 15:41:22,505 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 15:41:22,506 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:22,506 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,506 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:22,507 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,508 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,508 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,510 [forward.py:104 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 768])
2023-10-11 15:41:22,510 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])
2023-10-11 15:41:22,510 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 15:41:22,510 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:41:22,511 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 15:41:22,511 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 768])",)
2023-10-11 15:41:22,511 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:41:22,520 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:22,528 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:22,536 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:22,544 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 1, 768])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 1, 50272])
2023-10-11 15:41:22,544 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 50272])
2023-10-11 15:41:22,544 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:41:22,551 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-11 15:41:22,552 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:41:22,552 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-11 15:41:22,552 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:41:22,552 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-11 15:41:22,552 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:41:22,552 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-11 15:41:22,552 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:41:22,552 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-11 15:41:22,552 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:41:22,552 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-11 15:41:22,553 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:41:22,553 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-11 15:41:22,553 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:41:22,553 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-11 15:41:22,553 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:41:22,561 [forward.py:23 in reset_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-11 15:41:22,562 [forward.py:23 in reset_forward] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-11 15:41:22,562 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-11 15:41:22,562 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-11 15:41:22,562 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-11 15:41:22,562 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-11 15:41:22,562 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-11 15:41:22,562 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-11 15:41:22,562 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-11 15:41:22,563 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-11 15:41:22,563 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-11 15:41:22,563 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-11 15:41:22,563 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-11 15:41:22,563 [forward.py:23 in reset_forward] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-11 15:41:22,563 [forward.py:23 in reset_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-11 15:41:22,563 [forward.py:23 in reset_forward] DEBUG - lm_head from flexgen to old.
