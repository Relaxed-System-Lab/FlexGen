2023-10-31 14:26:41,419 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmp999nh0nl
2023-10-31 14:26:41,419 [instantiator.py:76 in _write] INFO - Writing /tmp/tmp999nh0nl/_remote_module_non_scriptable.py
2023-10-31 14:26:42,030 [connectionpool.py:957 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-31 14:26:42,229 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 14:26:42,531 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 14:26:42,607 [model.py:132 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'weights_offload_dir/facebook.opt-125m'
2023-10-31 14:26:42,607 [model.py:71 in __init__] INFO - weights offload folder: weights_offload_dir/facebook.opt-125m
2023-10-31 14:26:42,748 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-31 14:26:42,827 [model.py:81 in __init__] INFO - tied_params: [['lm_head.weight', 'model.decoder.embed_tokens.weight']]
2023-10-31 14:26:42,832 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-31 14:26:42,833 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0.         0.03918046 0.96081954], size_todo: 85056000
2023-10-31 14:26:42,834 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [1.91116887e-05 3.91789619e-02 9.60801926e-01], size_todo: 85054464
2023-10-31 14:26:42,835 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.04997319 0.08332656 0.86670024], size_todo: 77966592
2023-10-31 14:26:42,836 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.07605041 0.1268119  0.79713769], size_todo: 70878720
2023-10-31 14:26:42,837 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.10571046 0.15066678 0.74362275], size_todo: 63790848
2023-10-31 14:26:42,838 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.12062976 0.17819364 0.7011766 ], size_todo: 56702976
2023-10-31 14:26:42,839 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.14055185 0.19276117 0.66668698], size_todo: 49615104
2023-10-31 14:26:42,840 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.1499285  0.21196308 0.63810841], size_todo: 42527232
2023-10-31 14:26:42,841 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.16439317 0.22156559 0.61404124], size_todo: 35439360
2023-10-31 14:26:42,841 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.17065379 0.23585085 0.59349535], size_todo: 28351488
2023-10-31 14:26:42,842 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.18173357 0.24251579 0.57575064], size_todo: 21263616
2023-10-31 14:26:42,843 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.18608848 0.27488348 0.53902803], size_todo: 14175744
2023-10-31 14:26:42,844 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.194913   0.27840721 0.5266798 ], size_todo: 7087872
2023-10-31 14:26:42,845 [model.py:263 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.19802909 0.30507996 0.49689094], size_todo: 0
2023-10-31 14:26:42,846 [model.py:263 in get_policy_weight_map] DEBUG - lm_head, [0.19802909 0.30507996 0.49689094], size_todo: 0
2023-10-31 14:26:42,846 [model.py:269 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-31 14:26:42,848 [model.py:305 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.05 GiB (19.80%), CPU Mem 0.07 GiB (30.51%), Disk Mem 0.12 Gib (49.69%)
2023-10-31 14:26:43,103 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-31 14:26:43,565 [model.py:395 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-31 14:26:43,566 [model.py:395 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-31 14:26:43,566 [model.py:395 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-31 14:26:43,566 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-31 14:26:43,566 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-31 14:26:43,566 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-31 14:26:43,567 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-31 14:26:43,567 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-31 14:26:43,567 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-31 14:26:43,567 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-31 14:26:43,567 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-31 14:26:43,567 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-31 14:26:43,567 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-31 14:26:43,568 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-31 14:26:43,568 [model.py:395 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-31 14:26:43,568 [model.py:395 in to_test_forward] DEBUG - lm_head to test forward
2023-10-31 14:26:43,572 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-31 14:26:43,574 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_tokens to meta


2023-10-31 14:26:43,576 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-31 14:26:43,577 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.embed_positions to meta


2023-10-31 14:26:43,577 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-31 14:26:43,584 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.0 to meta


2023-10-31 14:26:43,587 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-31 14:26:43,593 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.1 to meta


2023-10-31 14:26:43,595 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-31 14:26:43,602 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.2 to meta


2023-10-31 14:26:43,605 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-31 14:26:43,611 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.3 to meta


2023-10-31 14:26:43,614 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-31 14:26:43,620 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.4 to meta


2023-10-31 14:26:43,623 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-31 14:26:43,629 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.5 to meta


2023-10-31 14:26:43,631 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-31 14:26:43,638 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.6 to meta


2023-10-31 14:26:43,640 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-31 14:26:43,646 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.7 to meta


2023-10-31 14:26:43,649 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-31 14:26:43,655 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.8 to meta


2023-10-31 14:26:43,657 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-31 14:26:43,663 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.9 to meta


2023-10-31 14:26:43,666 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-31 14:26:43,672 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.10 to meta


2023-10-31 14:26:43,674 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-31 14:26:43,681 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.layers.11 to meta


2023-10-31 14:26:43,683 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-31 14:26:43,684 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: model.decoder.final_layer_norm to meta


2023-10-31 14:26:43,685 [model.py:350 in layer_cpu_load] DEBUG - load_layer_weights: lm_head to cpu
2023-10-31 14:26:43,698 [model.py:368 in layer_cpu_offload] DEBUG - offload_layer_weights: lm_head to meta


2023-10-31 14:26:43,704 [model.py:403 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-31 14:26:43,705 [model.py:403 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-31 14:26:43,705 [model.py:403 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-31 14:26:43,705 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-31 14:26:43,705 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-31 14:26:43,705 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-31 14:26:43,706 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-31 14:26:43,706 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-31 14:26:43,706 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-31 14:26:43,706 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-31 14:26:43,706 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-31 14:26:43,706 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-31 14:26:43,706 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-31 14:26:43,707 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-31 14:26:43,707 [model.py:403 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-31 14:26:43,707 [model.py:403 in reset_forward] DEBUG - lm_head from test to old.
2023-10-31 14:26:43,718 [model.py:518 in init_all_weights] DEBUG - init all weights...
2023-10-31 14:26:45,375 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-31 14:26:45,376 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-31 14:26:45,376 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-31 14:26:45,376 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-31 14:26:45,376 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-31 14:26:45,377 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-31 14:26:45,377 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-31 14:26:45,377 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-31 14:26:45,377 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-31 14:26:45,378 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-31 14:26:45,378 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-31 14:26:45,379 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-31 14:26:45,379 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-31 14:26:45,379 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-31 14:26:45,379 [flexgen.py:253 in layer_to_flexgen] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-31 14:26:45,379 [flexgen.py:253 in layer_to_flexgen] DEBUG - lm_head to flexgen forward
2023-10-31 14:26:45,520 [connectionpool.py:428 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-31 14:26:45,739 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 14:26:45,740 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 9), dtype=torch.int64)',)
2023-10-31 14:26:45,740 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:45,741 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 14:26:45,741 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:45,743 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:45,776 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 9), dtype=torch.int64)',)
2023-10-31 14:26:45,776 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:45,777 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:45,777 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9), dtype=torch.int64)',), {})
2023-10-31 14:26:45,779 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,781 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,782 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,783 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,785 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,786 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,789 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,790 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,791 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:45,792 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 14:26:45,792 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 9), dtype=torch.int64)', '0')
2023-10-31 14:26:45,792 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:45,793 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 14:26:45,793 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:26:45,811 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:45,815 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 9), dtype=torch.int64)', '0')
2023-10-31 14:26:45,815 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:45,816 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:45,816 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9), dtype=torch.int64)', '0'), {})
2023-10-31 14:26:45,818 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,820 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,821 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,823 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,825 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,827 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,829 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,830 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)
2023-10-31 14:26:45,831 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:45,844 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 14:26:45,844 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:45,844 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:45,845 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 14:26:45,851 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:26:45,867 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:26:45,883 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:45,883 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:45,884 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:45,885 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:47,286 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,293 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,300 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,307 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,314 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,321 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,329 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,333 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,333 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:47,333 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 14:26:47,333 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:47,333 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:47,334 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 14:26:47,343 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:26:47,357 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:26:47,372 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:47,373 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:47,374 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:47,375 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:47,381 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,388 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,396 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,403 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,410 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,417 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,424 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,429 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,429 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:47,429 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 14:26:47,430 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:47,430 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:47,430 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 14:26:47,443 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:26:47,458 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:26:47,473 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:47,474 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:47,476 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:47,476 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:47,482 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,489 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,496 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,503 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,511 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,518 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,526 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,531 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,531 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:47,532 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 14:26:47,532 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:47,532 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:47,532 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 14:26:47,542 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:26:47,557 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:26:47,572 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:47,572 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:47,573 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:47,574 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:47,581 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,587 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,594 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,600 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,608 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,615 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,622 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,627 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,627 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:47,627 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 14:26:47,627 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:47,627 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:47,627 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 14:26:47,640 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:26:47,655 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:26:47,669 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:47,670 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:47,671 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:47,671 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:47,678 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,684 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,692 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,699 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,706 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,714 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,722 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,727 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,727 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:47,727 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 14:26:47,727 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:47,728 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:47,728 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 14:26:47,738 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:26:47,753 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:26:47,768 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:47,769 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:47,771 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:47,771 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:47,778 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,785 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,792 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,799 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,807 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,815 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,823 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,827 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,827 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:47,827 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 14:26:47,827 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:47,827 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:47,828 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 14:26:47,840 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:26:47,855 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:26:47,870 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:47,870 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:47,872 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:47,872 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:47,881 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,888 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,894 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,901 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,908 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,915 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,923 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,927 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,928 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:47,928 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 14:26:47,928 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:47,928 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:47,928 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 14:26:47,938 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:26:47,952 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:26:47,967 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:47,967 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:47,969 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:47,969 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:47,976 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,983 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,990 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:47,997 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,005 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,012 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,021 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,025 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,025 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:48,025 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 14:26:48,026 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:48,026 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:48,026 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 14:26:48,038 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:26:48,056 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:26:48,071 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:48,071 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:48,073 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:48,073 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:48,080 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,087 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,094 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,101 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,109 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,116 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,124 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,128 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,129 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:48,129 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 14:26:48,129 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:48,129 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:48,129 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 14:26:48,139 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:26:48,153 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:26:48,168 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:48,168 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:48,170 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:48,170 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:48,176 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,183 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,190 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,197 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,205 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,212 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,220 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,229 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,230 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:48,230 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 14:26:48,230 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:48,230 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:48,230 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 14:26:48,251 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:26:48,266 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:26:48,281 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:48,281 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:48,283 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:48,283 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:48,291 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,298 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,306 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,313 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,321 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,328 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,336 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,341 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,341 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:48,342 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 14:26:48,342 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:48,342 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:48,342 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 14:26:48,353 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:26:48,354 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:26:48,369 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:48,370 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:48,372 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:48,372 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 9, 9), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': 'None', 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:48,379 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,387 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,394 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,401 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,409 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,417 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,425 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,429 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'))
2023-10-31 14:26:48,429 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:48,429 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 14:26:48,430 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:48,430 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:48,430 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 14:26:48,450 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:26:48,505 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:26:48,507 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:48,507 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:48,509 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:48,509 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',), {})
2023-10-31 14:26:48,511 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:48,514 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:48,516 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:48,518 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:48,521 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:48,523 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:48,525 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 9, 768), dtype=torch.float32)
2023-10-31 14:26:48,526 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)
2023-10-31 14:26:48,526 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:48,527 [flexgen.py:202 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 14:26:48,527 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:48,527 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:48,527 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 14:26:48,528 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:48,582 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:26:48,629 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',)
2023-10-31 14:26:48,630 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:48,631 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:48,631 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 9, 768), dtype=torch.float32)',), {})
2023-10-31 14:26:48,671 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 9, 50272), dtype=torch.float32)
2023-10-31 14:26:48,710 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 9, 50272), dtype=torch.float32)
2023-10-31 14:26:48,751 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 9, 50272), dtype=torch.float32)
2023-10-31 14:26:48,791 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 9, 50272), dtype=torch.float32)
2023-10-31 14:26:48,834 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 9, 50272), dtype=torch.float32)
2023-10-31 14:26:48,874 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 9, 50272), dtype=torch.float32)
2023-10-31 14:26:48,917 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 9, 50272), dtype=torch.float32)
2023-10-31 14:26:49,032 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(256, 9, 50272), dtype=torch.float32)
2023-10-31 14:26:49,033 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:49,039 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 14:26:49,039 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 1), dtype=torch.int64)',)
2023-10-31 14:26:49,039 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:49,039 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 14:26:49,047 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:49,051 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:49,101 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 1), dtype=torch.int64)',)
2023-10-31 14:26:49,101 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:49,102 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:49,103 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1), dtype=torch.int64)',), {})
2023-10-31 14:26:49,104 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,105 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,106 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,107 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,108 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,109 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,111 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,111 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,111 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:49,112 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 14:26:49,112 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 10), dtype=torch.int64)', '9')
2023-10-31 14:26:49,112 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:49,112 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 14:26:49,116 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:26:49,139 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:49,142 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 10), dtype=torch.int64)', '9')
2023-10-31 14:26:49,143 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:49,144 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:49,144 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 10), dtype=torch.int64)', '9'), {})
2023-10-31 14:26:49,145 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,147 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,148 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,149 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,150 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,151 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,153 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,154 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:49,154 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:49,161 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 14:26:49,161 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,161 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,161 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 14:26:49,165 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:26:49,181 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:26:49,197 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,197 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,202 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:49,203 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:49,212 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,219 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,227 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,235 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,243 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,250 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,260 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,265 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,265 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:49,266 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 14:26:49,266 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,266 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,266 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 14:26:49,280 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:26:49,295 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:26:49,310 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,310 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,314 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:49,314 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:49,322 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,330 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,337 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,345 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,353 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,361 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,371 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,375 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,375 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:49,375 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 14:26:49,375 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,375 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,376 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 14:26:49,396 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:26:49,411 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:26:49,426 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,426 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,430 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:49,430 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:49,439 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,447 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,455 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,462 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,471 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,479 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,488 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,492 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,492 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:49,492 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 14:26:49,492 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,493 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,493 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 14:26:49,500 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:26:49,516 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:26:49,531 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,531 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,535 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:49,535 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:49,543 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,552 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,560 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,568 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,577 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,585 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,594 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,598 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,598 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:49,598 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 14:26:49,598 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,598 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,599 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 14:26:49,608 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:26:49,622 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:26:49,637 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,637 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,641 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:49,641 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:49,648 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,656 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,664 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,672 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,680 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,688 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,696 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,700 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,700 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:49,700 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 14:26:49,701 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,701 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,701 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 14:26:49,708 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:26:49,723 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:26:49,737 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,737 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,741 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:49,741 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:49,749 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,758 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,765 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,774 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,782 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,790 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,800 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,804 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,804 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:49,804 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 14:26:49,804 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,805 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,805 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 14:26:49,814 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:26:49,829 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:26:49,844 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,844 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,848 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:49,849 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:49,857 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,865 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,874 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,882 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,891 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,899 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,908 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,912 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,912 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:49,913 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 14:26:49,913 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,913 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,913 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 14:26:49,920 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:26:49,935 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:26:49,950 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:49,950 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:49,954 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:49,954 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:49,962 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,970 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,978 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,986 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:49,993 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,001 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,009 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,013 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,013 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:50,013 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 14:26:50,013 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,014 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:50,014 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 14:26:50,022 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:26:50,037 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:26:50,052 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,053 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:50,057 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:50,057 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:50,065 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,073 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,080 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,089 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,097 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,105 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,114 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,118 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,119 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:50,119 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 14:26:50,119 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,119 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:50,119 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 14:26:50,126 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:26:50,142 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:26:50,156 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,157 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:50,160 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:50,161 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:50,169 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,177 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,184 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,192 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,200 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,208 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,216 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,220 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,220 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:50,221 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 14:26:50,221 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,221 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:50,221 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 14:26:50,236 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:26:50,250 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:26:50,265 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,265 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:50,269 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:50,269 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:50,277 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,285 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,294 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,302 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,310 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,319 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,327 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,331 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,332 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:50,332 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 14:26:50,332 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,332 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:50,332 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 14:26:50,339 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:26:50,340 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:26:50,356 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,356 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:50,360 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:50,360 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 10), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 9, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:50,369 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,377 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,386 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,394 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,402 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,411 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,419 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,423 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,423 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:50,424 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 14:26:50,424 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,424 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:50,424 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 14:26:50,440 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:26:50,487 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:26:50,488 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,488 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:50,489 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:50,489 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:26:50,491 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,492 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,493 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,494 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,495 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,496 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,498 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,498 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,498 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:50,499 [flexgen.py:202 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 14:26:50,499 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,499 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:50,499 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 14:26:50,499 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:50,546 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:26:50,593 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,593 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:50,594 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:50,595 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:26:50,601 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:50,608 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:50,614 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:50,621 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:50,627 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:50,634 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:50,640 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:50,652 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(256, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:50,652 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:50,653 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 14:26:50,654 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 1), dtype=torch.int64)',)
2023-10-31 14:26:50,654 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:50,654 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 14:26:50,669 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:50,673 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:50,723 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 1), dtype=torch.int64)',)
2023-10-31 14:26:50,723 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:50,724 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:50,725 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1), dtype=torch.int64)',), {})
2023-10-31 14:26:50,726 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,727 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,728 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,728 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,729 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,730 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,731 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,732 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,732 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:50,732 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 14:26:50,732 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 11), dtype=torch.int64)', '10')
2023-10-31 14:26:50,733 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:50,733 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 14:26:50,734 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:26:50,754 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:50,757 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 11), dtype=torch.int64)', '10')
2023-10-31 14:26:50,757 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:50,758 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:50,758 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 11), dtype=torch.int64)', '10'), {})
2023-10-31 14:26:50,759 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,760 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,761 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,762 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,763 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,764 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,765 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,766 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:50,766 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:50,772 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 14:26:50,773 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,773 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:50,773 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 14:26:50,775 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:26:50,789 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:26:50,804 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,805 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:50,809 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:50,809 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:50,817 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,825 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,833 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,841 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,849 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,857 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,866 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,869 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,870 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:50,870 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 14:26:50,870 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,870 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:50,870 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 14:26:50,877 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:26:50,892 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:26:50,907 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,908 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:50,912 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:50,912 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:50,920 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,929 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,937 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,945 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,954 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,962 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,970 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,973 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:50,974 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:50,974 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 14:26:50,974 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:50,974 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:50,974 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 14:26:50,980 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:26:50,995 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:26:51,009 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,010 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,014 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:51,014 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:51,022 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,030 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,037 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,045 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,053 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,062 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,070 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,073 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,074 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:51,074 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 14:26:51,074 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,074 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,074 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 14:26:51,079 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:26:51,093 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:26:51,108 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,108 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,113 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:51,113 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:51,121 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,128 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,136 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,144 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,152 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,159 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,167 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,171 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,171 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:51,172 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 14:26:51,172 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,172 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,172 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 14:26:51,177 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:26:51,192 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:26:51,206 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,207 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,211 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:51,211 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:51,219 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,227 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,234 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,242 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,250 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,257 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,265 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,269 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,269 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:51,269 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 14:26:51,270 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,270 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,270 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 14:26:51,275 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:26:51,289 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:26:51,304 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,304 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,308 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:51,309 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:51,316 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,324 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,332 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,339 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,347 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,355 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,363 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,366 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,367 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:51,367 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 14:26:51,367 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,367 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,367 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 14:26:51,372 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:26:51,387 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:26:51,401 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,402 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,406 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:51,406 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:51,414 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,422 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,430 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,438 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,445 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,453 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,461 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,465 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,465 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:51,465 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 14:26:51,465 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,465 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,465 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 14:26:51,470 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:26:51,484 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:26:51,499 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,499 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,504 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:51,504 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:51,512 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,519 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,527 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,535 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,543 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,550 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,558 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,562 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,563 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:51,563 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 14:26:51,563 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,563 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,563 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 14:26:51,569 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:26:51,584 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:26:51,599 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,599 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,604 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:51,604 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:51,612 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,620 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,628 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,636 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,644 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,653 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,661 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,665 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,665 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:51,665 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 14:26:51,665 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,666 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,666 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 14:26:51,671 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:26:51,687 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:26:51,702 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,702 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,707 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:51,707 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:51,715 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,723 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,730 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,738 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,746 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,754 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,762 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,766 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,766 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:51,767 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 14:26:51,767 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,767 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,767 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 14:26:51,775 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:26:51,790 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:26:51,804 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,804 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,809 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:51,809 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:51,818 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,825 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,833 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,841 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,849 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,857 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,865 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,869 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,869 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:51,869 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 14:26:51,869 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,870 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,870 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 14:26:51,874 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:26:51,875 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:26:51,890 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,890 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:51,894 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:51,895 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 11), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 10, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:51,903 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,911 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,920 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,928 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,936 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,945 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,955 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,959 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'))
2023-10-31 14:26:51,959 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:51,959 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 14:26:51,959 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:51,960 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:51,960 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 14:26:51,968 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:26:52,015 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:26:52,016 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,017 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:52,018 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:52,018 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:26:52,019 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,021 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,022 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,024 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,025 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,026 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,027 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,028 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,028 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:52,028 [flexgen.py:202 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 14:26:52,029 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,029 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:52,029 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 14:26:52,030 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:52,079 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:26:52,126 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,126 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:52,127 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:52,128 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:26:52,133 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:52,139 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:52,145 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:52,150 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:52,156 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:52,161 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:52,167 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:52,178 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(256, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:52,179 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:52,180 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 14:26:52,180 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 1), dtype=torch.int64)',)
2023-10-31 14:26:52,181 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:52,181 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 14:26:52,184 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:52,188 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:52,238 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 1), dtype=torch.int64)',)
2023-10-31 14:26:52,238 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:52,240 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:52,240 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1), dtype=torch.int64)',), {})
2023-10-31 14:26:52,241 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,242 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,243 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,244 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,245 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,246 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,247 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,248 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,248 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:52,249 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 14:26:52,249 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 12), dtype=torch.int64)', '11')
2023-10-31 14:26:52,249 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:52,249 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 14:26:52,255 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:26:52,273 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:52,276 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 12), dtype=torch.int64)', '11')
2023-10-31 14:26:52,277 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:52,278 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:52,278 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 12), dtype=torch.int64)', '11'), {})
2023-10-31 14:26:52,279 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,280 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,281 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,283 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,284 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,285 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,286 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,287 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:52,287 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:52,293 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 14:26:52,294 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,294 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:52,294 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 14:26:52,296 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:26:52,310 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:26:52,325 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,325 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:52,330 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:52,330 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:52,339 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,348 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,356 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,365 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,373 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,382 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,390 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,394 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,395 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:52,395 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 14:26:52,395 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,395 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:52,395 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 14:26:52,402 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:26:52,418 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:26:52,432 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,433 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:52,438 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:52,438 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:52,447 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,455 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,464 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,473 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,481 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,490 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,498 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,502 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,502 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:52,503 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 14:26:52,503 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,503 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:52,503 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 14:26:52,511 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:26:52,526 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:26:52,541 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,541 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:52,546 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:52,546 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:52,554 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,563 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,571 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,579 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,588 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,596 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,604 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,608 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,608 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:52,609 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 14:26:52,609 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,609 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:52,609 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 14:26:52,616 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:26:52,630 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:26:52,645 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,645 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:52,650 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:52,650 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:52,658 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,667 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,675 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,683 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,691 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,700 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,708 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,712 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,712 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:52,712 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 14:26:52,712 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,713 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:52,713 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 14:26:52,721 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:26:52,736 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:26:52,750 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,751 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:52,755 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:52,756 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:52,764 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,773 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,781 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,790 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,798 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,807 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,815 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,819 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,820 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:52,820 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 14:26:52,820 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,820 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:52,820 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 14:26:52,828 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:26:52,843 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:26:52,857 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,857 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:52,862 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:52,862 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:52,871 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,879 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,887 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,895 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,903 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,912 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,921 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,924 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,924 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:52,925 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 14:26:52,925 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,925 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:52,925 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 14:26:52,933 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:26:52,948 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:26:52,962 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:52,962 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:52,967 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:52,967 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:52,975 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,983 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:52,992 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,000 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,008 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,016 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,025 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,029 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,029 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:53,029 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 14:26:53,029 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,030 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:53,030 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 14:26:53,036 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:26:53,052 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:26:53,066 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,066 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:53,071 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:53,071 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:53,080 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,088 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,097 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,106 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,114 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,123 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,132 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,135 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,136 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:53,136 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 14:26:53,136 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,136 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:53,136 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 14:26:53,144 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:26:53,158 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:26:53,172 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,173 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:53,177 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:53,177 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:53,186 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,194 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,202 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,210 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,218 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,227 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,235 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,238 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,239 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:53,239 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 14:26:53,239 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,239 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:53,239 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 14:26:53,246 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:26:53,260 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:26:53,275 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,275 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:53,280 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:53,280 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:53,288 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,297 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,305 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,313 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,322 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,330 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,339 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,343 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,343 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:53,343 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 14:26:53,343 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,344 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:53,344 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 14:26:53,358 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:26:53,372 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:26:53,387 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,387 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:53,392 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:53,392 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:53,401 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,409 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,418 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,427 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,435 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,443 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,452 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,455 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,456 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:53,456 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 14:26:53,456 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,456 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:53,456 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 14:26:53,463 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:26:53,464 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:26:53,478 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,478 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:53,483 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:53,483 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 12), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 11, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:53,491 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,499 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,508 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,516 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,525 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,533 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,542 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,546 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,546 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:53,546 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 14:26:53,547 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,547 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:53,547 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 14:26:53,562 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:26:53,609 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:26:53,610 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,611 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:53,612 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:53,612 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:26:53,613 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,614 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,616 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,617 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,618 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,619 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,620 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,621 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,621 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:53,621 [flexgen.py:202 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 14:26:53,621 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,621 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:53,622 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 14:26:53,622 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:53,670 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:26:53,716 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,717 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:53,718 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:53,718 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:26:53,723 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:53,727 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:53,732 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:53,736 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:53,741 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:53,746 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:53,752 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:53,763 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(256, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:53,763 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:53,764 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 14:26:53,764 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 1), dtype=torch.int64)',)
2023-10-31 14:26:53,764 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:53,765 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 14:26:53,765 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:53,768 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:53,820 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 1), dtype=torch.int64)',)
2023-10-31 14:26:53,820 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:53,822 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:53,822 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1), dtype=torch.int64)',), {})
2023-10-31 14:26:53,823 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,825 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,826 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,828 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,829 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,830 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,831 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,832 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,832 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:53,833 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 14:26:53,833 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 13), dtype=torch.int64)', '12')
2023-10-31 14:26:53,833 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:53,833 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 14:26:53,834 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:26:53,856 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:53,859 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 13), dtype=torch.int64)', '12')
2023-10-31 14:26:53,859 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:53,860 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:53,860 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 13), dtype=torch.int64)', '12'), {})
2023-10-31 14:26:53,862 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,863 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,865 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,866 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,867 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,868 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,870 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,871 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:53,871 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:53,879 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 14:26:53,879 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,879 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:53,879 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 14:26:53,882 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:26:53,900 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:26:53,917 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:53,917 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:53,922 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:53,923 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:53,932 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,941 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,951 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,960 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,971 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,982 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:53,994 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,000 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,000 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:54,000 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 14:26:54,000 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,001 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,001 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 14:26:54,006 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:26:54,023 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:26:54,039 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,039 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,045 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:54,045 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:54,055 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,066 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,076 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,086 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,096 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,107 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,118 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,122 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,122 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:54,122 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 14:26:54,123 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,123 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,123 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 14:26:54,133 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:26:54,148 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:26:54,163 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,163 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,168 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:54,169 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:54,177 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,187 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,198 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,208 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,219 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,229 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,239 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,243 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,244 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:54,244 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 14:26:54,244 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,244 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,244 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 14:26:54,248 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:26:54,263 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:26:54,278 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,278 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,283 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:54,284 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:54,293 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,302 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,312 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,321 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,330 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,339 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,349 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,353 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,353 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:54,354 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 14:26:54,354 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,354 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,354 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 14:26:54,358 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:26:54,373 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:26:54,389 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,390 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,395 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:54,396 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:54,406 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,416 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,426 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,436 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,447 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,457 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,467 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,472 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,472 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:54,472 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 14:26:54,472 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,473 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,473 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 14:26:54,476 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:26:54,491 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:26:54,507 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,507 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,512 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:54,513 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:54,522 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,532 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,541 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,551 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,560 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,572 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,583 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,587 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,588 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:54,588 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 14:26:54,588 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,588 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,588 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 14:26:54,593 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:26:54,607 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:26:54,622 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,622 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,627 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:54,628 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:54,636 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,645 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,654 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,663 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,672 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,681 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,690 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,694 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,694 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:54,695 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 14:26:54,695 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,695 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,695 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 14:26:54,699 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:26:54,714 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:26:54,729 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,729 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,734 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:54,735 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:54,744 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,753 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,763 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,773 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,786 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,795 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,805 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,809 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,809 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:54,810 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 14:26:54,810 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,810 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,810 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 14:26:54,815 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:26:54,830 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:26:54,844 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,845 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,850 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:54,850 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:54,859 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,868 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,878 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,886 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,895 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,905 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,916 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,921 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,922 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:54,922 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 14:26:54,922 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,922 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,922 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 14:26:54,926 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:26:54,940 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:26:54,955 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:54,955 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:54,960 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:54,960 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:54,969 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,978 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,987 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:54,995 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,004 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,013 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,021 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,025 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,026 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:55,026 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 14:26:55,026 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,026 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:55,026 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 14:26:55,036 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:26:55,050 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:26:55,065 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,065 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:55,070 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:55,071 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:55,079 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,088 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,098 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,109 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,119 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,128 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,138 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,142 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,142 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:55,143 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 14:26:55,143 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,143 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:55,143 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 14:26:55,150 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:26:55,151 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:26:55,166 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,166 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:55,172 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:55,172 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 13), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 12, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:55,182 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,191 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,201 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,210 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,220 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,229 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,239 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,243 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,243 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:55,243 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 14:26:55,244 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,244 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:55,244 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 14:26:55,251 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:26:55,298 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:26:55,299 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,299 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:55,300 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:55,300 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:26:55,302 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,303 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,304 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,305 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,307 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,308 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,309 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,309 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,310 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:55,310 [flexgen.py:202 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 14:26:55,310 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,310 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:55,310 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 14:26:55,310 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:55,358 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:26:55,405 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,405 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:55,406 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:55,406 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:26:55,410 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:55,415 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:55,420 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:55,424 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:55,429 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:55,433 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:55,438 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:55,450 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(256, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:55,450 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:55,451 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 14:26:55,452 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 1), dtype=torch.int64)',)
2023-10-31 14:26:55,452 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:55,452 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 14:26:55,453 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:55,458 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:55,508 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 1), dtype=torch.int64)',)
2023-10-31 14:26:55,508 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:55,509 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:55,510 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1), dtype=torch.int64)',), {})
2023-10-31 14:26:55,511 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,512 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,513 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,513 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,514 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,515 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,516 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,517 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,517 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:55,517 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 14:26:55,518 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 14), dtype=torch.int64)', '13')
2023-10-31 14:26:55,518 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:55,518 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 14:26:55,519 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:26:55,539 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:55,541 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 14), dtype=torch.int64)', '13')
2023-10-31 14:26:55,542 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:55,542 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:55,543 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 14), dtype=torch.int64)', '13'), {})
2023-10-31 14:26:55,544 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,545 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,546 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,547 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,548 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,549 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,551 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,551 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:55,552 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:55,558 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 14:26:55,559 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,559 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:55,559 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 14:26:55,562 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:26:55,576 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:26:55,591 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,591 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:55,597 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:55,597 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:55,606 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,616 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,625 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,634 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,644 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,653 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,663 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,667 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,668 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:55,668 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 14:26:55,668 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,668 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:55,668 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 14:26:55,672 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:26:55,687 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:26:55,701 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,702 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:55,707 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:55,708 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:55,717 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,727 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,736 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,745 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,755 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,764 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,774 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,779 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,779 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:55,779 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 14:26:55,779 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,779 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:55,780 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 14:26:55,784 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:26:55,799 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:26:55,815 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,815 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:55,821 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:55,821 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:55,831 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,840 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,849 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,859 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,868 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,878 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,887 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,892 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,892 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:55,892 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 14:26:55,892 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,892 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:55,893 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 14:26:55,896 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:26:55,911 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:26:55,926 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:55,926 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:55,931 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:55,932 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:55,941 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,951 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,960 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,970 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,979 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,989 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:55,998 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,003 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,003 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:56,003 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 14:26:56,003 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,003 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,003 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 14:26:56,008 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:26:56,023 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:26:56,037 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,038 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,043 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:56,043 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:56,053 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,062 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,071 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,081 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,091 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,100 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,109 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,114 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,114 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:56,114 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 14:26:56,114 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,114 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,114 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 14:26:56,118 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:26:56,133 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:26:56,147 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,148 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,153 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:56,153 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:56,163 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,173 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,182 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,192 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,201 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,211 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,221 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,225 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,226 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:56,226 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 14:26:56,226 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,226 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,226 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 14:26:56,230 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:26:56,245 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:26:56,260 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,260 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,265 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:56,266 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:56,276 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,285 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,295 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,305 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,314 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,324 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,334 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,338 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,339 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:56,339 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 14:26:56,339 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,339 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,339 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 14:26:56,343 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:26:56,358 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:26:56,373 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,373 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,379 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:56,380 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:56,391 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,401 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,411 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,421 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,431 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,441 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,451 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,456 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,456 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:56,456 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 14:26:56,456 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,456 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,457 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 14:26:56,461 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:26:56,477 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:26:56,492 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,492 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,498 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:56,498 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:56,507 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,517 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,527 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,536 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,545 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,555 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,564 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,569 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,569 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:56,569 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 14:26:56,569 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,569 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,569 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 14:26:56,573 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:26:56,588 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:26:56,602 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,602 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,607 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:56,608 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:56,617 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,627 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,636 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,645 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,655 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,664 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,673 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,678 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,678 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:56,678 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 14:26:56,678 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,678 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,679 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 14:26:56,687 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:26:56,701 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:26:56,715 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,716 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,721 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:56,722 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:56,732 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,741 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,751 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,760 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,770 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,780 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,790 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,794 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,794 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:56,794 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 14:26:56,795 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,795 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,795 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 14:26:56,804 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:26:56,805 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:26:56,819 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,820 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:56,825 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:56,826 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 14), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 13, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:56,835 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,844 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,854 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,863 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,873 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,886 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,896 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,900 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'))
2023-10-31 14:26:56,900 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:56,900 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 14:26:56,901 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,901 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:56,901 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 14:26:56,920 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:26:56,966 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:26:56,968 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,968 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:56,969 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:56,969 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:26:56,970 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:56,971 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:56,973 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:56,974 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:56,975 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:56,976 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:56,977 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:56,978 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:56,978 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:56,979 [flexgen.py:202 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 14:26:56,979 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:56,979 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:56,979 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 14:26:56,979 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:57,026 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:26:57,074 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:57,075 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:57,076 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:57,076 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:26:57,082 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:57,088 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:57,093 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:57,099 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:57,105 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:57,110 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:57,116 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:57,127 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(256, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:57,128 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:57,130 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 14:26:57,130 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 1), dtype=torch.int64)',)
2023-10-31 14:26:57,130 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:57,131 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 14:26:57,133 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:57,138 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:57,187 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 1), dtype=torch.int64)',)
2023-10-31 14:26:57,187 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:57,188 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:57,188 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1), dtype=torch.int64)',), {})
2023-10-31 14:26:57,189 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,190 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,191 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,192 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,193 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,194 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,195 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,196 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,196 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:57,197 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 14:26:57,197 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 15), dtype=torch.int64)', '14')
2023-10-31 14:26:57,197 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:57,197 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 14:26:57,198 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:26:57,219 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:57,222 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 15), dtype=torch.int64)', '14')
2023-10-31 14:26:57,222 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:57,223 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:57,223 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 15), dtype=torch.int64)', '14'), {})
2023-10-31 14:26:57,224 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,226 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,227 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,228 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,229 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,230 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,231 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,232 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:57,232 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:57,238 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 14:26:57,239 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:57,239 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:57,239 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 14:26:57,243 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:26:57,258 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:26:57,272 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:57,272 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:57,278 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:57,279 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:57,290 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,300 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,310 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,320 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,330 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,340 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,350 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,356 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,356 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:57,357 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 14:26:57,357 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:57,357 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:57,357 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 14:26:57,364 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:26:57,379 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:26:57,395 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:57,395 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:57,401 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:57,401 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:57,412 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,422 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,432 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,442 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,452 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,464 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,474 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,478 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,478 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:57,479 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 14:26:57,479 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:57,479 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:57,479 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 14:26:57,488 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:26:57,502 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:26:57,517 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:57,517 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:57,523 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:57,524 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:57,533 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,543 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,553 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,563 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,574 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,584 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,594 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,598 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,599 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:57,599 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 14:26:57,599 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:57,599 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:57,599 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 14:26:57,606 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:26:57,621 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:26:57,635 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:57,636 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:57,641 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:57,642 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:57,651 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,661 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,671 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,682 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,692 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,701 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,712 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,716 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,716 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:57,717 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 14:26:57,717 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:57,717 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:57,717 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 14:26:57,725 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:26:57,740 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:26:57,755 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:57,755 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:57,761 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:57,761 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:57,771 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,781 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,791 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,801 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,811 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,822 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,832 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,836 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,836 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:57,837 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 14:26:57,837 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:57,837 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:57,837 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 14:26:57,844 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:26:57,859 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:26:57,874 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:57,874 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:57,880 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:57,880 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:57,891 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,901 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,911 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,922 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,932 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,943 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,955 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,960 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:57,961 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:57,961 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 14:26:57,961 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:57,961 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:57,962 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 14:26:57,971 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:26:57,986 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:26:58,001 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:58,001 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:58,007 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:58,007 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:58,017 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,041 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,051 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,061 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,071 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,081 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,091 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,095 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,095 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:59,095 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 14:26:59,095 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:59,096 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:59,096 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 14:26:59,103 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:26:59,118 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:26:59,133 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:59,133 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:59,139 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:59,139 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:59,149 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,159 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,169 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,178 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,188 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,198 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,208 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,213 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,213 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:59,213 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 14:26:59,213 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:59,213 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:59,214 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 14:26:59,222 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:26:59,237 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:26:59,252 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:59,252 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:59,258 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:59,259 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:59,270 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,280 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,290 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,300 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,310 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,321 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,331 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,336 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,336 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:59,336 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 14:26:59,336 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:59,337 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:59,337 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 14:26:59,344 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:26:59,360 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:26:59,375 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:59,375 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:59,381 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:59,382 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:59,392 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,402 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,413 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,424 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,434 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,445 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,455 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,460 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,460 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:59,460 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 14:26:59,460 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:59,461 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:59,461 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 14:26:59,475 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:26:59,490 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:26:59,507 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:59,507 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:59,513 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:59,513 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:59,524 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,534 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,545 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,555 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,566 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,580 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,592 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,596 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,596 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:59,596 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 14:26:59,597 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:59,597 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:59,597 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 14:26:59,604 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:26:59,605 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:26:59,620 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:59,621 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:26:59,627 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:59,627 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 15), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 14, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:26:59,638 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,650 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,660 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,670 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,682 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,695 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,705 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,709 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'))
2023-10-31 14:26:59,710 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:59,710 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 14:26:59,710 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:59,710 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:59,710 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 14:26:59,724 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:26:59,771 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:26:59,772 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:59,773 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:59,773 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:59,774 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:26:59,775 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,776 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,778 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,779 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,780 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,781 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,783 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,783 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,784 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:59,784 [flexgen.py:202 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 14:26:59,784 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:59,784 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:59,784 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 14:26:59,785 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:59,832 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:26:59,879 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:26:59,880 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:59,881 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:59,881 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:26:59,886 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:59,892 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:59,898 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:59,904 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:59,910 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:59,916 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:59,921 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:59,933 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(256, 1, 50272), dtype=torch.float32)
2023-10-31 14:26:59,933 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:59,935 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 14:26:59,935 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 1), dtype=torch.int64)',)
2023-10-31 14:26:59,935 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:59,935 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 14:26:59,936 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:26:59,939 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:26:59,988 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 1), dtype=torch.int64)',)
2023-10-31 14:26:59,989 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:26:59,990 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:26:59,990 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1), dtype=torch.int64)',), {})
2023-10-31 14:26:59,991 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,992 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,993 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,994 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,995 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,996 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,998 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,998 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:26:59,998 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:26:59,999 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 14:26:59,999 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 16), dtype=torch.int64)', '15')
2023-10-31 14:26:59,999 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:26:59,999 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 14:27:00,001 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:27:00,020 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:27:00,024 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 16), dtype=torch.int64)', '15')
2023-10-31 14:27:00,024 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:27:00,025 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:00,025 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 16), dtype=torch.int64)', '15'), {})
2023-10-31 14:27:00,026 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:00,028 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:00,029 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:00,030 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:00,031 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:00,032 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:00,034 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:00,035 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:27:00,035 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:00,042 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 14:27:00,042 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,042 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,042 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 14:27:00,046 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:27:00,061 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:27:00,076 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,076 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,082 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:00,083 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:00,093 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,103 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,114 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,124 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,134 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,146 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,156 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,161 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,162 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:00,162 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 14:27:00,162 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,162 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,162 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 14:27:00,170 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:27:00,185 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:27:00,199 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,200 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,206 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:00,206 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:00,217 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,227 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,238 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,249 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,260 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,270 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,281 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,285 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,286 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:00,286 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 14:27:00,286 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,286 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,286 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 14:27:00,295 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:27:00,310 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:27:00,325 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,325 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,331 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:00,332 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:00,343 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,353 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,364 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,375 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,386 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,398 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,409 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,414 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,414 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:00,415 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 14:27:00,415 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,415 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,415 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 14:27:00,422 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:27:00,438 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:27:00,453 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,453 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,460 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:00,460 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:00,471 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,482 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,492 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,503 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,513 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,523 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,534 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,539 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,539 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:00,539 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 14:27:00,539 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,540 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,540 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 14:27:00,548 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:27:00,563 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:27:00,578 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,578 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,584 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:00,585 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:00,595 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,606 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,616 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,626 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,637 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,647 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,657 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,662 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,662 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:00,662 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 14:27:00,662 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,663 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,663 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 14:27:00,670 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:27:00,685 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:27:00,699 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,699 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,705 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:00,706 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:00,716 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,727 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,737 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,748 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,758 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,769 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,780 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,784 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,785 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:00,785 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 14:27:00,785 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,785 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,785 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 14:27:00,794 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:27:00,809 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:27:00,823 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,824 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,829 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:00,830 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:00,840 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,851 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,862 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,872 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,882 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,893 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,903 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,907 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,908 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:00,908 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 14:27:00,908 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,908 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,908 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 14:27:00,915 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:27:00,930 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:27:00,944 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:00,945 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:00,951 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:00,951 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:00,962 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,972 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,983 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:00,995 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,007 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,018 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,028 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,033 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,033 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:01,033 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 14:27:01,034 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:01,034 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:01,034 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 14:27:01,043 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:27:01,058 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:27:01,074 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:01,075 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:01,082 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:01,082 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:01,096 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,106 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,116 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,126 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,136 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,146 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,157 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,161 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,162 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:01,162 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 14:27:01,162 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:01,162 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:01,162 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 14:27:01,169 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:27:01,184 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:27:01,198 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:01,198 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:01,205 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:01,205 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:01,216 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,226 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,236 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,245 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,261 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,300 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,313 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,318 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,318 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:01,318 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 14:27:01,318 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:01,319 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:01,319 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 14:27:01,331 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:27:01,347 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:27:01,365 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:01,365 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:01,374 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:01,374 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:01,387 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,400 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,413 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,437 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,450 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,463 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,478 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,483 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,483 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:01,484 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 14:27:01,484 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:01,484 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:01,484 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 14:27:01,492 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:27:01,493 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:27:01,509 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:01,509 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:01,517 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:01,518 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 16), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 15, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:01,531 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,543 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,555 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,567 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,579 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,593 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,615 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,619 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'))
2023-10-31 14:27:01,619 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:01,619 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 14:27:01,619 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:01,620 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:27:01,620 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 14:27:01,635 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:27:01,686 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:27:01,688 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:01,688 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:27:01,690 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:01,690 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:27:01,692 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,694 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,696 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,697 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,699 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,700 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,702 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,703 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,703 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:01,703 [flexgen.py:202 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 14:27:01,703 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:01,703 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:27:01,703 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 14:27:01,704 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:27:01,754 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:27:01,803 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:01,804 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:27:01,805 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:01,805 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:27:01,812 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:01,818 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:01,824 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:01,830 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:01,837 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:01,849 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:01,856 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:01,868 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(256, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:01,869 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:01,870 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 14:27:01,871 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 1), dtype=torch.int64)',)
2023-10-31 14:27:01,871 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:27:01,871 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 14:27:01,873 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:27:01,880 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:27:01,932 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 1), dtype=torch.int64)',)
2023-10-31 14:27:01,932 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:27:01,934 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:01,934 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1), dtype=torch.int64)',), {})
2023-10-31 14:27:01,936 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,937 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,938 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,939 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,940 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,941 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,943 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,943 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,943 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:01,944 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 14:27:01,944 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 17), dtype=torch.int64)', '16')
2023-10-31 14:27:01,944 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:27:01,944 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 14:27:01,946 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:27:01,966 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:27:01,969 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 17), dtype=torch.int64)', '16')
2023-10-31 14:27:01,969 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:27:01,970 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:01,971 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 17), dtype=torch.int64)', '16'), {})
2023-10-31 14:27:01,972 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,973 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,975 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,976 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,977 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,979 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,980 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,981 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:27:01,981 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:01,989 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 14:27:01,989 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:01,990 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:01,990 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 14:27:01,992 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:27:02,007 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:27:02,022 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:02,022 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:02,030 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:02,031 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:02,045 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,058 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,070 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,083 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,096 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,109 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,123 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,128 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,128 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:02,128 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 14:27:02,129 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:02,129 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:02,129 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 14:27:02,137 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:27:02,153 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:27:02,169 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:02,169 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:02,178 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:02,179 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:02,192 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,206 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,219 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,232 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,246 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,259 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,278 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,284 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,284 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:02,285 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 14:27:02,285 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:02,285 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:02,285 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 14:27:02,295 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:27:02,311 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:27:02,329 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:02,330 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:02,339 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:02,339 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:02,353 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,367 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,381 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,394 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,408 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,422 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,436 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,441 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,441 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:02,441 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 14:27:02,441 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:02,442 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:02,442 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 14:27:02,449 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:27:02,465 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:27:02,481 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:02,481 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:02,490 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:02,490 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:02,504 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,520 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,534 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,548 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,563 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,578 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,592 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,596 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,597 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:02,597 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 14:27:02,597 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:02,597 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:02,597 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 14:27:02,607 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:27:02,623 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:27:02,638 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:02,638 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:02,648 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:02,648 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:02,662 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,675 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,689 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,705 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,719 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,734 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,747 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,752 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,752 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:02,752 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 14:27:02,752 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:02,753 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:02,753 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 14:27:02,760 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:27:02,776 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:27:02,794 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:02,795 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:02,804 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:02,805 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:02,819 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,832 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,846 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,861 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,876 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,890 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,904 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,909 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,909 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:02,909 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 14:27:02,909 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:02,909 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:02,910 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 14:27:02,919 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:27:02,935 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:27:02,960 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:02,960 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:02,969 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:02,969 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:02,984 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:02,997 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,011 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,025 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,038 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,052 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,066 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,071 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,071 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:03,072 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 14:27:03,072 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:03,072 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:03,072 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 14:27:03,080 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:27:03,096 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:27:03,111 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:03,112 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:03,123 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:03,124 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:03,139 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,154 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,169 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,183 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,198 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,213 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,227 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,231 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,232 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:03,232 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 14:27:03,232 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:03,232 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:03,232 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 14:27:03,241 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:27:03,257 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:27:03,273 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:03,273 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:03,285 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:03,285 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:03,301 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,320 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,347 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,364 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,378 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,393 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,407 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,411 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,412 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:03,412 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 14:27:03,412 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:03,412 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:03,412 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 14:27:03,420 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:27:03,436 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:27:03,451 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:03,451 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:03,461 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:03,461 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:03,477 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,491 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,504 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,519 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,532 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,547 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,560 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,565 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,565 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:03,566 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 14:27:03,566 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:03,566 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:03,566 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 14:27:03,578 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:27:03,594 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:27:03,609 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:03,610 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:03,618 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:03,619 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:03,632 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,646 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,659 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,675 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,689 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,703 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,718 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,723 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,725 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:03,725 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 14:27:03,725 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:03,726 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:03,726 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 14:27:03,733 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:27:03,735 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:27:03,751 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:03,751 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:03,761 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:03,761 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 17), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 16, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:03,775 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,791 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,805 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,818 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,832 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,846 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,860 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,864 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'))
2023-10-31 14:27:03,865 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:03,865 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 14:27:03,865 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:03,865 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:27:03,865 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 14:27:03,883 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:27:03,933 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:27:03,934 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:03,934 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:27:03,936 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:03,936 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:27:03,938 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:03,939 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:03,941 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:03,943 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:03,944 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:03,946 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:03,948 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:03,948 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:27:03,949 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:03,949 [flexgen.py:202 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 14:27:03,949 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:03,949 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:27:03,949 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 14:27:03,950 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:27:04,002 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:27:04,051 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:04,052 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:27:04,053 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:04,054 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:27:04,062 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:04,069 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:04,076 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:04,083 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:04,089 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:04,096 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:04,103 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:04,115 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(256, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:04,115 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:04,117 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_tokens
2023-10-31 14:27:04,118 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 1), dtype=torch.int64)',)
2023-10-31 14:27:04,118 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:27:04,118 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head
2023-10-31 14:27:04,118 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:27:04,122 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:27:04,173 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 1), dtype=torch.int64)',)
2023-10-31 14:27:04,173 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:27:04,175 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:04,175 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1), dtype=torch.int64)',), {})
2023-10-31 14:27:04,176 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,178 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,179 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,180 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,181 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,182 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,183 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,184 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,184 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:04,184 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.embed_positions
2023-10-31 14:27:04,185 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('Tensor(shape=(256, 18), dtype=torch.int64)', '17')
2023-10-31 14:27:04,185 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:27:04,185 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens
2023-10-31 14:27:04,186 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:27:04,205 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cuda:0
2023-10-31 14:27:04,209 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('Tensor(shape=(32, 18), dtype=torch.int64)', '17')
2023-10-31 14:27:04,209 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:27:04,210 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:04,210 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 18), dtype=torch.int64)', '17'), {})
2023-10-31 14:27:04,212 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,213 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,215 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,216 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,217 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,218 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,220 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,221 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:27:04,221 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:04,228 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.0
2023-10-31 14:27:04,229 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:04,229 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:04,229 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions
2023-10-31 14:27:04,231 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:27:04,246 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cuda:0
2023-10-31 14:27:04,264 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:04,264 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:04,274 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:04,274 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:04,290 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,305 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,319 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,333 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,348 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,363 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,376 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,382 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,382 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:04,382 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.1
2023-10-31 14:27:04,382 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:04,383 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:04,383 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0
2023-10-31 14:27:04,391 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:27:04,407 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cuda:0
2023-10-31 14:27:04,423 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:04,423 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:04,432 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:04,433 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:04,449 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,464 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,478 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,493 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,507 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,521 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,535 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,540 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,540 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:04,540 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.2
2023-10-31 14:27:04,540 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:04,540 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:04,541 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1
2023-10-31 14:27:04,550 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:27:04,565 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cuda:0
2023-10-31 14:27:04,581 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:04,581 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:04,591 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:04,591 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:04,606 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,620 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,634 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,649 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,664 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,678 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,692 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,697 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,697 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:04,698 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.3
2023-10-31 14:27:04,698 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:04,698 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:04,698 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2
2023-10-31 14:27:04,706 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:27:04,722 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cuda:0
2023-10-31 14:27:04,738 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:04,738 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:04,746 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:04,746 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:04,760 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,775 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,789 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,804 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,818 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,832 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,847 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,852 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,852 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:04,852 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.4
2023-10-31 14:27:04,852 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:04,852 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:04,852 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3
2023-10-31 14:27:04,863 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:27:04,879 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cuda:0
2023-10-31 14:27:04,895 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:04,895 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:04,903 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:04,904 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:04,917 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,932 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,946 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,961 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,975 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:04,990 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,004 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,008 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,009 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:05,009 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.5
2023-10-31 14:27:05,009 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:05,009 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:05,009 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4
2023-10-31 14:27:05,016 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:27:05,032 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cuda:0
2023-10-31 14:27:05,048 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:05,048 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:05,058 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:05,058 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:05,072 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,087 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,103 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,117 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,132 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,147 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,161 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,166 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,166 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:05,166 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.6
2023-10-31 14:27:05,167 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:05,167 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:05,167 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5
2023-10-31 14:27:05,177 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:27:05,194 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cuda:0
2023-10-31 14:27:05,209 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:05,210 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:05,219 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:05,220 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:05,234 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,249 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,263 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,279 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,295 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,311 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,325 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,331 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,331 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:05,331 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.7
2023-10-31 14:27:05,331 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:05,331 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:05,332 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6
2023-10-31 14:27:05,340 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:27:05,359 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cuda:0
2023-10-31 14:27:05,375 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:05,375 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:05,385 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:05,385 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:05,399 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,413 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,428 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,444 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,458 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,474 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,488 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,493 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,493 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:05,493 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.8
2023-10-31 14:27:05,493 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:05,494 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:05,494 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7
2023-10-31 14:27:05,503 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:27:05,519 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cuda:0
2023-10-31 14:27:05,536 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:05,536 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:05,545 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:05,546 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:05,560 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,578 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,592 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,609 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,625 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,641 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,655 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,659 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,660 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:05,660 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.9
2023-10-31 14:27:05,660 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:05,660 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:05,660 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8
2023-10-31 14:27:05,668 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:27:05,683 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cuda:0
2023-10-31 14:27:05,698 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:05,699 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:05,709 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:05,709 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:05,723 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,740 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,755 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,769 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,785 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,800 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,815 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,819 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,820 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:05,820 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.10
2023-10-31 14:27:05,820 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:05,820 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:05,820 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9
2023-10-31 14:27:05,835 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:27:05,850 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cuda:0
2023-10-31 14:27:05,867 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:05,867 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:05,876 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:05,876 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:05,891 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,906 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,921 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,936 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,952 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,967 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,983 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,988 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:05,988 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:05,989 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.layers.11
2023-10-31 14:27:05,989 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:05,989 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {'attention_mask': 'Tensor(shape=(256, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:05,989 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10
2023-10-31 14:27:05,997 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:27:05,998 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cuda:0
2023-10-31 14:27:06,013 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:06,013 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {'attention_mask': 'Tensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'}
2023-10-31 14:27:06,022 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:06,023 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {'attention_mask': 'MixTensor(shape=(32, 1, 1, 18), dtype=torch.float32)', 'layer_head_mask': 'None', 'past_key_value': ('MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 17, 64), dtype=torch.float32)'), 'output_attentions': 'False', 'use_cache': 'True'})
2023-10-31 14:27:06,037 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:06,051 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:06,069 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:06,093 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:06,107 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:06,121 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:06,134 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)', ('MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)', 'MixTensor(shape=(32, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:06,139 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)', ('BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)', 'BatchListTensor(shape=(256, 12, 18, 64), dtype=torch.float32)'))
2023-10-31 14:27:06,139 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:06,139 [flexgen.py:202 in flexgen_forward] DEBUG - layer: model.decoder.final_layer_norm
2023-10-31 14:27:06,140 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:06,140 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:27:06,140 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11
2023-10-31 14:27:06,154 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:27:06,205 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cuda:0
2023-10-31 14:27:06,207 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:06,207 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:27:06,208 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:06,208 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:27:06,210 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:06,211 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:06,213 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:06,214 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:06,216 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:06,217 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:06,219 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 768), dtype=torch.float32)
2023-10-31 14:27:06,219 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)
2023-10-31 14:27:06,219 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:06,220 [flexgen.py:202 in flexgen_forward] DEBUG - layer: lm_head
2023-10-31 14:27:06,220 [flexgen.py:203 in flexgen_forward] DEBUG - args: ('BatchListTensor(shape=(256, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:06,220 [flexgen.py:204 in flexgen_forward] DEBUG - kwargs: {}
2023-10-31 14:27:06,220 [model.py:507 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm
2023-10-31 14:27:06,220 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cuda:0
2023-10-31 14:27:06,270 [model.py:489 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cuda:0
2023-10-31 14:27:06,320 [flexgen.py:112 in prepare_curr_layer] DEBUG - args_k: ('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',)
2023-10-31 14:27:06,320 [flexgen.py:113 in prepare_curr_layer] DEBUG - kwarg_k: {}
2023-10-31 14:27:06,321 [flexgen.py:125 in _store_prev_batch] DEBUG - output of last layer, as curr layer's input
2023-10-31 14:27:06,322 [flexgen.py:126 in _store_prev_batch] DEBUG - batch: 7, offloaded input: (('MixTensor(shape=(32, 1, 768), dtype=torch.float32)',), {})
2023-10-31 14:27:06,328 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 0, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:06,333 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 1, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:06,339 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 2, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:06,345 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 3, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:06,351 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 4, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:06,356 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 5, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:06,361 [flexgen.py:120 in _store_prev_batch] DEBUG - batch: 6, offloaded output: MixTensor(shape=(32, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:06,373 [flexgen.py:225 in flexgen_forward] DEBUG - outputs after concat: Tensor(shape=(256, 1, 50272), dtype=torch.float32)
2023-10-31 14:27:06,374 [flexgen.py:228 in flexgen_forward] DEBUG - over.


2023-10-31 14:27:06,390 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,390 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,390 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,390 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,391 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,391 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,391 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,391 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,391 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,391 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,391 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,391 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,391 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,391 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,391 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,391 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,391 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,391 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,391 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,391 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,392 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,392 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,392 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,392 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,392 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,392 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,392 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,392 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,392 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,392 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,392 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,392 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,392 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,392 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,392 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,392 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,392 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,393 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,393 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,393 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,393 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,393 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,393 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,393 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,393 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,393 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,393 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,393 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,393 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,393 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,393 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,393 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,393 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,393 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,393 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,394 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,401 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,401 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,401 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,401 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,401 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,401 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,401 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,402 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,402 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,402 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,402 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,402 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,402 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,402 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,402 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,402 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,402 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,402 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,402 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,402 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,402 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,402 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,402 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,402 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,403 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,403 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,403 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,403 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,403 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,403 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,403 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,403 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,403 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,403 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,403 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,403 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,403 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,403 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,403 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,404 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,404 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,404 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,404 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,404 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,404 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,404 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,404 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,404 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,404 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,404 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,404 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,404 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,404 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,405 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,405 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,405 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,405 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,405 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,405 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,405 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,405 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,405 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,405 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,405 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,405 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,405 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,405 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,405 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,405 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,405 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,406 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,406 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,406 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,406 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,406 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,406 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,406 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,406 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,406 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,406 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,406 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,406 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,406 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,406 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,406 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,406 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,406 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,406 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,407 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,407 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,407 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,407 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,407 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,407 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,407 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,407 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,407 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,407 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,407 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,407 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,407 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,407 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,407 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,408 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,408 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,408 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,408 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,408 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,408 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,408 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,408 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,408 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,408 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,408 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,408 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,408 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,408 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,408 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,408 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,408 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,408 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,409 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,409 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,409 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,409 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,409 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,409 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,409 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,409 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,409 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,409 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,409 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,409 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,409 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,409 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,409 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,409 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,409 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,409 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,410 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,410 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,410 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,410 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,410 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,410 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,410 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,410 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,410 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,410 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,410 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,410 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,410 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,410 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,410 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,410 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,411 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,411 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,411 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,411 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,411 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,411 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,411 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,411 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,411 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,411 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,411 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,411 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,411 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,411 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,411 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,411 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,411 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,411 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,412 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,412 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,412 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,412 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,412 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,412 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,412 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,412 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,412 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,412 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,412 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,412 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,412 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,412 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,412 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,412 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,412 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,413 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,413 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,413 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,413 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,413 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,413 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,413 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,413 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,413 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,413 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,413 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,413 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,413 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,414 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,414 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,414 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,414 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,414 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,414 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,414 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,414 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,414 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,414 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,414 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,414 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,414 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,414 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,414 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,414 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,414 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,415 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,415 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,415 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,415 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,415 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,415 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,415 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,415 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,415 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,415 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,415 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,415 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,415 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,415 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,415 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,416 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,416 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,416 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,416 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,416 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,416 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,416 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,416 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,416 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,416 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,416 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,416 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,416 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,416 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,416 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,416 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,416 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,416 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,417 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,417 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,417 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,417 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,417 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,417 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,417 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,417 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,417 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,417 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,417 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,417 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,417 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,417 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,417 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,417 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,417 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,418 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,418 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,418 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,418 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,418 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,418 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,418 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,418 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,418 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,418 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,418 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,418 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,418 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,418 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,418 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,418 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,418 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,419 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,419 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,419 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,419 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,419 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,419 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,419 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,419 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,419 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,419 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,419 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,419 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,419 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,419 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,419 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,419 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,419 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,419 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,420 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,420 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,420 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,420 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,420 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,420 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,420 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,420 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,420 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,420 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,420 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,420 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,420 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,420 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,420 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,420 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,420 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,420 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,421 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,421 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,421 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,421 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,421 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,421 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,421 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,421 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,421 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,421 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,421 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,421 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,421 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,421 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,421 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,421 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,421 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,421 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,421 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,422 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,422 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,422 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,422 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,422 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,422 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,422 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,422 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,422 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,422 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,422 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,422 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,423 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,423 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,423 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,423 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,423 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,423 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,423 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,423 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,423 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,423 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,423 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,423 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,424 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,424 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,424 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,424 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,424 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,424 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,424 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,424 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,424 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,424 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,424 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,424 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,425 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,425 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,425 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,425 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,425 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,425 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,425 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,426 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,426 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,426 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,426 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,426 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,426 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,426 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,426 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,427 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,427 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,427 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,427 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,427 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,427 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,427 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,427 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,427 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,427 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,427 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,427 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,428 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,428 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,428 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,428 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,428 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,428 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,428 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,428 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,428 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,428 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,428 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,428 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,429 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,429 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,429 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,429 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,429 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,429 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,429 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,429 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,429 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,429 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,429 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,429 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,430 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,430 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,430 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,430 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,430 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,430 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,430 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,430 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,430 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,430 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,430 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,430 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,430 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,431 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,431 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,431 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,431 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,431 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,431 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,431 [test.py:45 in test_hf_gen] INFO - for i in range(10):           
2023-10-31 14:27:06,431 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,431 [test.py:45 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious
2023-10-31 14:27:06,431 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,431 [test.py:45 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-31 14:27:06,431 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,432 [test.py:45 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone
2023-10-31 14:27:06,432 [test.py:46 in test_hf_gen] INFO - ----------
2023-10-31 14:27:06,459 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-31 14:27:06,460 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-31 14:27:06,460 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-31 14:27:06,460 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-31 14:27:06,460 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-31 14:27:06,460 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-31 14:27:06,460 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-31 14:27:06,460 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-31 14:27:06,461 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-31 14:27:06,461 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-31 14:27:06,461 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-31 14:27:06,461 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-31 14:27:06,461 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-31 14:27:06,461 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-31 14:27:06,461 [flexgen.py:84 in layer_reset] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-31 14:27:06,461 [flexgen.py:84 in layer_reset] DEBUG - lm_head from flexgen to old.
