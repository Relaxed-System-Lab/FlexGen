2023-10-09 05:38:19,633 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpr1roihtz
2023-10-09 05:38:19,633 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpr1roihtz/_remote_module_non_scriptable.py
2023-10-09 05:38:19,777 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-09 05:38:19,841 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-09 05:38:21,659 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-09 05:38:21,955 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-09 05:38:21,955 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-09 05:38:21,955 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-09 05:38:21,956 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-09 05:38:22,841 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-09 05:38:22,928 [model.py:158 in check_disk] INFO - [], ['lm_head.weight']
2023-10-09 05:38:22,969 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-09 05:38:23,054 [model.py:158 in check_disk] INFO - [], ['lm_head.weight']
2023-10-09 05:38:23,055 [model.py:181 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'
2023-10-09 05:38:23,061 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-09 05:38:23,062 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-09 05:38:23,062 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-09 05:38:23,063 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-09 05:38:23,064 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-09 05:38:23,065 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-09 05:38:23,066 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-09 05:38:23,067 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-09 05:38:23,068 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-09 05:38:23,068 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-09 05:38:23,069 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-09 05:38:23,070 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-09 05:38:23,071 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-09 05:38:23,072 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-09 05:38:23,073 [model.py:137 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-09 05:38:23,073 [model.py:137 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-09 05:38:23,073 [model.py:141 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-09 05:38:23,075 [model.py:147 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-09 05:38:23,076 [model.py:240 in init_all_weights] DEBUG - init all weights...
2023-10-09 05:38:23,103 [forward.py:44 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-09 05:38:23,103 [forward.py:44 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-09 05:38:23,103 [forward.py:44 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-09 05:38:23,103 [forward.py:44 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-09 05:38:23,104 [forward.py:44 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-09 05:38:23,104 [forward.py:44 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-09 05:38:23,104 [forward.py:44 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-09 05:38:23,104 [forward.py:44 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-09 05:38:23,104 [forward.py:44 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-09 05:38:23,104 [forward.py:44 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-09 05:38:23,104 [forward.py:44 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-09 05:38:23,105 [forward.py:44 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-09 05:38:23,105 [forward.py:44 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-09 05:38:23,105 [forward.py:44 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-09 05:38:23,105 [forward.py:44 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-09 05:38:23,105 [forward.py:44 in to_test_forward] DEBUG - lm_head to test forward
2023-10-09 05:38:23,147 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-09 05:38:23,344 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:23,345 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:23,346 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:23,347 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:23,347 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:23,361 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:23,364 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:23,372 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:23,374 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:23,382 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:23,384 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:23,390 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:23,392 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:23,398 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:23,400 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:23,407 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:23,408 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:23,414 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:23,415 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:23,422 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:23,423 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:23,430 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:23,431 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:23,438 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:23,439 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:23,445 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:23,447 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:23,453 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:23,455 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:23,455 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:23,456 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:23,464 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:23,469 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-09 05:38:23,469 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:38:23,478 [forward.py:24 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-09 05:38:23,478 [forward.py:24 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-09 05:38:23,479 [forward.py:24 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-09 05:38:23,479 [forward.py:24 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-09 05:38:23,479 [forward.py:24 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-09 05:38:23,479 [forward.py:24 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-09 05:38:23,479 [forward.py:24 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-09 05:38:23,479 [forward.py:24 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-09 05:38:23,479 [forward.py:24 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-09 05:38:23,479 [forward.py:24 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-09 05:38:23,479 [forward.py:24 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-09 05:38:23,480 [forward.py:24 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-09 05:38:23,480 [forward.py:24 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-09 05:38:23,480 [forward.py:24 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-09 05:38:23,480 [forward.py:24 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-09 05:38:23,480 [forward.py:24 in reset_forward] DEBUG - lm_head from test to old.
2023-10-09 05:38:23,480 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-09 05:38:23,480 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-09 05:38:23,480 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-09 05:38:23,480 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-09 05:38:23,481 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-09 05:38:23,481 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-09 05:38:23,481 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-09 05:38:23,481 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-09 05:38:23,481 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-09 05:38:23,481 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-09 05:38:23,481 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-09 05:38:23,481 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-09 05:38:23,481 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-09 05:38:23,482 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-09 05:38:23,482 [forward.py:115 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-09 05:38:23,482 [forward.py:115 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-09 05:38:23,521 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-09 05:38:23,690 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:23,690 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:23,691 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9]),)
2023-10-09 05:38:23,691 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:23,691 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9]),)
2023-10-09 05:38:23,691 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:23,691 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:23,691 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:23,692 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:23,692 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:23,692 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 768])
2023-10-09 05:38:23,692 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 768])
2023-10-09 05:38:23,692 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:23,693 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:23,693 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:23,697 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9]), 0)
2023-10-09 05:38:23,697 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:23,697 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9]), 0)
2023-10-09 05:38:23,697 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:23,697 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:23,698 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:23,698 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:23,698 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:23,698 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 768])
2023-10-09 05:38:23,699 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 768])
2023-10-09 05:38:23,699 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:23,699 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:23,702 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:23,706 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:38:23,706 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:23,707 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:38:23,707 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:23,707 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:23,712 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:23,716 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:23,722 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:23,777 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:38:23,778 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:38:23,778 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:23,782 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:23,795 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:23,802 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:38:23,803 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:23,803 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:38:23,803 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:23,804 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:23,870 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:23,915 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:23,943 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:23,959 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:38:23,960 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:38:23,960 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:23,962 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:23,965 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:23,969 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:38:23,969 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:23,969 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:38:23,970 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:23,970 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:23,978 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:23,990 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:23,993 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:23,997 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:38:23,997 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:38:23,998 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:23,999 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:24,003 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:24,007 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:38:24,007 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,007 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:38:24,007 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,007 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:24,011 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:24,014 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:24,017 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:24,020 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:38:24,021 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:38:24,021 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:24,022 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:24,026 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:24,030 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:38:24,030 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,030 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:38:24,030 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,030 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:24,034 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:24,037 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:24,040 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:24,043 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:38:24,043 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:38:24,043 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:24,045 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:24,049 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:24,053 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:38:24,053 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,054 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:38:24,054 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,054 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:24,059 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:24,063 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:24,066 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:24,070 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:38:24,071 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:38:24,071 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:24,072 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:24,076 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:24,080 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:38:24,080 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,080 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:38:24,080 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,080 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:24,085 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:24,090 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:24,095 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:24,098 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:38:24,099 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:38:24,099 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:24,101 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:24,104 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:24,108 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:38:24,108 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,108 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:38:24,108 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,108 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:24,112 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:24,116 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:24,120 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:24,124 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:38:24,124 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:38:24,124 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:24,126 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:24,129 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:24,133 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:38:24,133 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,133 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:38:24,133 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,133 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:24,138 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:24,143 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:24,146 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:24,151 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:38:24,151 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:38:24,151 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:24,153 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:24,156 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:24,160 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:38:24,160 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,160 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:38:24,160 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,161 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:24,166 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:24,169 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:24,173 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:24,177 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:38:24,177 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:38:24,177 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:24,179 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:24,182 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:24,186 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:38:24,187 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,187 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:38:24,187 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,187 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:24,191 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:24,196 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:24,200 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:24,203 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:38:24,204 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:38:24,204 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:24,205 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:24,209 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:24,210 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:38:24,210 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,210 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:38:24,210 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 9, 9]), 'layer_head_mask': None, 'past_key_value': None, 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,210 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:24,215 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:24,219 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:24,222 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:24,225 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 9, 768]), (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])))
2023-10-09 05:38:24,226 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 9, 768]), (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])))
2023-10-09 05:38:24,226 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:24,227 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:24,228 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:24,228 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:38:24,228 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:24,228 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:38:24,229 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:24,229 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:24,229 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:24,229 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:24,229 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:24,229 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 768])
2023-10-09 05:38:24,230 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 768])
2023-10-09 05:38:24,230 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:24,230 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:24,231 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:24,231 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 9, 768]),)
2023-10-09 05:38:24,231 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:24,232 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 9, 768]),)
2023-10-09 05:38:24,232 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:24,232 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:24,246 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:24,258 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:24,271 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:24,283 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 9, 50272])
2023-10-09 05:38:24,287 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 9, 50272])
2023-10-09 05:38:24,288 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:24,293 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:24,294 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:24,294 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:24,294 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:24,294 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:24,294 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:24,294 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:24,295 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:24,295 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:24,295 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:24,295 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:24,296 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:24,296 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:24,296 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:24,297 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:24,303 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 10]), 9)
2023-10-09 05:38:24,303 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:24,303 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 10]), 9)
2023-10-09 05:38:24,303 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:24,304 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:24,304 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:24,304 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:24,305 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:24,305 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:24,306 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:24,306 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:24,306 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:24,311 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:24,315 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,315 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,316 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,316 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,316 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:24,324 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:24,327 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:24,329 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:24,332 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:38:24,332 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:38:24,332 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:24,334 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:24,337 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:24,343 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,343 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,343 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,343 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,344 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:24,347 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:24,351 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:24,354 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:24,357 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:38:24,358 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:38:24,358 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:24,361 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:24,365 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:24,370 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,370 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,370 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,370 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,370 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:24,374 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:24,379 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:24,382 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:24,384 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:38:24,385 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:38:24,385 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:24,387 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:24,393 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:24,397 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,397 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,397 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,398 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,398 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:24,401 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:24,403 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:24,405 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:24,407 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:38:24,407 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:38:24,408 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:24,409 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:24,412 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:24,416 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,416 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,417 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,417 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,417 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:24,420 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:24,422 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:24,424 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:24,426 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:38:24,427 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:38:24,427 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:24,428 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:24,432 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:24,435 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,435 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,436 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,436 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,436 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:24,439 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:24,442 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:24,444 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:24,446 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:38:24,446 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:38:24,446 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:24,448 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:24,451 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:24,455 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,455 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,455 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,455 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,455 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:24,459 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:24,461 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:24,463 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:24,465 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:38:24,465 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:38:24,466 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:24,467 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:24,470 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:24,474 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,474 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,474 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,474 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,474 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:24,478 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:24,480 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:24,482 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:24,484 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:38:24,484 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:38:24,485 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:24,486 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:24,489 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:24,493 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,493 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,493 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,494 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,494 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:24,497 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:24,499 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:24,501 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:24,502 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:38:24,503 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:38:24,503 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:24,504 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:24,508 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:24,511 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,511 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,512 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,512 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,512 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:24,515 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:24,517 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:24,518 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:24,520 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:38:24,520 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:38:24,521 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:24,522 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:24,525 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:24,529 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,529 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,529 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,529 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,529 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:24,532 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:24,534 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:24,536 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:24,538 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:38:24,539 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:38:24,539 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:24,540 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:24,543 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:24,544 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,544 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 9, 64]), torch.Size([8, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,544 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,545 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 10]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 9, 64]), torch.Size([2, 12, 9, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,545 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:24,548 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:24,550 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:24,552 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:24,554 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])))
2023-10-09 05:38:24,554 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])))
2023-10-09 05:38:24,554 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:24,555 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:24,556 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:24,556 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,557 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:24,557 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,557 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:24,557 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:24,559 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:24,559 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:24,559 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:24,559 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:24,560 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:24,560 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:24,560 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:24,560 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:24,561 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,561 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:24,561 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,561 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:24,561 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:24,573 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:24,583 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:24,595 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:24,604 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:24,605 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:24,605 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:24,610 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:24,611 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:24,611 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:24,612 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:24,612 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:24,612 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:24,612 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:24,612 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:24,612 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:24,613 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:24,613 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:24,613 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:24,613 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:24,614 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:24,614 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:24,618 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 11]), 10)
2023-10-09 05:38:24,618 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:24,618 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 11]), 10)
2023-10-09 05:38:24,618 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:24,618 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:24,619 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:24,619 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:24,619 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:24,619 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:24,620 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:24,620 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:24,620 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:24,623 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:24,627 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,627 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,627 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,627 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,627 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:24,631 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:24,634 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:24,636 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:24,640 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:38:24,641 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:38:24,641 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:24,642 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:24,646 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:24,650 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,650 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,650 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,650 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,651 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:24,654 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:24,656 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:24,658 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:24,660 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:38:24,660 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:38:24,661 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:24,662 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:24,665 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:24,669 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,669 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,669 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,669 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,670 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:24,673 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:24,675 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:24,676 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:24,683 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:38:24,683 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:38:24,683 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:24,685 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:24,689 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:24,693 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,693 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,693 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,693 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,693 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:24,701 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:24,704 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:24,706 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:24,709 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:38:24,709 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:38:24,710 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:24,711 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:24,715 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:24,719 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,719 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,719 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,720 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,720 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:24,724 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:24,726 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:24,728 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:24,731 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:38:24,731 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:38:24,731 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:24,733 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:24,737 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:24,741 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,741 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,741 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,741 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,741 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:24,745 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:24,747 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:24,749 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:24,751 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:38:24,751 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:38:24,751 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:24,753 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:24,756 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:24,761 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,761 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,761 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,761 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,761 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:24,764 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:24,766 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:24,769 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:24,771 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:38:24,772 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:38:24,772 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:24,774 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:24,777 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:24,781 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,781 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,781 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,782 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,782 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:24,785 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:24,787 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:24,789 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:24,791 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:38:24,792 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:38:24,792 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:24,793 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:24,797 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:24,801 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,801 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,801 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,801 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,802 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:24,827 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:24,830 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:24,832 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:24,835 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:38:24,835 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:38:24,835 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:24,837 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:24,841 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:24,844 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,845 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,845 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,845 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,845 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:24,849 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:24,851 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:24,853 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:24,855 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:38:24,856 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:38:24,856 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:24,858 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:24,862 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:24,866 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,866 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,866 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,866 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,867 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:24,870 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:24,872 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:24,875 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:24,876 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:38:24,877 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:38:24,877 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:24,879 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:24,882 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:24,883 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,883 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 10, 64]), torch.Size([8, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,883 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,883 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 11]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 10, 64]), torch.Size([2, 12, 10, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,884 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:24,888 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:24,891 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:24,893 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:24,896 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])))
2023-10-09 05:38:24,896 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])))
2023-10-09 05:38:24,897 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:24,898 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:24,899 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:24,899 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,899 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:24,899 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,899 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:24,899 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:24,900 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:24,900 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:24,900 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:24,900 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:24,900 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:24,901 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:24,901 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:24,901 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:24,902 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,902 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:24,902 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,902 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:24,902 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:24,912 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:24,919 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:24,927 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:24,935 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:24,936 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:24,936 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:24,941 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:24,941 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:24,942 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:24,942 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:24,942 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:24,942 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:24,942 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:24,943 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:24,943 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:24,943 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:24,943 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:24,943 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:24,943 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:24,944 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:24,944 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:24,948 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 12]), 11)
2023-10-09 05:38:24,948 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:24,949 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 12]), 11)
2023-10-09 05:38:24,949 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:24,949 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:24,949 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:24,949 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:24,950 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:24,950 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:24,950 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:24,950 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:24,950 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:24,954 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:24,958 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,958 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,958 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,958 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,958 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:24,962 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:24,964 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:24,966 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:24,969 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:38:24,970 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:38:24,970 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:24,972 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:24,975 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:24,979 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:24,979 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,980 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:24,980 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:24,980 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:24,984 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:24,986 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:24,988 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:24,990 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:38:24,990 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:38:24,990 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:24,992 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:24,996 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:24,999 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,000 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,000 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,000 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,000 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:25,003 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:25,005 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:25,008 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:25,010 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:38:25,010 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:38:25,010 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:25,012 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:25,015 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:25,019 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,019 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,020 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,020 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,020 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:25,028 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:25,037 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:25,039 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:25,041 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:38:25,042 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:38:25,042 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:25,044 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:25,047 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:25,051 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,051 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,052 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,052 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,052 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:25,056 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:25,059 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:25,061 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:25,062 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:38:25,063 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:38:25,063 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:25,065 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:25,068 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:25,072 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,072 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,072 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,072 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,072 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:25,075 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:25,077 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:25,080 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:25,082 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:38:25,083 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:38:25,083 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:25,084 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:25,087 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:25,091 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,092 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,092 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,092 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,092 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:25,095 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:25,097 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:25,099 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:25,101 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:38:25,102 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:38:25,102 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:25,103 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:25,107 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:25,110 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,111 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,111 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,111 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,111 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:25,114 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:25,116 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:25,118 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:25,120 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:38:25,120 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:38:25,120 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:25,121 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:25,125 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:25,129 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,129 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,129 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,129 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,130 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:25,139 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:25,141 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:25,143 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:25,146 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:38:25,147 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:38:25,148 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:25,150 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:25,155 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:25,160 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,160 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,160 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,160 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,161 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:25,164 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:25,166 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:25,168 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:25,170 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:38:25,171 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:38:25,171 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:25,172 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:25,176 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:25,180 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,181 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,181 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,181 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,181 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:25,184 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:25,186 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:25,188 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:25,190 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:38:25,190 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:38:25,191 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:25,192 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:25,196 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:25,196 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,196 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 11, 64]), torch.Size([8, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,197 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,197 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 12]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 11, 64]), torch.Size([2, 12, 11, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,197 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:25,204 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:25,206 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:25,208 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:25,210 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])))
2023-10-09 05:38:25,211 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])))
2023-10-09 05:38:25,211 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:25,212 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:25,213 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:25,214 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,214 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:25,214 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,214 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:25,214 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:25,214 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:25,214 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:25,215 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:25,215 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:25,215 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:25,215 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:25,216 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:25,216 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:25,216 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,217 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:25,217 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,217 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:25,217 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:25,228 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:25,237 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:25,245 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:25,255 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:25,256 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:25,256 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:25,262 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:25,263 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:25,263 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:25,263 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:25,264 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:25,264 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:25,264 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:25,264 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:25,265 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:25,265 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:25,265 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:25,265 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:25,265 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:25,266 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:25,266 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:25,270 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 13]), 12)
2023-10-09 05:38:25,270 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:25,270 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 13]), 12)
2023-10-09 05:38:25,270 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:25,271 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:25,271 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:25,271 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:25,271 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:25,272 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:25,272 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:25,272 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:25,272 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:25,276 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:25,279 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,280 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,280 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,280 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,280 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:25,285 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:25,287 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:25,289 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:25,292 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:38:25,292 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:38:25,292 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:25,294 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:25,297 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:25,301 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,301 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,301 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,302 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,302 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:25,305 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:25,307 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:25,310 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:25,313 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:38:25,313 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:38:25,314 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:25,315 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:25,319 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:25,323 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,323 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,323 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,323 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,323 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:25,327 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:25,329 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:25,331 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:25,333 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:38:25,334 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:38:25,334 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:25,335 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:25,339 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:25,343 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,343 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,343 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,343 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,343 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:25,347 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:25,349 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:25,351 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:25,353 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:38:25,354 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:38:25,354 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:25,356 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:25,359 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:25,363 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,363 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,363 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,364 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,364 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:25,367 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:25,369 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:25,371 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:25,373 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:38:25,373 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:38:25,374 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:25,375 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:25,379 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:25,383 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,383 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,383 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,383 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,383 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:25,388 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:25,390 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:25,392 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:25,394 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:38:25,395 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:38:25,395 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:25,396 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:25,399 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:25,403 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,404 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,404 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,404 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,404 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:25,414 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:25,423 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:25,427 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:25,429 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:38:25,429 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:38:25,430 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:25,432 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:25,437 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:25,443 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,443 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,444 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,444 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,444 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:25,448 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:25,451 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:25,453 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:25,455 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:38:25,456 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:38:25,456 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:25,458 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:25,464 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:25,470 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,470 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,470 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,471 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,471 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:25,475 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:25,477 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:25,480 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:25,483 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:38:25,484 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:38:25,484 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:25,487 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:25,491 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:25,494 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,495 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,495 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,495 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,495 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:25,498 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:25,500 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:25,502 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:25,505 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:38:25,505 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:38:25,505 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:25,506 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:25,512 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:25,518 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,518 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,519 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,519 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,519 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:25,523 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:25,525 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:25,528 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:25,530 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:38:25,530 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:38:25,531 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:25,533 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:25,539 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:25,540 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,540 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 12, 64]), torch.Size([8, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,540 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,540 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 13]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 12, 64]), torch.Size([2, 12, 12, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,540 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:25,545 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:25,547 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:25,549 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:25,552 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])))
2023-10-09 05:38:25,552 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])))
2023-10-09 05:38:25,553 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:25,554 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:25,555 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:25,555 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,555 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:25,555 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,555 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:25,555 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:25,556 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:25,556 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:25,556 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:25,556 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:25,556 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:25,557 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:25,557 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:25,557 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:25,558 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,558 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:25,558 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,558 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:25,558 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:25,568 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:25,575 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:25,582 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:25,590 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:25,591 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:25,591 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:25,596 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:25,596 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:25,597 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:25,597 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:25,598 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:25,598 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:25,598 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:25,598 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:25,599 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:25,599 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:25,599 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:25,600 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:25,600 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:25,601 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:25,602 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:25,606 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 14]), 13)
2023-10-09 05:38:25,606 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:25,606 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 14]), 13)
2023-10-09 05:38:25,607 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:25,607 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:25,607 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:25,608 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:25,608 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:25,608 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:25,609 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:25,609 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:25,609 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:25,613 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:25,617 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,617 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,617 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,617 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,617 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:25,620 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:25,622 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:25,624 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:25,626 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:38:25,626 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:38:25,626 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:25,628 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:25,631 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:25,635 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,635 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,635 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,636 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,636 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:25,639 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:25,641 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:25,642 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:25,644 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:38:25,644 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:38:25,645 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:25,646 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:25,650 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:25,654 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,654 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,654 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,654 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,654 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:25,657 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:25,659 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:25,661 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:25,662 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:38:25,663 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:38:25,663 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:25,665 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:25,668 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:25,672 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,672 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,672 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,672 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,673 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:25,676 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:25,678 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:25,680 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:25,682 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:38:25,682 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:38:25,682 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:25,684 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:25,687 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:25,691 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,691 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,691 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,692 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,692 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:25,695 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:25,697 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:25,699 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:25,700 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:38:25,701 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:38:25,701 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:25,702 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:25,706 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:25,710 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,710 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,710 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,710 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,710 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:25,718 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:25,720 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:25,723 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:25,726 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:38:25,727 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:38:25,727 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:25,729 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:25,734 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:25,738 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,738 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,738 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,739 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,739 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:25,742 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:25,744 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:25,746 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:25,748 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:38:25,748 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:38:25,748 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:25,750 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:25,754 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:25,758 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,758 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,758 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,758 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,758 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:25,762 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:25,765 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:25,766 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:25,769 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:38:25,769 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:38:25,770 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:25,771 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:25,776 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:25,780 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,780 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,780 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,781 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,781 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:25,784 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:25,786 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:25,788 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:25,789 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:38:25,790 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:38:25,790 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:25,792 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:25,795 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:25,799 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,799 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,800 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,800 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,800 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:25,811 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:25,813 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:25,815 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:25,817 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:38:25,817 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:38:25,818 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:25,819 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:25,823 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:25,826 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,827 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,827 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,827 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,827 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:25,830 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:25,833 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:25,835 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:25,837 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:38:25,837 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:38:25,838 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:25,839 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:25,843 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:25,843 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,844 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 13, 64]), torch.Size([8, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,844 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,844 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 14]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 13, 64]), torch.Size([2, 12, 13, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,844 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:25,848 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:25,850 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:25,852 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:25,854 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])))
2023-10-09 05:38:25,855 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])))
2023-10-09 05:38:25,855 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:25,856 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:25,857 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:25,857 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,857 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:25,857 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,858 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:25,858 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:25,860 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:25,860 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:25,860 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:25,861 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:25,861 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:25,861 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:25,862 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:25,863 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:25,863 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,863 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:25,863 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,864 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:25,864 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:25,874 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:25,884 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:25,895 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:25,910 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:25,911 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:25,911 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:25,915 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:25,916 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:25,916 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:25,916 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:25,916 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:25,917 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:25,917 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:25,917 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:25,917 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:25,917 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:25,918 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:25,918 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:25,918 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:25,918 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:25,919 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:25,923 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 15]), 14)
2023-10-09 05:38:25,923 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:25,923 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 15]), 14)
2023-10-09 05:38:25,923 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:25,923 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:25,923 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:25,924 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:25,924 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:25,924 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:25,925 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:25,925 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:25,925 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:25,928 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:25,932 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,932 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,933 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,933 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,933 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:25,937 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:25,939 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:25,941 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:25,944 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:38:25,944 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:38:25,945 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:25,946 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:25,950 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:25,954 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,954 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,954 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,954 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,954 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:25,958 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:25,960 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:25,961 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:25,964 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:38:25,964 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:38:25,964 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:25,966 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:25,969 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:25,973 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,973 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,974 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,974 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,974 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:25,978 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:25,980 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:25,982 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:25,985 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:38:25,985 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:38:25,985 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:25,987 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:25,990 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:25,994 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:25,994 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,995 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:25,995 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:25,995 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:25,999 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:26,001 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:26,003 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:26,006 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:38:26,007 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:38:26,007 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:26,008 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:26,012 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:26,016 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,016 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,016 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,017 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,017 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:26,021 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:26,023 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:26,025 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:26,027 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:38:26,027 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:38:26,028 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:26,029 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:26,033 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:26,036 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,037 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,037 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,037 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,037 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:26,040 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:26,042 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:26,044 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:26,046 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:38:26,047 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:38:26,047 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:26,048 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:26,052 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:26,056 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,056 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,056 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,056 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,056 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:26,060 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:26,063 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:26,065 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:26,067 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:38:26,067 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:38:26,068 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:26,069 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:26,073 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:26,077 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,077 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,077 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,077 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,077 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:26,081 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:26,083 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:26,085 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:26,088 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:38:26,088 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:38:26,088 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:26,089 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:26,093 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:26,097 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,097 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,097 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,098 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,098 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:26,102 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:26,104 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:26,107 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:26,109 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:38:26,110 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:38:26,110 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:26,112 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:26,115 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:26,119 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,119 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,119 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,119 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,119 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:26,123 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:26,125 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:26,127 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:26,130 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:38:26,130 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:38:26,130 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:26,132 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:26,135 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:26,139 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,139 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,139 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,140 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,140 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:26,145 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:26,148 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:26,150 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:26,152 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:38:26,153 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:38:26,153 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:26,155 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:26,158 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:26,159 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,159 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 14, 64]), torch.Size([8, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,159 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,159 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 15]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 14, 64]), torch.Size([2, 12, 14, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,159 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:26,164 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:26,167 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:26,169 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:26,172 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])))
2023-10-09 05:38:26,172 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])))
2023-10-09 05:38:26,172 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:26,174 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:26,174 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:26,175 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,175 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:26,175 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,175 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:26,175 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:26,175 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:26,176 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:26,176 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:26,176 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:26,176 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:26,176 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:26,177 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:26,177 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:26,178 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,178 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:26,178 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,178 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:26,178 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:26,189 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:26,197 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:26,205 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:26,214 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:26,215 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:26,215 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:26,220 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:26,220 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:26,221 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:26,221 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:26,221 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:26,221 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:26,221 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:26,221 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:26,222 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:26,222 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:26,222 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:26,222 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:26,222 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:26,223 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:26,223 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:26,227 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 16]), 15)
2023-10-09 05:38:26,227 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:26,227 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 16]), 15)
2023-10-09 05:38:26,228 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:26,228 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:26,228 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:26,228 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:26,229 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:26,229 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:26,229 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:26,229 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:26,229 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:26,233 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:26,237 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,237 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,237 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,237 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,237 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:26,243 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:26,245 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:26,247 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:26,249 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:38:26,249 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:38:26,249 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:26,251 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:26,255 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:26,258 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,259 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,259 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,259 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,259 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:26,262 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:26,264 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:26,266 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:26,268 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:38:26,268 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:38:26,269 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:26,270 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:26,274 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:26,277 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,278 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,278 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,278 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,278 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:26,281 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:26,283 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:26,285 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:26,287 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:38:26,288 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:38:26,288 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:26,290 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:26,293 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:26,297 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,297 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,297 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,297 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,298 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:26,301 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:26,303 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:26,305 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:26,307 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:38:26,308 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:38:26,308 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:26,310 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:26,313 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:26,317 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,317 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,317 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,318 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,318 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:26,321 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:26,323 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:26,325 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:26,327 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:38:26,328 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:38:26,328 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:26,329 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:26,333 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:26,337 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,337 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,337 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,337 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,337 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:26,340 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:26,342 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:26,344 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:26,346 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:38:26,347 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:38:26,347 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:26,348 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:26,352 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:26,356 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,356 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,356 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,356 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,356 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:26,360 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:26,363 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:26,365 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:26,367 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:38:26,367 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:38:26,368 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:26,370 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:26,373 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:26,377 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,377 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,377 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,377 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,378 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:26,381 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:26,384 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:26,386 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:26,388 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:38:26,389 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:38:26,389 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:26,390 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:26,394 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:26,398 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,398 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,398 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,398 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,398 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:26,403 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:26,406 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:26,407 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:26,410 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:38:26,410 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:38:26,411 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:26,412 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:26,416 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:26,420 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,420 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,420 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,420 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,420 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:26,424 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:26,426 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:26,427 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:26,429 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:38:26,430 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:38:26,430 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:26,431 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:26,435 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:26,439 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,439 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,439 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,439 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,439 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:26,450 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:26,453 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:26,455 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:26,458 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:38:26,458 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:38:26,459 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:26,460 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:26,464 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:26,465 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,465 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 15, 64]), torch.Size([8, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,465 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,465 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 16]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 15, 64]), torch.Size([2, 12, 15, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,465 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:26,483 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:26,485 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:26,487 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:26,489 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])))
2023-10-09 05:38:26,490 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])))
2023-10-09 05:38:26,490 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:26,491 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:26,492 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:26,492 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,493 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:26,493 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,493 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:26,493 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:26,493 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:26,494 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:26,494 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:26,494 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:26,494 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:26,495 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:26,495 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:26,495 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:26,496 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,496 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:26,496 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,496 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:26,496 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:26,505 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:26,513 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:26,521 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:26,531 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:26,532 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:26,532 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:26,537 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:26,537 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:26,538 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:26,538 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:26,538 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:26,538 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:26,538 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:26,538 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:26,539 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:26,539 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:26,539 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:26,539 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:26,539 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:26,540 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:26,540 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:26,544 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 17]), 16)
2023-10-09 05:38:26,544 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:26,544 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 17]), 16)
2023-10-09 05:38:26,545 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:26,545 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:26,545 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:26,545 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:26,546 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:26,546 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:26,546 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:26,546 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:26,547 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:26,550 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:26,554 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,554 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,554 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,554 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,555 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:26,558 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:26,560 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:26,562 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:26,564 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:38:26,565 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:38:26,565 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:26,567 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:26,570 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:26,574 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,574 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,575 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,575 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,575 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:26,579 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:26,581 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:26,583 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:26,586 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:38:26,586 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:38:26,586 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:26,588 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:26,591 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:26,595 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,595 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,596 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,596 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,596 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:26,600 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:26,602 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:26,605 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:26,607 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:38:26,608 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:38:26,608 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:26,610 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:26,613 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:26,617 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,617 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,617 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,617 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,617 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:26,620 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:26,622 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:26,626 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:26,628 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:38:26,629 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:38:26,629 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:26,630 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:26,634 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:26,638 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,638 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,638 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,638 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,639 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:26,642 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:26,644 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:26,646 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:26,649 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:38:26,649 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:38:26,649 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:26,651 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:26,655 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:26,659 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,659 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,660 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,660 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,660 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:26,663 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:26,665 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:26,667 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:26,669 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:38:26,670 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:38:26,670 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:26,671 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:26,675 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:26,679 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,679 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,679 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,679 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,679 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:26,683 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:26,684 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:26,686 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:26,688 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:38:26,688 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:38:26,688 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:26,690 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:26,693 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:26,697 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,697 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,698 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,698 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,698 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:26,706 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:26,709 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:26,710 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:26,713 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:38:26,713 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:38:26,713 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:26,715 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:26,718 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:26,724 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,724 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,724 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,724 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,725 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:26,729 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:26,732 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:26,734 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:26,737 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:38:26,737 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:38:26,738 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:26,740 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:26,744 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:26,747 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,747 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,748 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,748 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,748 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:26,751 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:26,754 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:26,756 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:26,759 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:38:26,759 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:38:26,759 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:26,762 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:26,766 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:26,770 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,770 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,770 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,770 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,770 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:26,773 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:26,775 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:26,778 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:26,780 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:38:26,781 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:38:26,781 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:26,783 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:26,788 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:26,789 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,789 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 16, 64]), torch.Size([8, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,790 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,790 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 17]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 16, 64]), torch.Size([2, 12, 16, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,790 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:26,794 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:26,797 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:26,799 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:26,802 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])))
2023-10-09 05:38:26,802 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])))
2023-10-09 05:38:26,803 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:26,804 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:26,805 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:26,805 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,805 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:26,805 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,806 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:26,806 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:26,806 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:26,806 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:26,806 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:26,807 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:26,807 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:26,807 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:26,807 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:26,808 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:26,808 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,808 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:26,808 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,808 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:26,809 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:26,818 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:26,827 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:26,836 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:26,845 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:26,846 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:26,846 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:26,851 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:26,852 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:26,852 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:26,853 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:26,853 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:26,853 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:26,853 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:26,853 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:26,853 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:26,853 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:26,854 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:26,854 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:26,854 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:26,854 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:26,855 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:26,859 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 18]), 17)
2023-10-09 05:38:26,859 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:26,859 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 18]), 17)
2023-10-09 05:38:26,859 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:26,859 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:26,859 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:26,860 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:26,860 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:26,860 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:26,860 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:26,861 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:26,861 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:26,864 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:26,868 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,868 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,868 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,868 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,869 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:26,872 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:26,875 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:26,877 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:26,880 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:38:26,881 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:38:26,881 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:26,883 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:26,887 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:26,891 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,891 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,891 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,891 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,891 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:26,895 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:26,898 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:26,901 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:26,903 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:38:26,903 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:38:26,903 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:26,905 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:26,909 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:26,913 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,913 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,914 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,914 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,914 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:26,918 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:26,920 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:26,922 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:26,925 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:38:26,926 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:38:26,926 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:26,928 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:26,932 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:26,936 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,936 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,937 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,937 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,937 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:26,940 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:26,943 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:26,946 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:26,948 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:38:26,949 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:38:26,949 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:26,951 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:26,955 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:26,959 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,959 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,960 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,960 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,960 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:26,965 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:26,968 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:26,971 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:26,973 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:38:26,974 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:38:26,974 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:26,976 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:26,979 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:26,983 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:26,983 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,984 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:26,984 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:26,984 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:26,987 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:26,990 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:26,992 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:26,994 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:38:26,995 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:38:26,995 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:26,996 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:27,000 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:27,004 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,004 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,004 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,005 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,005 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:27,009 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:27,011 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:27,014 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:27,016 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:38:27,016 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:38:27,016 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:27,018 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:27,022 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:27,026 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,026 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,026 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,026 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,026 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:27,030 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:27,032 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:27,034 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:27,037 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:38:27,037 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:38:27,038 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:27,039 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:27,044 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:27,049 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,049 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,049 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,049 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,049 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:27,053 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:27,055 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:27,057 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:27,059 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:38:27,060 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:38:27,060 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:27,062 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:27,066 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:27,070 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,070 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,070 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,070 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,070 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:27,073 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:27,075 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:27,079 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:27,080 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:38:27,081 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:38:27,081 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:27,082 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:27,086 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:27,090 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,090 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,090 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,090 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,090 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:27,093 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:27,095 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:27,097 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:27,099 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:38:27,099 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:38:27,099 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:27,101 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:27,104 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:27,105 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,105 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 17, 64]), torch.Size([8, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,105 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,106 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 18]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 17, 64]), torch.Size([2, 12, 17, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,106 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:27,109 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:27,111 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:27,113 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:27,114 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])))
2023-10-09 05:38:27,115 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])))
2023-10-09 05:38:27,115 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:27,116 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:27,117 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:27,117 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,117 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:27,118 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,118 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:27,118 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:27,118 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:27,118 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:27,118 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:27,119 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:27,119 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:27,119 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:27,119 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:27,120 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:27,120 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,120 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:27,120 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,120 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:27,120 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:27,129 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:27,137 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:27,144 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:27,151 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:27,153 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:27,153 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:27,157 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:27,158 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:27,158 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:27,158 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:27,159 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:27,159 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:27,159 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:27,159 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:27,160 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:27,160 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:27,160 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:27,160 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:27,161 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:27,161 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:27,162 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:27,166 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 19]), 18)
2023-10-09 05:38:27,166 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:27,166 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 19]), 18)
2023-10-09 05:38:27,166 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:27,166 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:27,166 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:27,167 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:27,167 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:27,167 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:27,167 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:27,167 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:27,168 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:27,171 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:27,175 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,175 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,175 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,175 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,175 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:27,179 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:27,181 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:27,182 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:27,184 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:38:27,184 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:38:27,185 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:27,186 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:27,190 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:27,194 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,194 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,194 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,194 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,194 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:27,197 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:27,199 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:27,201 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:27,203 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:38:27,203 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:38:27,203 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:27,205 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:27,208 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:27,212 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,212 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,212 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,213 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,213 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:27,216 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:27,219 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:27,220 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:27,222 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:38:27,223 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:38:27,223 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:27,224 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:27,228 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:27,232 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,232 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,232 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,232 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,232 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:27,235 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:27,237 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:27,239 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:27,240 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:38:27,241 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:38:27,241 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:27,242 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:27,246 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:27,250 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,250 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,250 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,250 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,250 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:27,254 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:27,256 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:27,258 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:27,259 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:38:27,260 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:38:27,260 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:27,261 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:27,265 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:27,269 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,269 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,269 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,269 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,269 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:27,273 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:27,275 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:27,276 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:27,278 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:38:27,278 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:38:27,279 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:27,280 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:27,283 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:27,287 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,288 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,288 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,288 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,288 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:27,291 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:27,293 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:27,295 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:27,296 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:38:27,297 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:38:27,297 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:27,299 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:27,302 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:27,306 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,306 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,307 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,307 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,307 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:27,310 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:27,312 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:27,314 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:27,315 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:38:27,316 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:38:27,316 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:27,317 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:27,321 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:27,325 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,325 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,325 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,325 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,326 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:27,328 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:27,330 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:27,333 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:27,335 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:38:27,336 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:38:27,336 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:27,337 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:27,341 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:27,344 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,344 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,345 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,345 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,345 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:27,348 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:27,350 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:27,352 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:27,354 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:38:27,355 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:38:27,355 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:27,356 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:27,360 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:27,364 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,364 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,364 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,364 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,364 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:27,368 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:27,370 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:27,372 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:27,374 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:38:27,374 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:38:27,375 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:27,376 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:27,380 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:27,380 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,381 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 18, 64]), torch.Size([8, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,381 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,381 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 19]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 18, 64]), torch.Size([2, 12, 18, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,381 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:27,388 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:27,391 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:27,393 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:27,395 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])))
2023-10-09 05:38:27,396 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])))
2023-10-09 05:38:27,396 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:27,398 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:27,398 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:27,399 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,399 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:27,399 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,399 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:27,399 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:27,401 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:27,401 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:27,402 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:27,402 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:27,402 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:27,402 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:27,402 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:27,403 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:27,403 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,403 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:27,404 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,404 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:27,404 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:27,416 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:27,426 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:27,438 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:27,447 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:27,448 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:27,448 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:27,453 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:27,453 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:27,454 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:27,454 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:27,454 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:27,454 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:27,454 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:27,455 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:27,455 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:27,455 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:27,455 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:27,455 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:27,456 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:27,456 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:27,457 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:27,461 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 20]), 19)
2023-10-09 05:38:27,461 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:27,461 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 20]), 19)
2023-10-09 05:38:27,461 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:27,461 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:27,461 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:27,462 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:27,462 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:27,462 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:27,462 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:27,463 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:27,463 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:27,466 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:27,470 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,470 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,471 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,471 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,471 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:27,474 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:27,477 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:27,490 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:27,493 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:38:27,493 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:38:27,494 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:27,495 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:27,499 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:27,503 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,503 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,504 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,504 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,504 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:27,508 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:27,510 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:27,512 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:27,514 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:38:27,515 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:38:27,515 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:27,517 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:27,521 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:27,525 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,525 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,525 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,525 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,525 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:27,529 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:27,532 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:27,534 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:27,537 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:38:27,537 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:38:27,537 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:27,539 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:27,543 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:27,547 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,547 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,547 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,547 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,548 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:27,551 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:27,554 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:27,557 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:27,559 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:38:27,560 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:38:27,560 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:27,562 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:27,565 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:27,570 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,570 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,570 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,570 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,570 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:27,574 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:27,577 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:27,579 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:27,581 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:38:27,581 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:38:27,582 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:27,584 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:27,587 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:27,591 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,591 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,592 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,592 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,592 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:27,596 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:27,598 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:27,600 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:27,603 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:38:27,603 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:38:27,603 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:27,605 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:27,608 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:27,612 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,613 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,613 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,613 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,613 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:27,617 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:27,620 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:27,623 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:27,627 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:38:27,627 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:38:27,628 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:27,629 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:27,633 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:27,637 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,637 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,637 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,638 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,638 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:27,641 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:27,643 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:27,645 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:27,648 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:38:27,648 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:38:27,648 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:27,650 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:27,654 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:27,658 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,658 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,658 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,658 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,658 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:27,662 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:27,664 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:27,667 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:27,669 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:38:27,670 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:38:27,670 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:27,671 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:27,675 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:27,679 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,679 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,679 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,680 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,680 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:27,683 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:27,685 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:27,687 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:27,689 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:38:27,690 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:38:27,690 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:27,691 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:27,695 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:27,699 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,699 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,700 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,700 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,700 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:27,704 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:27,706 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:27,708 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:27,710 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:38:27,711 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:38:27,711 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:27,713 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:27,716 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:27,717 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,717 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 19, 64]), torch.Size([8, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,717 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,717 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 20]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 19, 64]), torch.Size([2, 12, 19, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,718 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:27,721 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:27,723 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:27,725 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:27,727 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])))
2023-10-09 05:38:27,727 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])))
2023-10-09 05:38:27,727 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:27,729 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:27,729 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:27,730 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,730 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:27,730 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,730 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:27,730 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:27,731 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:27,731 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:27,731 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:27,731 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:27,731 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:27,731 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:27,732 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:27,732 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:27,733 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,733 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:27,733 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,733 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:27,733 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:27,745 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:27,752 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:27,760 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:27,769 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:27,770 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:27,770 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:27,779 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:27,780 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:27,780 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:27,780 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:27,781 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:27,781 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:27,781 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:27,782 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:27,782 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:27,782 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:27,782 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:27,783 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:27,783 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:27,783 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:27,784 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:27,789 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 21]), 20)
2023-10-09 05:38:27,789 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:27,789 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 21]), 20)
2023-10-09 05:38:27,789 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:27,789 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:27,790 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:27,790 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:27,790 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:27,790 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:27,791 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:27,791 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:27,791 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:27,796 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:27,801 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,801 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,802 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,802 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,802 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:27,805 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:27,808 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:27,810 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:27,812 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:38:27,813 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:38:27,813 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:27,814 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:27,818 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:27,822 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,822 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,822 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,822 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,822 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:27,826 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:27,828 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:27,829 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:27,831 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:38:27,832 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:38:27,832 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:27,834 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:27,837 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:27,841 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,841 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,841 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,841 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,841 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:27,845 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:27,847 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:27,849 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:27,851 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:38:27,852 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:38:27,852 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:27,853 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:27,857 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:27,861 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,861 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,861 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,861 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,861 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:27,865 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:27,867 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:27,869 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:27,871 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:38:27,871 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:38:27,872 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:27,873 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:27,876 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:27,880 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,880 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,881 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,881 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,881 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:27,884 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:27,886 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:27,888 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:27,890 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:38:27,891 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:38:27,891 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:27,892 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:27,896 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:27,899 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,899 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,900 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,900 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,900 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:27,903 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:27,905 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:27,907 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:27,909 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:38:27,910 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:38:27,910 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:27,911 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:27,915 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:27,919 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,919 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,919 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,919 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,919 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:27,923 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:27,925 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:27,927 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:27,928 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:38:27,929 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:38:27,929 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:27,931 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:27,934 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:27,938 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,938 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,938 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,938 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,938 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:27,941 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:27,944 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:27,946 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:27,948 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:38:27,949 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:38:27,949 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:27,950 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:27,953 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:27,957 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,958 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,958 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,958 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,958 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:27,962 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:27,965 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:27,967 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:27,969 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:38:27,969 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:38:27,969 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:27,971 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:27,974 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:27,978 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,978 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,978 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,978 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,979 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:27,983 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:27,986 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:27,987 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:27,989 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:38:27,990 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:38:27,990 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:27,991 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:27,994 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:27,998 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:27,998 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,999 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:27,999 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:27,999 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:28,002 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:28,004 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:28,006 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:28,007 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:38:28,008 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:38:28,008 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:28,010 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:28,013 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:28,013 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,014 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 20, 64]), torch.Size([8, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,014 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,014 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 21]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 20, 64]), torch.Size([2, 12, 20, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,014 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:28,021 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:28,023 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:28,025 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:28,026 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])))
2023-10-09 05:38:28,027 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])))
2023-10-09 05:38:28,027 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:28,028 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:28,029 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:28,029 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,029 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,029 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,030 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,030 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:28,030 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:28,030 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:28,030 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:28,030 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:28,031 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:28,031 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:28,031 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:28,031 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:28,032 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,032 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,032 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,032 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,032 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:28,041 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:28,048 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:28,056 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:28,063 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:28,064 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:28,064 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:28,070 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:28,071 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:28,071 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:28,071 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,071 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:28,072 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,072 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:28,072 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:28,072 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:28,072 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:28,073 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:28,073 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:28,073 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:28,073 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:28,074 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:28,077 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 22]), 21)
2023-10-09 05:38:28,078 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,078 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 22]), 21)
2023-10-09 05:38:28,078 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,078 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:28,078 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:28,078 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:28,079 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:28,079 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:28,079 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:28,079 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:28,080 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:28,083 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:28,087 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,087 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,088 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,088 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,088 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:28,091 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:28,093 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:28,095 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:28,097 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:38:28,098 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:38:28,098 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:28,100 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:28,103 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:28,107 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,107 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,107 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,107 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,108 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:28,111 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:28,113 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:28,114 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:28,116 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:38:28,117 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:38:28,117 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:28,119 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:28,122 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:28,126 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,126 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,126 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,126 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,126 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:28,129 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:28,131 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:28,133 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:28,135 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:38:28,135 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:38:28,135 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:28,137 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:28,140 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:28,144 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,144 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,144 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,144 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,144 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:28,148 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:28,150 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:28,152 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:28,153 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:38:28,154 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:38:28,154 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:28,156 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:28,159 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:28,163 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,163 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,163 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,163 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,163 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:28,166 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:28,168 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:28,170 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:28,172 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:38:28,173 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:38:28,173 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:28,174 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:28,178 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:28,181 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,181 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,182 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,182 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,182 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:28,185 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:28,187 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:28,189 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:28,191 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:38:28,192 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:38:28,192 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:28,193 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:28,197 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:28,201 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,201 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,202 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,202 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,202 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:28,205 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:28,207 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:28,208 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:28,210 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:38:28,211 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:38:28,211 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:28,215 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:28,221 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:28,226 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,230 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,230 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,230 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,231 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:28,247 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:28,250 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:28,253 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:28,256 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:38:28,257 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:38:28,257 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:28,259 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:28,263 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:28,267 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,267 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,268 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,268 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,268 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:28,272 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:28,275 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:28,279 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:28,282 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:38:28,283 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:38:28,283 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:28,285 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:28,290 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:28,294 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,295 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,295 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,295 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,296 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:28,301 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:28,304 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:28,307 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:28,310 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:38:28,311 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:38:28,311 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:28,312 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:28,315 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:28,319 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,319 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,320 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,320 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,320 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:28,324 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:28,326 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:28,328 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:28,330 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:38:28,331 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:38:28,331 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:28,332 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:28,336 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:28,337 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,337 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 21, 64]), torch.Size([8, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,337 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,337 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 22]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 21, 64]), torch.Size([2, 12, 21, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,337 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:28,340 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:28,343 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:28,345 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:28,346 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])))
2023-10-09 05:38:28,347 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])))
2023-10-09 05:38:28,347 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:28,348 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:28,349 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:28,349 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,349 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,350 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,350 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,350 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:28,350 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:28,350 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:28,350 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:28,350 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:28,351 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:28,351 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:28,351 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:28,352 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:28,352 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,352 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,352 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,352 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,352 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:28,361 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:28,369 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:28,377 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:28,384 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:28,385 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:28,385 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:28,389 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:28,390 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:28,390 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:28,390 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,391 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:28,391 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,391 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:28,391 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:28,391 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:28,391 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:28,392 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:28,392 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:28,392 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:28,392 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:28,393 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:28,397 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 23]), 22)
2023-10-09 05:38:28,397 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,397 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 23]), 22)
2023-10-09 05:38:28,397 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,397 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:28,397 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:28,398 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:28,398 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:28,398 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:28,398 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:28,399 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:28,399 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:28,402 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:28,406 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,406 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,406 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,407 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,407 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:28,410 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:28,412 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:28,414 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:28,416 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:38:28,416 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:38:28,417 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:28,418 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:28,422 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:28,425 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,426 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,426 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,426 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,426 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:28,429 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:28,431 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:28,433 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:28,435 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:38:28,436 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:38:28,436 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:28,437 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:28,441 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:28,444 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,445 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,445 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,445 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,445 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:28,449 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:28,452 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:28,454 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:28,456 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:38:28,457 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:38:28,457 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:28,458 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:28,462 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:28,466 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,467 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,467 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,467 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,467 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:28,470 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:28,473 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:28,475 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:28,477 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:38:28,477 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:38:28,477 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:28,479 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:28,482 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:28,486 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,486 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,486 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,486 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,487 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:28,490 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:28,492 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:28,494 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:28,496 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:38:28,497 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:38:28,497 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:28,499 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:28,502 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:28,506 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,506 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,506 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,506 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,506 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:28,510 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:28,512 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:28,514 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:28,516 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:38:28,517 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:38:28,517 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:28,518 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:28,522 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:28,526 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,526 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,526 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,526 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,526 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:28,530 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:28,533 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:28,535 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:28,537 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:38:28,537 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:38:28,537 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:28,539 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:28,542 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:28,546 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,546 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,547 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,547 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,547 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:28,551 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:28,553 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:28,555 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:28,557 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:38:28,558 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:38:28,558 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:28,559 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:28,562 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:28,566 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,566 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,566 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,566 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,567 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:28,570 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:28,572 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:28,574 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:28,575 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:38:28,576 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:38:28,576 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:28,578 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:28,581 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:28,585 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,585 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,585 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,585 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,585 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:28,588 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:28,590 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:28,592 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:28,594 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:38:28,594 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:38:28,594 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:28,595 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:28,599 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:28,603 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,603 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,603 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,603 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,603 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:28,606 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:28,609 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:28,611 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:28,613 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:38:28,613 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:38:28,613 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:28,615 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:28,618 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:28,619 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,619 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 22, 64]), torch.Size([8, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,619 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,619 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 23]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 22, 64]), torch.Size([2, 12, 22, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,619 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:28,623 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:28,625 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:28,626 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:28,628 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])))
2023-10-09 05:38:28,629 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])))
2023-10-09 05:38:28,629 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:28,630 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:28,631 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:28,631 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,631 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,632 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,632 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,632 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:28,632 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:28,632 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:28,632 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:28,633 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:28,633 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:28,633 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:28,633 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:28,634 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:28,634 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,634 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,634 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,634 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,634 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:28,644 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:28,654 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:28,663 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:28,671 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:28,672 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:28,672 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:28,677 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:28,677 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:28,678 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:28,678 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,678 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:28,678 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,678 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:28,678 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:28,678 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:28,679 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:28,679 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:28,679 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:28,679 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:28,680 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:28,680 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:28,684 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 24]), 23)
2023-10-09 05:38:28,684 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,684 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 24]), 23)
2023-10-09 05:38:28,684 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,684 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:28,684 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:28,685 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:28,685 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:28,685 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:28,685 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:28,686 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:28,686 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:28,689 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:28,693 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,693 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,693 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,693 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,693 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:28,697 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:28,699 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:28,701 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:28,704 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:38:28,705 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:38:28,705 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:28,706 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:28,710 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:28,713 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,713 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,714 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,714 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,714 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:28,717 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:28,719 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:28,721 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:28,723 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:38:28,724 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:38:28,724 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:28,726 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:28,729 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:28,733 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,733 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,733 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,733 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,733 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:28,737 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:28,739 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:28,741 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:28,743 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:38:28,744 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:38:28,744 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:28,746 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:28,750 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:28,753 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,754 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,754 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,754 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,754 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:28,758 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:28,760 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:28,763 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:28,765 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:38:28,765 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:38:28,765 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:28,767 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:28,770 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:28,774 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,774 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,774 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,774 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,774 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:28,778 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:28,780 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:28,781 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:28,783 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:38:28,784 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:38:28,784 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:28,786 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:28,789 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:28,792 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,793 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,793 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,793 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,793 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:28,798 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:28,799 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:28,802 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:28,805 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:38:28,805 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:38:28,805 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:28,807 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:28,810 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:28,814 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,814 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,814 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,814 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,814 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:28,818 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:28,819 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:28,822 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:28,823 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:38:28,824 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:38:28,824 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:28,825 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:28,829 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:28,832 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,833 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,833 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,833 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,833 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:28,837 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:28,839 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:28,841 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:28,843 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:38:28,844 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:38:28,844 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:28,845 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:28,849 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:28,852 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,853 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,853 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,853 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,853 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:28,857 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:28,860 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:28,862 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:28,864 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:38:28,865 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:38:28,865 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:28,866 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:28,870 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:28,873 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,873 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,874 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,874 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,874 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:28,878 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:28,881 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:28,883 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:28,885 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:38:28,885 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:38:28,885 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:28,887 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:28,890 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:28,894 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,894 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,894 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,894 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,894 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:28,898 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:28,900 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:28,902 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:28,904 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:38:28,905 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:38:28,905 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:28,906 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:28,910 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:28,910 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,910 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 23, 64]), torch.Size([8, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,910 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,911 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 24]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 23, 64]), torch.Size([2, 12, 23, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,911 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:28,914 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:28,916 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:28,918 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:28,919 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])))
2023-10-09 05:38:28,920 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])))
2023-10-09 05:38:28,920 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:28,922 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:28,922 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:28,923 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,923 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,923 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,923 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,923 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:28,923 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:28,923 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:28,924 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:28,924 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:28,924 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:28,924 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:28,924 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:28,925 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:28,925 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,925 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,926 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,926 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,926 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:28,934 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:28,942 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:28,949 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:28,956 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:28,957 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:28,957 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:28,961 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:28,962 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:28,962 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:28,962 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,962 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:28,963 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,963 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:28,963 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:28,963 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:28,963 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:28,964 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:28,964 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:28,964 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:28,964 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:28,965 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:28,968 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 25]), 24)
2023-10-09 05:38:28,968 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:28,969 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 25]), 24)
2023-10-09 05:38:28,969 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:28,969 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:28,969 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:28,969 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:28,970 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:28,970 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:28,970 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:28,970 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:28,971 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:28,974 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:28,978 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,978 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,978 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,978 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,978 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:28,982 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:28,984 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:28,986 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:28,988 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:38:28,989 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:38:28,989 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:28,990 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:28,994 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:28,997 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:28,998 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,998 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:28,998 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:28,998 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:29,002 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:29,004 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:29,006 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:29,008 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:38:29,008 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:38:29,008 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:29,010 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:29,013 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:29,017 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,017 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,018 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,018 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,018 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:29,022 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:29,024 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:29,026 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:29,028 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:38:29,029 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:38:29,029 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:29,031 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:29,034 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:29,038 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,038 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,038 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,038 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,038 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:29,044 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:29,046 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:29,048 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:29,051 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:38:29,051 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:38:29,052 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:29,053 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:29,057 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:29,061 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,061 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,061 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,061 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,061 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:29,064 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:29,066 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:29,069 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:29,070 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:38:29,071 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:38:29,071 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:29,073 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:29,076 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:29,079 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,080 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,080 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,080 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,080 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:29,084 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:29,086 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:29,088 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:29,090 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:38:29,091 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:38:29,091 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:29,092 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:29,096 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:29,099 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,100 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,100 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,100 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,100 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:29,108 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:29,111 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:29,113 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:29,121 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:38:29,122 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:38:29,122 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:29,123 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:29,127 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:29,131 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,131 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,131 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,131 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,131 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:29,135 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:29,138 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:29,140 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:29,143 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:38:29,143 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:38:29,143 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:29,145 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:29,148 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:29,153 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,153 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,153 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,153 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,153 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:29,157 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:29,160 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:29,162 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:29,165 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:38:29,166 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:38:29,166 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:29,168 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:29,171 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:29,175 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,175 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,175 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,175 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,176 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:29,179 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:29,182 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:29,184 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:29,187 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:38:29,187 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:38:29,188 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:29,189 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:29,193 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:29,197 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,197 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,197 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,197 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,197 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:29,201 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:29,205 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:29,207 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:29,209 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:38:29,210 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:38:29,210 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:29,212 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:29,215 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:29,216 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,216 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 24, 64]), torch.Size([8, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,216 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,216 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 25]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 24, 64]), torch.Size([2, 12, 24, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,216 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:29,220 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:29,223 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:29,225 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:29,227 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])))
2023-10-09 05:38:29,227 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])))
2023-10-09 05:38:29,228 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:29,229 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:29,230 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:29,230 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,230 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:29,230 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,230 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:29,230 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:29,231 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:29,231 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:29,231 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:29,231 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:29,231 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:29,231 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:29,232 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:29,232 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:29,233 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,233 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:29,233 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,233 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:29,233 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:29,243 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:29,251 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:29,260 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:29,268 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:29,269 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:29,269 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:29,273 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:29,274 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:29,274 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:29,274 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:29,275 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:29,275 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:29,275 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:29,275 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:29,275 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:29,275 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:29,276 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:29,276 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:29,276 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:29,276 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:29,277 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:29,280 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 26]), 25)
2023-10-09 05:38:29,280 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:29,281 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 26]), 25)
2023-10-09 05:38:29,281 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:29,281 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:29,281 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:29,281 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:29,282 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:29,282 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:29,282 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:29,282 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:29,283 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:29,286 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:29,290 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,290 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,290 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,290 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,290 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:29,294 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:29,296 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:29,299 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:29,300 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:38:29,301 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:38:29,301 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:29,303 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:29,306 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:29,310 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,310 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,310 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,310 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,311 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:29,314 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:29,316 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:29,318 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:29,319 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:38:29,320 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:38:29,320 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:29,322 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:29,325 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:29,329 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,329 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,329 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,329 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,330 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:29,333 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:29,335 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:29,337 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:29,339 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:38:29,340 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:38:29,340 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:29,342 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:29,345 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:29,349 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,349 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,349 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,349 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,350 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:29,353 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:29,355 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:29,357 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:29,359 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:38:29,360 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:38:29,360 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:29,361 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:29,365 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:29,369 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,369 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,369 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,369 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,369 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:29,372 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:29,374 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:29,376 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:29,378 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:38:29,378 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:38:29,378 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:29,380 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:29,383 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:29,387 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,387 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,388 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,388 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,388 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:29,391 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:29,393 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:29,395 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:29,397 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:38:29,397 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:38:29,397 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:29,399 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:29,402 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:29,406 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,406 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,406 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,407 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,407 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:29,410 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:29,412 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:29,414 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:29,416 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:38:29,417 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:38:29,417 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:29,419 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:29,422 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:29,426 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,426 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,426 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,426 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,426 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:29,430 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:29,432 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:29,434 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:29,436 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:38:29,436 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:38:29,436 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:29,438 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:29,441 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:29,445 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,445 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,445 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,446 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,446 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:29,449 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:29,452 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:29,454 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:29,456 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:38:29,456 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:38:29,457 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:29,458 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:29,462 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:29,466 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,466 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,466 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,466 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,466 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:29,470 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:29,472 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:29,474 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:29,476 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:38:29,476 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:38:29,477 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:29,478 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:29,482 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:29,486 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,486 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,486 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,486 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,486 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:29,490 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:29,493 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:29,495 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:29,498 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:38:29,499 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:38:29,499 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:29,500 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:29,504 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:29,504 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,505 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 25, 64]), torch.Size([8, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,505 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,505 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 26]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 25, 64]), torch.Size([2, 12, 25, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,505 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:29,509 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:29,511 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:29,514 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:29,517 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])))
2023-10-09 05:38:29,518 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])))
2023-10-09 05:38:29,518 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:29,519 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:29,520 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:29,520 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,521 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:29,521 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,521 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:29,521 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:29,521 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:29,521 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:29,522 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:29,522 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:29,522 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:29,522 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:29,522 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:29,523 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:29,523 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,523 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:29,523 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,524 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:29,524 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:29,532 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:29,541 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:29,550 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:29,559 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:29,561 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:29,561 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:29,566 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:29,567 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:29,567 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:29,567 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:29,568 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:29,568 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:29,568 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:29,568 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:29,569 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:29,569 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:29,569 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:29,570 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:29,570 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:29,571 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:29,571 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:29,575 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 27]), 26)
2023-10-09 05:38:29,575 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:29,576 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 27]), 26)
2023-10-09 05:38:29,576 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:29,576 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:29,576 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:29,576 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:29,577 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:29,577 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:29,577 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:29,577 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:29,577 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:29,581 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:29,584 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,585 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,585 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,585 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,585 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:29,589 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:29,591 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:29,592 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:29,594 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:38:29,595 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:38:29,595 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:29,597 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:29,600 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:29,604 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,604 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,604 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,604 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,604 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:29,607 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:29,610 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:29,612 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:29,614 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:38:29,614 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:38:29,614 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:29,616 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:29,619 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:29,623 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,623 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,623 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,623 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,623 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:29,626 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:29,628 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:29,632 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:29,634 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:38:29,635 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:38:29,635 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:29,636 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:29,640 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:29,643 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,644 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,644 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,644 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,644 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:29,647 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:29,649 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:29,651 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:29,653 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:38:29,654 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:38:29,654 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:29,655 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:29,659 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:29,663 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,663 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,663 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,663 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,663 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:29,668 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:29,670 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:29,672 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:29,674 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:38:29,674 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:38:29,674 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:29,676 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:29,679 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:29,683 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,683 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,683 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,683 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,683 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:29,687 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:29,689 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:29,691 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:29,693 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:38:29,693 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:38:29,693 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:29,695 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:29,698 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:29,702 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,702 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,702 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,702 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,702 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:29,705 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:29,707 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:29,709 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:29,711 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:38:29,711 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:38:29,712 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:29,713 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:29,716 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:29,720 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,720 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,721 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,721 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,721 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:29,724 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:29,726 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:29,728 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:29,730 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:38:29,730 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:38:29,730 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:29,732 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:29,735 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:29,739 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,739 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,739 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,739 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,739 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:29,743 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:29,745 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:29,746 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:29,748 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:38:29,749 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:38:29,749 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:29,751 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:29,754 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:29,758 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,758 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,758 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,758 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,758 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:29,762 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:29,764 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:29,766 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:29,767 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:38:29,768 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:38:29,768 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:29,769 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:29,773 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:29,777 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,777 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,777 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,777 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,777 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:29,780 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:29,782 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:29,784 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:29,786 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:38:29,786 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:38:29,786 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:29,788 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:29,791 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:29,792 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,792 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 26, 64]), torch.Size([8, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,792 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,793 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 27]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 26, 64]), torch.Size([2, 12, 26, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,793 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:29,796 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:29,800 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:29,801 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:29,803 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])))
2023-10-09 05:38:29,804 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])))
2023-10-09 05:38:29,804 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:29,805 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:29,806 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:29,806 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,806 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:29,807 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,807 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:29,807 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:29,809 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:29,810 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:29,811 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:29,811 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:29,811 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:29,811 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:29,812 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:29,812 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:29,813 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,813 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:29,813 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,813 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:29,813 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:29,823 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:29,833 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:29,844 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:29,855 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:29,856 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:29,857 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:29,862 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:29,863 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:29,863 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:29,864 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:29,864 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:29,864 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:29,864 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:29,864 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:29,865 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:29,865 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:29,865 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:29,866 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:29,866 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:29,866 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:29,867 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:29,872 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 28]), 27)
2023-10-09 05:38:29,872 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:29,872 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 28]), 27)
2023-10-09 05:38:29,873 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:29,873 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:29,873 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:29,874 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:29,874 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:29,875 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:29,875 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:29,875 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:29,875 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:29,880 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:29,886 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,887 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,887 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,887 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,887 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:29,892 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:29,895 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:29,897 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:29,901 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:38:29,901 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:38:29,902 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:29,904 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:29,910 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:29,916 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,916 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,917 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,917 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,917 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:29,921 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:29,925 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:29,927 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:29,930 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:38:29,931 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:38:29,931 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:29,934 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:29,940 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:29,946 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,946 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,946 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,947 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,947 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:29,951 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:29,954 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:29,956 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:29,959 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:38:29,960 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:38:29,960 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:29,963 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:29,968 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:29,974 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,974 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,974 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,974 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,975 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:29,978 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:29,980 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:29,982 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:29,984 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:38:29,984 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:38:29,984 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:29,986 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:29,989 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:29,994 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:29,994 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,994 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:29,994 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:29,994 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:29,998 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:30,000 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:30,002 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:30,004 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:38:30,005 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:38:30,005 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:30,006 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:30,010 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:30,013 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,014 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,014 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,014 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,014 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:30,017 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:30,019 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:30,021 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:30,023 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:38:30,023 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:38:30,023 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:30,024 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:30,028 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:30,032 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,032 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,032 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,032 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,033 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:30,036 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:30,038 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:30,040 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:30,042 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:38:30,043 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:38:30,043 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:30,044 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:30,048 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:30,052 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,052 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,052 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,052 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,052 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:30,056 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:30,058 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:30,059 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:30,061 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:38:30,062 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:38:30,062 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:30,063 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:30,067 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:30,071 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,071 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,071 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,071 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,071 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:30,074 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:30,076 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:30,080 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:30,082 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:38:30,082 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:38:30,082 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:30,084 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:30,088 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:30,091 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,091 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,092 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,092 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,092 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:30,095 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:30,097 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:30,099 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:30,101 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:38:30,102 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:38:30,102 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:30,103 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:30,107 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:30,111 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,111 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,111 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,111 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,111 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:30,114 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:30,117 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:30,119 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:30,121 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:38:30,121 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:38:30,121 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:30,123 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:30,126 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:30,127 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,127 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 27, 64]), torch.Size([8, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,127 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,127 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 28]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 27, 64]), torch.Size([2, 12, 27, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,127 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:30,130 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:30,132 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:30,135 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:30,136 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])))
2023-10-09 05:38:30,137 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])))
2023-10-09 05:38:30,137 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:30,139 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:30,139 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:30,140 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,140 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:30,140 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,140 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:30,140 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:30,141 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:30,141 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:30,141 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:30,141 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:30,142 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:30,142 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:30,142 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:30,143 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:30,143 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,143 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:30,144 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,144 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:30,144 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:30,154 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:30,161 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:30,168 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:30,176 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:30,177 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:30,177 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:30,181 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:30,182 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:30,182 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:30,182 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:30,182 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:30,183 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:30,183 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:30,183 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:30,183 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:30,183 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:30,183 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:30,184 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:30,184 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:30,184 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:30,185 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:30,188 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 29]), 28)
2023-10-09 05:38:30,188 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:30,189 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 29]), 28)
2023-10-09 05:38:30,189 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:30,189 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:30,189 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:30,189 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:30,190 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:30,190 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:30,190 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:30,190 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:30,190 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:30,194 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:30,197 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,197 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,198 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,198 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,198 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:30,201 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:30,203 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:30,205 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:30,207 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:38:30,208 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:38:30,208 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:30,210 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:30,213 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:30,217 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,217 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,217 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,218 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,218 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:30,221 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:30,223 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:30,225 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:30,226 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:38:30,227 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:38:30,227 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:30,229 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:30,234 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:30,238 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,238 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,238 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,238 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,238 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:30,242 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:30,244 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:30,246 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:30,248 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:38:30,249 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:38:30,249 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:30,251 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:30,254 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:30,258 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,258 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,258 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,259 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,259 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:30,263 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:30,265 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:30,267 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:30,269 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:38:30,270 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:38:30,270 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:30,272 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:30,275 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:30,279 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,279 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,279 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,279 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,279 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:30,283 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:30,285 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:30,287 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:30,288 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:38:30,290 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:38:30,290 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:30,292 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:30,296 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:30,300 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,300 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,300 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,301 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,301 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:30,306 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:30,309 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:30,310 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:30,312 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:38:30,313 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:38:30,313 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:30,314 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:30,317 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:30,321 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,321 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,322 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,322 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,322 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:30,325 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:30,327 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:30,329 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:30,330 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:38:30,331 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:38:30,331 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:30,333 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:30,336 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:30,340 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,340 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,340 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,340 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,340 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:30,344 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:30,345 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:30,347 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:30,348 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:38:30,349 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:38:30,349 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:30,350 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:30,354 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:30,358 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,358 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,358 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,358 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,358 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:30,362 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:30,363 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:30,365 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:30,367 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:38:30,368 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:38:30,368 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:30,369 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:30,373 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:30,377 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,377 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,377 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,377 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,377 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:30,381 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:30,383 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:30,385 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:30,388 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:38:30,389 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:38:30,389 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:30,390 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:30,394 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:30,398 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,398 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,398 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,398 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,398 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:30,409 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:30,412 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:30,414 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:30,417 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:38:30,417 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:38:30,417 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:30,419 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:30,422 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:30,423 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,423 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 28, 64]), torch.Size([8, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,423 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,423 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 29]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 28, 64]), torch.Size([2, 12, 28, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,424 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:30,427 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:30,430 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:30,432 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:30,433 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])))
2023-10-09 05:38:30,434 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])))
2023-10-09 05:38:30,434 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:30,435 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:30,436 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:30,436 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,436 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:30,437 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,437 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:30,437 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:30,437 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:30,437 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:30,437 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:30,438 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:30,438 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:30,438 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:30,438 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:30,439 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:30,439 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,439 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:30,439 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,439 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:30,440 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:30,450 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:30,459 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:30,468 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:30,477 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:30,478 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:30,478 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:30,483 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:30,483 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:30,484 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:30,484 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:30,484 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:30,484 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:30,484 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:30,484 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:30,485 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:30,485 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:30,485 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:30,485 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:30,485 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:30,486 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:30,486 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:30,490 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 30]), 29)
2023-10-09 05:38:30,490 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:30,490 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 30]), 29)
2023-10-09 05:38:30,491 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:30,491 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:30,491 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:30,491 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:30,492 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:30,492 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:30,492 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:30,492 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:30,493 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:30,496 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:30,500 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,500 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,500 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,500 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,500 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:30,504 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:30,506 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:30,509 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:30,512 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:38:30,513 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:38:30,513 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:30,515 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:30,518 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:30,522 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,522 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,522 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,523 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,523 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:30,527 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:30,529 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:30,531 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:30,533 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:38:30,534 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:38:30,534 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:30,536 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:30,539 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:30,543 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,543 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,543 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,544 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,544 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:30,547 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:30,550 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:30,552 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:30,557 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:38:30,558 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:38:30,558 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:30,561 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:30,566 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:30,572 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,572 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,572 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,573 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,573 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:30,583 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:30,587 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:30,592 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:30,595 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:38:30,595 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:38:30,595 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:30,597 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:30,601 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:30,605 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,605 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,605 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,605 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,605 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:30,609 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:30,612 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:30,615 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:30,618 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:38:30,619 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:38:30,619 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:30,622 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:30,626 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:30,631 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,631 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,631 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,631 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,631 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:30,635 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:30,637 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:30,640 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:30,642 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:38:30,643 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:38:30,643 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:30,644 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:30,648 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:30,652 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,652 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,652 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,652 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,652 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:30,655 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:30,657 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:30,659 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:30,661 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:38:30,662 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:38:30,662 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:30,663 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:30,667 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:30,670 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,671 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,671 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,671 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,671 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:30,677 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:30,680 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:30,682 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:30,684 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:38:30,685 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:38:30,685 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:30,686 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:30,689 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:30,694 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,694 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,694 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,694 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,694 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:30,697 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:30,699 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:30,702 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:30,704 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:38:30,704 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:38:30,704 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:30,706 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:30,710 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:30,713 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,714 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,714 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,714 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,714 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:30,718 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:30,719 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:30,722 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:30,724 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:38:30,724 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:38:30,724 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:30,725 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:30,729 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:30,733 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,733 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,733 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,733 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,734 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:30,737 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:30,739 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:30,740 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:30,742 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:38:30,743 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:38:30,743 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:30,745 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:30,748 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:30,749 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,749 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 29, 64]), torch.Size([8, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,749 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,749 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 30]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 29, 64]), torch.Size([2, 12, 29, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,749 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:30,752 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:30,754 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:30,756 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:30,758 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])))
2023-10-09 05:38:30,758 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])))
2023-10-09 05:38:30,759 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:30,760 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:30,761 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:30,761 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,761 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:30,761 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,762 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:30,762 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:30,762 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:30,762 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:30,762 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:30,763 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:30,763 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:30,763 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:30,763 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:30,764 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:30,764 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,764 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:30,764 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,765 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:30,765 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:30,774 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:30,782 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:30,790 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:30,798 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:30,799 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:30,799 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:30,803 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:30,804 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:30,804 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:30,804 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:30,805 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:30,805 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:30,805 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:30,805 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:30,805 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:30,805 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:30,806 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:30,806 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:30,806 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:30,806 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:30,807 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:30,811 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 31]), 30)
2023-10-09 05:38:30,811 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:30,811 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 31]), 30)
2023-10-09 05:38:30,811 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:30,811 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:30,812 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:30,812 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:30,812 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:30,812 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:30,812 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:30,813 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:30,813 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:30,816 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:30,820 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,820 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,821 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,821 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,821 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:30,824 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:30,826 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:30,828 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:30,830 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:38:30,831 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:38:30,831 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:30,833 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:30,836 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:30,840 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,840 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,840 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,841 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,841 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:30,844 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:30,846 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:30,848 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:30,850 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:38:30,851 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:38:30,851 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:30,853 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:30,856 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:30,860 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,860 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,860 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,860 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,861 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:30,864 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:30,866 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:30,868 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:30,870 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:38:30,871 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:38:30,871 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:30,873 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:30,876 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:30,880 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,880 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,880 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,881 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,881 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:30,884 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:30,886 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:30,888 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:30,889 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:38:30,894 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:38:30,894 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:30,896 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:30,900 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:30,904 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,904 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,905 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,905 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,905 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:30,908 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:30,911 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:30,913 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:30,915 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:38:30,915 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:38:30,915 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:30,917 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:30,921 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:30,925 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,925 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,925 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,926 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,926 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:30,930 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:30,932 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:30,934 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:30,936 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:38:30,937 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:38:30,937 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:30,939 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:30,943 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:30,947 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,947 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,947 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,947 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,948 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:30,952 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:30,954 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:30,957 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:30,959 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:38:30,960 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:38:30,960 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:30,962 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:30,966 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:30,970 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,970 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,971 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,971 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,971 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:30,975 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:30,977 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:30,979 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:30,981 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:38:30,982 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:38:30,982 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:30,984 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:30,988 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:30,992 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:30,992 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,992 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:30,992 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:30,993 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:30,996 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:30,998 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:31,000 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:31,002 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:38:31,003 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:38:31,003 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:31,005 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:31,009 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:31,013 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,013 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,013 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,013 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,013 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:31,017 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:31,019 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:31,021 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:31,023 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:38:31,024 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:38:31,024 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:31,026 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:31,030 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:31,034 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,034 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,034 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,034 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,035 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:31,038 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:31,040 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:31,043 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:31,045 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:38:31,045 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:38:31,045 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:31,047 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:31,051 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:31,052 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,052 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 30, 64]), torch.Size([8, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,052 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,052 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 31]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 30, 64]), torch.Size([2, 12, 30, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,053 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:31,056 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:31,059 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:31,061 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:31,063 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])))
2023-10-09 05:38:31,064 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])))
2023-10-09 05:38:31,064 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:31,065 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:31,066 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:31,067 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,067 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:31,067 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,067 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:31,067 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:31,067 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:31,068 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:31,068 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:31,068 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:31,068 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:31,068 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:31,069 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:31,069 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:31,070 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,070 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:31,070 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,070 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:31,070 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:31,080 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:31,087 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:31,095 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:31,104 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:31,105 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:31,105 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:31,110 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:31,111 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:31,111 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:31,112 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:31,112 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:31,112 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:31,112 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:31,112 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:31,112 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:31,113 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:31,113 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:31,113 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:31,113 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:31,114 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:31,114 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:31,118 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 32]), 31)
2023-10-09 05:38:31,118 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:31,119 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 32]), 31)
2023-10-09 05:38:31,119 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:31,119 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:31,119 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:31,119 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:31,120 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:31,120 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:31,120 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:31,120 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:31,121 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:31,124 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:31,128 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,129 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,129 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,129 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,129 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:31,133 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:31,135 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:31,139 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:31,141 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:38:31,142 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:38:31,142 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:31,144 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:31,148 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:31,152 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,152 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,152 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,153 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,153 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:31,157 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:31,159 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:31,161 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:31,163 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:38:31,163 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:38:31,164 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:31,166 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:31,169 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:31,174 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,174 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,174 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,174 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,174 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:31,178 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:31,180 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:31,183 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:31,185 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:38:31,186 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:38:31,186 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:31,188 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:31,192 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:31,196 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,196 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,196 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,196 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,196 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:31,200 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:31,203 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:31,205 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:31,207 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:38:31,208 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:38:31,208 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:31,210 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:31,213 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:31,218 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,218 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,218 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,218 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,219 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:31,222 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:31,224 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:31,226 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:31,228 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:38:31,229 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:38:31,229 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:31,231 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:31,235 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:31,239 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,239 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,239 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,239 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,239 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:31,244 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:31,247 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:31,249 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:31,251 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:38:31,251 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:38:31,251 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:31,253 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:31,257 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:31,261 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,261 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,262 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,262 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,262 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:31,265 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:31,268 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:31,270 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:31,272 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:38:31,272 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:38:31,272 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:31,274 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:31,277 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:31,281 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,281 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,281 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,282 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,282 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:31,285 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:31,287 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:31,289 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:31,291 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:38:31,292 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:38:31,292 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:31,294 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:31,297 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:31,301 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,301 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,301 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,302 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,302 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:31,305 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:31,307 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:31,309 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:31,312 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:38:31,312 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:38:31,312 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:31,314 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:31,318 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:31,321 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,322 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,322 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,322 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,322 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:31,326 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:31,328 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:31,330 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:31,332 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:38:31,332 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:38:31,332 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:31,333 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:31,337 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:31,341 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,341 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,341 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,342 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,342 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:31,345 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:31,347 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:31,348 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:31,350 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:38:31,351 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:38:31,351 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:31,353 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:31,356 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:31,357 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,357 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 31, 64]), torch.Size([8, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,357 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,357 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 32]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 31, 64]), torch.Size([2, 12, 31, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,357 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:31,363 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:31,365 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:31,366 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:31,368 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])))
2023-10-09 05:38:31,370 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])))
2023-10-09 05:38:31,370 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:31,371 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:31,372 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:31,372 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,372 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:31,373 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,373 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:31,373 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:31,373 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:31,373 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:31,374 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:31,374 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:31,374 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:31,374 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:31,374 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:31,375 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:31,375 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,375 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:31,376 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,376 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:31,376 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:31,384 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:31,392 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:31,399 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:31,407 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:31,408 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:31,409 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:31,413 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:31,414 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:31,414 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:31,414 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:31,414 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:31,415 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:31,415 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:31,415 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:31,415 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:31,415 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:31,415 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:31,416 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:31,416 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:31,416 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:31,417 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:31,421 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 33]), 32)
2023-10-09 05:38:31,421 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:31,421 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 33]), 32)
2023-10-09 05:38:31,421 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:31,421 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:31,421 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:31,422 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:31,422 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:31,422 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:31,422 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:31,423 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:31,423 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:31,426 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:31,430 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,430 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,431 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,431 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,431 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:31,435 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:31,437 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:31,439 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:31,442 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:38:31,443 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:38:31,443 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:31,445 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:31,448 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:31,452 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,452 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,452 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,452 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,453 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:31,457 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:31,459 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:31,461 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:31,464 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:38:31,465 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:38:31,465 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:31,466 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:31,470 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:31,474 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,474 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,474 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,475 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,475 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:31,478 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:31,481 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:31,483 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:31,486 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:38:31,487 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:38:31,487 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:31,488 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:31,492 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:31,496 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,496 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,496 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,496 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,496 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:31,500 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:31,503 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:31,506 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:31,508 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:38:31,509 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:38:31,509 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:31,511 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:31,515 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:31,519 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,519 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,519 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,520 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,520 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:31,524 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:31,526 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:31,528 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:31,531 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:38:31,531 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:38:31,531 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:31,533 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:31,537 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:31,540 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,541 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,541 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,541 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,541 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:31,545 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:31,548 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:31,551 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:31,553 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:38:31,554 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:38:31,554 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:31,555 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:31,559 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:31,563 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,563 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,564 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,564 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,564 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:31,568 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:31,570 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:31,572 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:31,575 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:38:31,575 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:38:31,576 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:31,577 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:31,581 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:31,584 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,585 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,585 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,585 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,585 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:31,589 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:31,591 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:31,593 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:31,595 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:38:31,596 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:38:31,596 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:31,597 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:31,601 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:31,605 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,605 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,605 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,606 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,606 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:31,609 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:31,611 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:31,613 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:31,615 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:38:31,616 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:38:31,616 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:31,618 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:31,622 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:31,625 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,626 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,626 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,626 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,626 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:31,630 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:31,632 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:31,634 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:31,636 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:38:31,637 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:38:31,637 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:31,638 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:31,642 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:31,646 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,646 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,646 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,646 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,647 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:31,651 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:31,654 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:31,656 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:31,658 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:38:31,659 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:38:31,659 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:31,660 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:31,664 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:31,665 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,665 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 32, 64]), torch.Size([8, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,665 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,665 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 33]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 32, 64]), torch.Size([2, 12, 32, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,665 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:31,668 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:31,671 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:31,673 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:31,676 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])))
2023-10-09 05:38:31,677 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])))
2023-10-09 05:38:31,677 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:31,679 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:31,679 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:31,680 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,680 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:31,680 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,680 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:31,680 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:31,680 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:31,681 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:31,681 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:31,681 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:31,681 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:31,681 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:31,682 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:31,682 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:31,683 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,683 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:31,683 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,683 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:31,683 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:31,692 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:31,701 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:31,708 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:31,716 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:31,717 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:31,717 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:31,721 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:31,722 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:31,723 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:31,723 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:31,723 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:31,723 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:31,723 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:31,724 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:31,724 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:31,724 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:31,724 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:31,725 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:31,725 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:31,725 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:31,726 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:31,729 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 34]), 33)
2023-10-09 05:38:31,730 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:31,730 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 34]), 33)
2023-10-09 05:38:31,730 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:31,730 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:31,730 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:31,731 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:31,731 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:31,731 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:31,731 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:31,731 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:31,732 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:31,735 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:31,739 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,739 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,739 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,739 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,739 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:31,743 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:31,745 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:31,747 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:31,749 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:38:31,749 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:38:31,749 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:31,751 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:31,755 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:31,759 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,759 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,759 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,759 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,759 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:31,762 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:31,764 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:31,767 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:31,768 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:38:31,769 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:38:31,769 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:31,771 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:31,774 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:31,778 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,778 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,779 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,779 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,779 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:31,783 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:31,785 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:31,786 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:31,789 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:38:31,789 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:38:31,789 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:31,791 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:31,795 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:31,798 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,798 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,799 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,799 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,799 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:31,802 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:31,804 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:31,806 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:31,808 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:38:31,809 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:38:31,809 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:31,810 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:31,814 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:31,818 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,818 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,819 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,819 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,819 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:31,823 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:31,825 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:31,827 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:31,829 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:38:31,830 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:38:31,830 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:31,831 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:31,835 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:31,839 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,839 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,839 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,839 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,839 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:31,843 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:31,845 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:31,846 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:31,848 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:38:31,849 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:38:31,849 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:31,850 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:31,854 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:31,858 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,858 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,858 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,859 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,859 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:31,862 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:31,865 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:31,867 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:31,869 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:38:31,870 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:38:31,870 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:31,872 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:31,875 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:31,879 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,879 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,880 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,880 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,880 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:31,885 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:31,887 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:31,889 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:31,891 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:38:31,891 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:38:31,891 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:31,893 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:31,896 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:31,900 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,900 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,900 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,901 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,901 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:31,904 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:31,906 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:31,908 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:31,910 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:38:31,910 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:38:31,910 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:31,912 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:31,916 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:31,923 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,923 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,923 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,924 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,924 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:31,927 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:31,930 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:31,932 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:31,934 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:38:31,934 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:38:31,934 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:31,936 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:31,939 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:31,943 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,943 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,944 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,944 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,944 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:31,947 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:31,949 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:31,951 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:31,953 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:38:31,953 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:38:31,953 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:31,955 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:31,959 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:31,959 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,959 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 33, 64]), torch.Size([8, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,960 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,960 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 34]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 33, 64]), torch.Size([2, 12, 33, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:31,960 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:31,964 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:31,966 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:31,968 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:31,969 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])))
2023-10-09 05:38:31,970 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])))
2023-10-09 05:38:31,970 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:31,972 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:31,972 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:31,973 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,973 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:31,973 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,973 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:31,973 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:31,973 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:31,974 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:31,974 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:31,974 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:31,974 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:31,974 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:31,975 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:31,975 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:31,976 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:31,976 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:31,976 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:31,976 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:31,976 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:31,988 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:31,996 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:32,004 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:32,012 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:32,013 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:32,013 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:32,017 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:32,017 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:32,018 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:32,018 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:32,018 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:32,019 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:32,019 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:32,019 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:32,019 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:32,020 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:32,020 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:32,020 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:32,020 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:32,020 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:32,021 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:32,025 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 35]), 34)
2023-10-09 05:38:32,025 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:32,025 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 35]), 34)
2023-10-09 05:38:32,025 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:32,025 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:32,026 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:32,026 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:32,026 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:32,027 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:32,027 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:32,027 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:32,027 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:32,031 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:32,034 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,035 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,035 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,035 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,035 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:32,038 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:32,040 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:32,043 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:32,044 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:38:32,045 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:38:32,045 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:32,047 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:32,050 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:32,054 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,054 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,054 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,055 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,055 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:32,058 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:32,060 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:32,062 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:32,064 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:38:32,064 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:38:32,065 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:32,066 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:32,070 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:32,074 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,074 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,074 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,074 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,074 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:32,078 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:32,080 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:32,082 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:32,083 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:38:32,084 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:38:32,084 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:32,086 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:32,090 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:32,094 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,094 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,094 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,094 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,094 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:32,098 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:32,100 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:32,102 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:32,104 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:38:32,105 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:38:32,105 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:32,107 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:32,110 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:32,114 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,114 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,115 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,115 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,115 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:32,118 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:32,120 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:32,122 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:32,124 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:38:32,125 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:38:32,125 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:32,126 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:32,130 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:32,134 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,134 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,134 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,134 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,134 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:32,139 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:32,141 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:32,143 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:32,146 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:38:32,147 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:38:32,147 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:32,148 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:32,152 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:32,155 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,156 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,156 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,156 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,156 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:32,159 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:32,161 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:32,163 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:32,165 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:38:32,166 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:38:32,166 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:32,168 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:32,171 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:32,175 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,175 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,175 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,176 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,176 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:32,179 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:32,181 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:32,183 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:32,185 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:38:32,188 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:38:32,188 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:32,189 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:32,193 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:32,197 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,197 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,197 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,197 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,198 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:32,201 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:32,203 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:32,205 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:32,207 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:38:32,207 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:38:32,207 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:32,209 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:32,213 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:32,217 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,217 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,217 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,217 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,217 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:32,220 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:32,222 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:32,225 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:32,226 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:38:32,227 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:38:32,227 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:32,228 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:32,232 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:32,236 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,236 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,237 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,237 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,237 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:32,240 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:32,242 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:32,244 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:32,246 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:38:32,246 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:38:32,247 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:32,248 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:32,252 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:32,252 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,253 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 34, 64]), torch.Size([8, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,253 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,253 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 35]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 34, 64]), torch.Size([2, 12, 34, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,253 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:32,256 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:32,258 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:32,260 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:32,262 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])))
2023-10-09 05:38:32,263 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])))
2023-10-09 05:38:32,263 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:32,265 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:32,265 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:32,266 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,266 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:32,266 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,266 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:32,266 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:32,267 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:32,267 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:32,267 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:32,267 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:32,267 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:32,267 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:32,268 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:32,268 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:32,269 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,269 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:32,269 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,269 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:32,269 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:32,280 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:32,288 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:32,295 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:32,302 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:32,303 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:32,303 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:32,308 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:32,308 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:32,309 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:32,309 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:32,309 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:32,309 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:32,309 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:32,310 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:32,310 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:32,310 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:32,310 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:32,310 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:32,310 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:32,311 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:32,311 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:32,315 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 36]), 35)
2023-10-09 05:38:32,315 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:32,316 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 36]), 35)
2023-10-09 05:38:32,316 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:32,316 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:32,316 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:32,316 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:32,317 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:32,317 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:32,317 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:32,317 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:32,318 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:32,321 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:32,325 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,326 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,326 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,326 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,326 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:32,329 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:32,331 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:32,333 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:32,335 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:38:32,335 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:38:32,336 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:32,337 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:32,341 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:32,345 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,345 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,345 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,345 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,345 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:32,349 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:32,351 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:32,352 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:32,354 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:38:32,355 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:38:32,355 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:32,357 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:32,360 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:32,364 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,364 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,364 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,365 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,365 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:32,368 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:32,370 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:32,372 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:32,374 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:38:32,374 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:38:32,374 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:32,376 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:32,380 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:32,383 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,384 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,384 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,384 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,384 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:32,387 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:32,389 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:32,391 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:32,393 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:38:32,393 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:38:32,394 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:32,395 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:32,399 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:32,403 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,403 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,403 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,403 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,403 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:32,407 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:32,408 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:32,410 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:32,412 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:38:32,413 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:38:32,413 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:32,414 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:32,418 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:32,422 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,422 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,422 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,422 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,422 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:32,426 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:32,427 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:32,429 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:32,431 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:38:32,432 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:38:32,432 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:32,433 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:32,437 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:32,441 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,441 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,441 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,441 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,441 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:32,446 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:32,448 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:32,450 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:32,452 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:38:32,452 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:38:32,452 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:32,454 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:32,458 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:32,462 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,462 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,462 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,462 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,462 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:32,465 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:32,467 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:32,469 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:32,471 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:38:32,471 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:38:32,471 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:32,472 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:32,476 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:32,480 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,480 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,480 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,480 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,481 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:32,484 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:32,485 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:32,487 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:32,489 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:38:32,490 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:38:32,490 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:32,492 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:32,495 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:32,499 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,499 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,499 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,499 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,499 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:32,503 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:32,505 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:32,506 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:32,508 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:38:32,509 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:38:32,509 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:32,511 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:32,514 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:32,518 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,518 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,518 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,519 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,519 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:32,522 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:32,524 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:32,526 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:32,528 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:38:32,528 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:38:32,528 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:32,530 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:32,533 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:32,534 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,534 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 35, 64]), torch.Size([8, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,534 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,534 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 36]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 35, 64]), torch.Size([2, 12, 35, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,534 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:32,537 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:32,539 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:32,541 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:32,543 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])))
2023-10-09 05:38:32,544 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])))
2023-10-09 05:38:32,544 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:32,545 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:32,546 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:32,546 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,546 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:32,546 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,547 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:32,547 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:32,547 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:32,547 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:32,547 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:32,548 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:32,548 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:32,548 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:32,548 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:32,549 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:32,549 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,549 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:32,549 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,549 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:32,549 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:32,559 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:32,566 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:32,576 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:32,585 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:32,586 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:32,587 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:32,591 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:32,592 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:32,592 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:32,592 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:32,593 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:32,593 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:32,593 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:32,593 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:32,593 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:32,594 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:32,594 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:32,594 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:32,594 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:32,595 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:32,595 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:32,599 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 37]), 36)
2023-10-09 05:38:32,599 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:32,599 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 37]), 36)
2023-10-09 05:38:32,599 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:32,599 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:32,600 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:32,600 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:32,600 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:32,601 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:32,601 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:32,601 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:32,601 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:32,605 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:32,609 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,609 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,609 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,609 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,609 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:32,613 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:32,615 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:32,616 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:32,618 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:38:32,619 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:38:32,619 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:32,621 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:32,624 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:32,628 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,628 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,629 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,629 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,629 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:32,634 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:32,636 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:32,638 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:32,640 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:38:32,640 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:38:32,641 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:32,642 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:32,646 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:32,650 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,650 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,650 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,650 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,651 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:32,654 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:32,656 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:32,658 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:32,660 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:38:32,661 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:38:32,661 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:32,663 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:32,667 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:32,670 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,671 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,671 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,671 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,671 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:32,675 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:32,676 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:32,678 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:32,680 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:38:32,681 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:38:32,681 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:32,683 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:32,686 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:32,691 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,691 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,691 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,691 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,691 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:32,695 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:32,697 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:32,699 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:32,701 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:38:32,702 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:38:32,702 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:32,704 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:32,708 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:32,712 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,712 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,712 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,712 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,712 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:32,716 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:32,719 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:32,720 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:32,722 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:38:32,723 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:38:32,723 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:32,724 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:32,728 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:32,732 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,732 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,732 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,732 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,733 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:32,736 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:32,737 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:32,740 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:32,741 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:38:32,742 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:38:32,742 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:32,744 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:32,747 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:32,751 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,751 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,751 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,752 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,752 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:32,756 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:32,759 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:32,761 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:32,764 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:38:32,764 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:38:32,765 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:32,766 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:32,770 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:32,774 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,774 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,774 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,775 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,775 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:32,778 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:32,780 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:32,782 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:32,784 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:38:32,785 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:38:32,785 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:32,786 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:32,790 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:32,794 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,794 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,794 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,795 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,795 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:32,798 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:32,800 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:32,802 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:32,804 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:38:32,805 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:38:32,805 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:32,806 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:32,810 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:32,814 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,814 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,814 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,814 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,815 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:32,818 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:32,819 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:32,822 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:32,824 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:38:32,824 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:38:32,824 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:32,826 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:32,830 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:32,830 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,830 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 36, 64]), torch.Size([8, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,830 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,831 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 37]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 36, 64]), torch.Size([2, 12, 36, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,831 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:32,834 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:32,837 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:32,839 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:32,841 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])))
2023-10-09 05:38:32,842 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])))
2023-10-09 05:38:32,842 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:32,843 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:32,844 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:32,844 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,844 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:32,844 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,845 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:32,845 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:32,845 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:32,845 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:32,845 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:32,846 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:32,846 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:32,846 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:32,846 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:32,847 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:32,847 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,847 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:32,848 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,848 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:32,848 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:32,856 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:32,864 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:32,871 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:32,879 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:32,880 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:32,880 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:32,885 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:32,885 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:32,886 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1]),)
2023-10-09 05:38:32,886 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:32,886 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1]),)
2023-10-09 05:38:32,886 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:32,886 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-09 05:38:32,887 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-09 05:38:32,887 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-09 05:38:32,887 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-09 05:38:32,887 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:32,887 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:32,887 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-09 05:38:32,888 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-09 05:38:32,888 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:32,893 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 38]), 37)
2023-10-09 05:38:32,893 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:32,894 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 38]), 37)
2023-10-09 05:38:32,894 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:32,894 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-09 05:38:32,894 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-09 05:38:32,894 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-09 05:38:32,895 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-09 05:38:32,895 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:32,895 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:32,895 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-09 05:38:32,896 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-09 05:38:32,899 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:32,903 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,903 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,904 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,904 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,904 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-09 05:38:32,907 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-09 05:38:32,909 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-09 05:38:32,911 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-09 05:38:32,913 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:38:32,913 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:38:32,913 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-09 05:38:32,915 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-09 05:38:32,919 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:32,923 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,923 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,923 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,923 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,923 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-09 05:38:32,927 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-09 05:38:32,929 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-09 05:38:32,931 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-09 05:38:32,932 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:38:32,933 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:38:32,933 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-09 05:38:32,935 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-09 05:38:32,939 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:32,942 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,942 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,943 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,943 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,943 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-09 05:38:32,946 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-09 05:38:32,948 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-09 05:38:32,951 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-09 05:38:32,954 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:38:32,954 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:38:32,954 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-09 05:38:32,956 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-09 05:38:32,960 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:32,965 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,965 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,965 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,965 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,965 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-09 05:38:32,968 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-09 05:38:32,970 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-09 05:38:32,972 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-09 05:38:32,974 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:38:32,974 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:38:32,975 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-09 05:38:32,976 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-09 05:38:32,980 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:32,984 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:32,984 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,984 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:32,984 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:32,984 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-09 05:38:32,988 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-09 05:38:32,990 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-09 05:38:32,992 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-09 05:38:32,994 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:38:32,995 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:38:32,995 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-09 05:38:32,996 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-09 05:38:33,000 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:33,004 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:33,004 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:33,004 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:33,005 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:33,005 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-09 05:38:33,008 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-09 05:38:33,010 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-09 05:38:33,013 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-09 05:38:33,015 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:38:33,016 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:38:33,016 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-09 05:38:33,017 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-09 05:38:33,021 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:33,025 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:33,025 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:33,025 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:33,025 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:33,026 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-09 05:38:33,029 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-09 05:38:33,031 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-09 05:38:33,033 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-09 05:38:33,034 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:38:33,035 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:38:33,035 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-09 05:38:33,037 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-09 05:38:33,040 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:33,044 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:33,044 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:33,045 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:33,045 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:33,045 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-09 05:38:33,049 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-09 05:38:33,051 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-09 05:38:33,053 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-09 05:38:33,055 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:38:33,056 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:38:33,056 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-09 05:38:33,057 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-09 05:38:33,061 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:33,065 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:33,065 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:33,065 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:33,065 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:33,066 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-09 05:38:33,070 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-09 05:38:33,072 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-09 05:38:33,074 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-09 05:38:33,076 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:38:33,076 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:38:33,077 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-09 05:38:33,078 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-09 05:38:33,082 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:33,086 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:33,086 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:33,086 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:33,086 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:33,086 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-09 05:38:33,089 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-09 05:38:33,091 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-09 05:38:33,094 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-09 05:38:33,095 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:38:33,096 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:38:33,096 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-09 05:38:33,097 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-09 05:38:33,101 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:33,105 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:33,105 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:33,106 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:33,106 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:33,106 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-09 05:38:33,109 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-09 05:38:33,111 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-09 05:38:33,113 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-09 05:38:33,115 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:38:33,115 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:38:33,115 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-09 05:38:33,117 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-09 05:38:33,121 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:33,121 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:33,121 [forward.py:81 in new_forward] DEBUG - kwargs: {'attention_mask': torch.Size([8, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([8, 12, 37, 64]), torch.Size([8, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:33,122 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:33,122 [forward.py:88 in new_forward] DEBUG - kwargs_0: {'attention_mask': torch.Size([2, 1, 1, 38]), 'layer_head_mask': None, 'past_key_value': (torch.Size([2, 12, 37, 64]), torch.Size([2, 12, 37, 64])), 'output_attentions': False, 'use_cache': True}
2023-10-09 05:38:33,122 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-09 05:38:33,125 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-09 05:38:33,127 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-09 05:38:33,129 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-09 05:38:33,131 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x (torch.Size([2, 1, 768]), (torch.Size([2, 12, 38, 64]), torch.Size([2, 12, 38, 64])))
2023-10-09 05:38:33,131 [forward.py:108 in new_forward] DEBUG - outputs after concat: (torch.Size([8, 1, 768]), (torch.Size([8, 12, 38, 64]), torch.Size([8, 12, 38, 64])))
2023-10-09 05:38:33,131 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-09 05:38:33,133 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-09 05:38:33,133 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:33,134 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:33,134 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:33,134 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:33,134 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:33,134 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-09 05:38:33,134 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-09 05:38:33,135 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-09 05:38:33,135 [forward.py:94 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-09 05:38:33,135 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 768])
2023-10-09 05:38:33,135 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 768])
2023-10-09 05:38:33,136 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-09 05:38:33,136 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-09 05:38:33,136 [model.py:256 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-09 05:38:33,137 [forward.py:80 in new_forward] DEBUG - args: (torch.Size([8, 1, 768]),)
2023-10-09 05:38:33,137 [forward.py:81 in new_forward] DEBUG - kwargs: {}
2023-10-09 05:38:33,137 [forward.py:87 in new_forward] DEBUG - args_0: (torch.Size([2, 1, 768]),)
2023-10-09 05:38:33,137 [forward.py:88 in new_forward] DEBUG - kwargs_0: {}
2023-10-09 05:38:33,137 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-09 05:38:33,146 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-09 05:38:33,155 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-09 05:38:33,163 [forward.py:94 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-09 05:38:33,171 [forward.py:106 in new_forward] DEBUG - outputs before concat: 4 x torch.Size([2, 1, 50272])
2023-10-09 05:38:33,172 [forward.py:108 in new_forward] DEBUG - outputs after concat: torch.Size([8, 1, 50272])
2023-10-09 05:38:33,172 [model.py:266 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-09 05:38:33,177 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-09 05:38:33,177 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:38:33,177 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-09 05:38:33,177 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:38:33,177 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-09 05:38:33,178 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:38:33,178 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-09 05:38:33,178 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:38:33,178 [test.py:40 in test_hf_gen] INFO - for i in range(10):                               
2023-10-09 05:38:33,178 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:38:33,178 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious?
I'm a woman. I'm not conscious.
I'm not conscious. I'm not conscious.
I'm not conscious. I'm
2023-10-09 05:38:33,178 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:38:33,178 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?
I'm in Germany.
2023-10-09 05:38:33,178 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:38:33,178 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?
Huawei Mate 60 Pro is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone that is a premium smartphone
2023-10-09 05:38:33,178 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-09 05:38:33,186 [forward.py:19 in reset_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-09 05:38:33,187 [forward.py:19 in reset_forward] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-09 05:38:33,187 [forward.py:19 in reset_forward] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-09 05:38:33,187 [forward.py:19 in reset_forward] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-09 05:38:33,187 [forward.py:19 in reset_forward] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-09 05:38:33,187 [forward.py:19 in reset_forward] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-09 05:38:33,187 [forward.py:19 in reset_forward] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-09 05:38:33,187 [forward.py:19 in reset_forward] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-09 05:38:33,188 [forward.py:19 in reset_forward] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-09 05:38:33,188 [forward.py:19 in reset_forward] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-09 05:38:33,188 [forward.py:19 in reset_forward] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-09 05:38:33,188 [forward.py:19 in reset_forward] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-09 05:38:33,188 [forward.py:19 in reset_forward] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-09 05:38:33,188 [forward.py:19 in reset_forward] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-09 05:38:33,188 [forward.py:19 in reset_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-09 05:38:33,188 [forward.py:19 in reset_forward] DEBUG - lm_head from flexgen to old.
