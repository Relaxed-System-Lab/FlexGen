2023-10-11 12:43:48,710 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmp0a4mmsrh
2023-10-11 12:43:48,711 [instantiator.py:76 in _write] INFO - Writing /tmp/tmp0a4mmsrh/_remote_module_non_scriptable.py
2023-10-11 12:43:49,147 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-11 12:43:49,213 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:43:50,758 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-11 12:43:51,048 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-11 12:43:51,048 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-11 12:43:51,048 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-11 12:43:51,049 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-11 12:43:51,839 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:43:51,918 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:43:51,960 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:43:52,037 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:43:52,037 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'
2023-10-11 12:43:52,044 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-11 12:43:52,044 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-11 12:43:52,045 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-11 12:43:52,046 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-11 12:43:52,046 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-11 12:43:52,047 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-11 12:43:52,048 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-11 12:43:52,049 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-11 12:43:52,050 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-11 12:43:52,051 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-11 12:43:52,052 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-11 12:43:52,053 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-11 12:43:52,054 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-11 12:43:52,055 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-11 12:43:52,056 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:43:52,056 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:43:52,056 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-11 12:43:52,058 [model.py:148 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-11 12:43:52,059 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-11 12:43:52,084 [forward.py:46 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-11 12:43:52,084 [forward.py:46 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-11 12:43:52,084 [forward.py:46 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-11 12:43:52,084 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-11 12:43:52,084 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-11 12:43:52,084 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-11 12:43:52,084 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-11 12:43:52,085 [forward.py:46 in to_test_forward] DEBUG - lm_head to test forward
2023-10-11 12:43:52,127 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:43:52,301 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:52,302 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:52,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:52,304 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:52,305 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:52,318 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:52,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:52,328 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:52,330 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:52,338 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:52,340 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:52,347 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:52,349 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:52,355 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:52,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:52,364 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:52,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:52,372 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:52,374 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:52,380 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:52,381 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:52,388 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:52,389 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:52,395 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:52,397 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:52,403 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:52,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:52,411 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:52,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:52,413 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:52,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:52,422 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:52,427 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-11 12:43:52,427 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:43:52,435 [forward.py:26 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-11 12:43:52,435 [forward.py:26 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-11 12:43:52,435 [forward.py:26 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-11 12:43:52,435 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-11 12:43:52,435 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-11 12:43:52,436 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-11 12:43:52,437 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-11 12:43:52,437 [forward.py:26 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-11 12:43:52,437 [forward.py:26 in reset_forward] DEBUG - lm_head from test to old.
2023-10-11 12:43:52,437 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-11 12:43:52,438 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-11 12:43:52,438 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-11 12:43:52,438 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-11 12:43:52,438 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-11 12:43:52,438 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-11 12:43:52,439 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-11 12:43:52,439 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-11 12:43:52,439 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-11 12:43:52,439 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-11 12:43:52,439 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-11 12:43:52,439 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-11 12:43:52,440 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-11 12:43:52,440 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-11 12:43:52,440 [forward.py:108 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-11 12:43:52,440 [forward.py:108 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-11 12:43:52,479 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:43:52,616 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:52,617 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:52,617 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])",)
2023-10-11 12:43:52,618 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:52,618 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:52,619 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:52,619 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:52,620 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:52,621 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:43:52,621 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:52,622 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:52,622 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:52,626 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])", "<class 'int'>: 0")
2023-10-11 12:43:52,626 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:52,626 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:52,627 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:52,628 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:52,629 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:52,629 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:43:52,630 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:52,634 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:52,637 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:52,641 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,642 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,642 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:52,654 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:52,658 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:52,663 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:52,667 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,667 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:52,669 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:52,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:52,676 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,676 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,676 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:52,681 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:52,685 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:52,689 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:52,695 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,695 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:52,697 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:52,701 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:52,705 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,705 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,705 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:52,727 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:52,732 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:52,737 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:52,742 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,742 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:52,744 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:52,747 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:52,751 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,751 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,751 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:52,757 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:52,762 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:52,767 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:52,772 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,772 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:52,773 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:52,777 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:52,780 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,781 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,781 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:52,786 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:52,790 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:52,795 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:52,799 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,799 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:52,801 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:52,804 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:52,808 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,808 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,809 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:52,815 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:52,819 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:52,823 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:52,827 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,828 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:52,829 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:52,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:52,836 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,836 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,836 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:52,841 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:52,846 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:52,850 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:52,854 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,854 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:52,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:52,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:52,863 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,863 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,863 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:52,868 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:52,874 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:52,878 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:52,883 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,883 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:52,884 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:52,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:52,892 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,892 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,892 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:52,899 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:52,904 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:52,911 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:52,915 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,915 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:52,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:52,920 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:52,924 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,924 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,925 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:52,929 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:52,934 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:52,938 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:52,942 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,942 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:52,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:52,947 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:52,951 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,951 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,951 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:52,957 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:52,961 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:52,966 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:52,970 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,970 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:52,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:52,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:52,977 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,977 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:52,977 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:52,982 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:52,987 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:52,991 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:52,996 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:43:52,996 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:52,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:52,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:52,998 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:52,998 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:52,999 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:53,000 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:53,001 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:53,002 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:53,003 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:43:53,003 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:53,003 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:53,004 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:53,004 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:43:53,004 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,004 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:53,020 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:53,034 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:53,048 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:53,061 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 25136])
2023-10-11 12:43:53,061 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:53,073 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:53,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:53,074 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:53,074 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,074 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:53,075 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:53,076 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:53,076 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:53,077 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,077 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:53,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:53,078 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:53,082 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 10])", "<class 'int'>: 9")
2023-10-11 12:43:53,082 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,082 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:53,082 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:53,083 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:53,084 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:53,085 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,085 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:53,088 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:53,092 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:53,096 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,096 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,096 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:53,102 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:53,107 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:53,112 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:53,116 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,116 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:53,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:53,121 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:53,124 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,124 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,124 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:53,129 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:53,134 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:53,137 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:53,142 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,142 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:53,143 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:53,146 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:53,150 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,150 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,150 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:53,155 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:53,160 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:53,164 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:53,168 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,168 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:53,169 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:53,173 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:53,176 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,176 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,176 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:53,181 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:53,185 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:53,189 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:53,193 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,193 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:53,195 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:53,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:53,202 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,202 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,202 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:53,207 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:53,211 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:53,215 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:53,239 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,239 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:53,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:53,244 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:53,248 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,248 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,248 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:53,280 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:53,284 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:53,288 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:53,292 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,292 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:53,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:53,297 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:53,301 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,301 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,301 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:53,307 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:53,311 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:53,323 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:53,327 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,327 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:53,328 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:53,332 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:53,335 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,336 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,336 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:53,341 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:53,345 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:53,348 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:53,352 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,352 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:53,354 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:53,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:53,361 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,361 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,361 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:53,366 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:53,371 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:53,374 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:53,381 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,381 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:53,382 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:53,386 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:53,390 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,390 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,390 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:53,395 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:53,399 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:53,402 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:53,406 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,406 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:53,408 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:53,411 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:53,415 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,415 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,415 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:53,420 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:53,424 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:53,429 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:53,433 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,433 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:53,434 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:53,438 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:53,438 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,438 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,439 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:53,444 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:53,448 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:53,452 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:53,455 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:43:53,455 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:53,457 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:53,457 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:53,458 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,458 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,458 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:53,459 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:53,460 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:53,460 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:53,461 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,461 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:53,462 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:53,462 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:53,463 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,463 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,463 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:53,476 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:53,486 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:53,495 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:53,503 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:53,503 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:53,510 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:53,511 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:53,511 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:53,512 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,512 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:53,512 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:53,513 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:53,513 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:53,514 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,514 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:53,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:53,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:53,519 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 11])", "<class 'int'>: 10")
2023-10-11 12:43:53,519 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,519 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:53,520 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:53,521 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:53,521 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:53,522 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,522 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:53,526 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:53,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:53,533 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,533 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,533 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:53,539 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:53,542 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:53,546 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:53,550 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,550 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:53,552 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:53,555 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:53,558 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,559 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,559 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:53,566 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:53,570 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:53,580 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:53,584 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,584 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:53,586 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:53,589 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:53,593 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,593 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,593 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:53,598 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:53,602 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:53,606 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:53,610 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,610 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:53,611 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:53,615 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:53,618 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,618 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,619 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:53,623 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:53,627 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:53,631 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:53,635 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,635 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:53,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:53,640 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:53,644 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,644 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,644 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:53,649 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:53,653 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:53,657 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:53,661 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,661 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:53,663 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:53,666 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:53,670 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,670 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,670 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:53,675 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:53,679 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:53,683 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:53,686 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,686 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:53,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:53,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:53,694 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,694 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,694 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:53,699 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:53,704 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:53,707 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:53,711 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,711 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:53,713 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:53,716 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:53,720 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,720 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,720 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:53,726 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:53,738 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:53,746 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:53,751 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,751 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:53,752 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:53,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:53,759 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,759 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,759 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:53,764 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:53,768 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:53,771 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:53,775 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,775 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:53,777 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:53,780 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:53,783 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,784 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,784 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:53,789 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:53,793 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:53,797 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:53,801 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,801 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:53,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:53,805 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:53,809 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,809 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,809 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:53,814 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:53,825 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:53,867 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:53,872 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,873 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:53,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:53,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:53,879 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,879 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,879 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:53,884 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:53,888 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:53,892 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:53,896 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:43:53,896 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:53,897 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:53,898 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:53,898 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,898 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,898 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:53,899 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:53,900 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:53,901 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:53,902 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,902 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:53,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:53,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:53,903 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,903 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,904 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:53,916 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:53,926 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:53,935 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:53,944 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:53,944 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:53,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:53,952 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:53,952 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:53,952 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,952 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:53,953 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:53,954 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:53,954 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:53,955 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,955 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:53,956 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:53,956 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:53,960 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 12])", "<class 'int'>: 11")
2023-10-11 12:43:53,960 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:53,961 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:53,961 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:53,962 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:53,963 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:53,963 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:53,963 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:53,967 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:53,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:53,974 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:53,974 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:53,974 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:53,981 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:53,986 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:53,990 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:53,995 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:53,995 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:53,996 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:54,000 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:54,003 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,003 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,004 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:54,010 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:54,014 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:54,018 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:54,022 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,022 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:54,024 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:54,027 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:54,031 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,031 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,031 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:54,037 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:54,043 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:54,048 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:54,052 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,052 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:54,053 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:54,056 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:54,060 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,060 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,060 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:54,065 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:54,069 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:54,074 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:54,090 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,090 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:54,091 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:54,094 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:54,098 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,098 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,098 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:54,103 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:54,107 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:54,111 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:54,117 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,117 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:54,118 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:54,121 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:54,125 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,125 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,125 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:54,130 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:54,159 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:54,166 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:54,173 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,173 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:54,175 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:54,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:54,183 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,183 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,183 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:54,190 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:54,194 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:54,200 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:54,205 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,205 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:54,207 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:54,210 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:54,214 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,214 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,214 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:54,220 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:54,225 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:54,230 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:54,235 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:54,237 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:54,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:54,244 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,244 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,244 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:54,250 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:54,255 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:54,260 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:54,265 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,265 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:54,266 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:54,270 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:54,273 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,274 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,274 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:54,280 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:54,284 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:54,288 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:54,291 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,291 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:54,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:54,296 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:54,299 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,299 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,300 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:54,304 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:54,308 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:54,314 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:54,318 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,318 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:54,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:54,323 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:54,324 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,324 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,324 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:54,331 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:54,336 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:54,341 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:54,345 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:43:54,346 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:54,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:54,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:54,348 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,348 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,348 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:54,349 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:54,350 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:54,351 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:54,351 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:54,352 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:54,352 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:54,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:54,353 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,353 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,353 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:54,363 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:54,370 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:54,377 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:54,385 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:54,385 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:54,391 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:54,392 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:54,392 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:54,392 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,393 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:54,393 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:54,394 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:54,394 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:54,395 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:54,395 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:54,396 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:54,396 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:54,400 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 13])", "<class 'int'>: 12")
2023-10-11 12:43:54,400 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,400 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:54,400 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:54,401 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:54,402 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:54,402 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:54,403 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:54,406 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:54,409 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:54,412 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,413 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,413 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:54,418 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:54,422 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:54,426 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:54,429 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,430 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:54,431 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:54,434 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:54,438 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,438 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,438 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:54,443 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:54,447 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:54,452 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:54,455 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,456 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:54,457 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:54,460 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:54,464 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,464 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,464 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:54,469 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:54,474 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:54,478 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:54,481 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,482 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:54,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:54,486 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:54,490 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,490 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,490 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:54,495 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:54,499 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:54,503 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:54,507 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,507 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:54,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:54,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:54,515 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,515 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,516 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:54,521 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:54,525 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:54,528 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:54,532 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,532 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:54,534 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:54,537 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:54,541 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,541 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,541 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:54,546 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:54,550 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:54,554 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:54,557 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,558 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:54,559 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:54,562 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:54,565 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,566 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,566 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:54,571 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:54,575 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:54,579 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:54,583 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,583 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:54,584 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:54,587 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:54,591 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,591 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,591 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:54,599 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:54,603 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:54,607 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:54,618 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,618 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:54,619 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:54,622 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:54,626 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,626 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,626 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:54,631 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:54,635 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:54,639 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:54,643 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,643 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:54,644 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:54,648 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:54,652 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,652 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,652 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:54,657 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:54,661 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:54,664 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:54,668 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,668 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:54,669 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:54,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:54,676 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,676 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,676 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:54,681 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:54,685 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:54,689 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:54,693 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,694 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:54,695 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:54,698 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:54,699 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,699 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,699 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:54,704 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:54,708 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:54,712 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:54,716 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:43:54,716 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:54,717 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:54,718 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:54,718 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,718 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,718 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:54,719 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:54,720 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:54,721 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:54,722 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:54,722 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:54,722 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:54,723 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:54,723 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,723 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,723 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:54,735 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:54,743 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:54,751 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:54,758 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:54,759 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:54,765 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:54,766 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:54,766 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:54,766 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,766 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:54,767 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:54,768 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:54,768 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:54,769 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:54,769 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:54,770 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:54,770 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:54,773 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 14])", "<class 'int'>: 13")
2023-10-11 12:43:54,774 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:54,774 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:54,774 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:54,775 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:54,776 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:54,776 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:54,776 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:54,779 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:54,783 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:54,786 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,786 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,786 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:54,792 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:54,796 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:54,800 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:54,804 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,804 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:54,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:54,810 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:54,814 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,814 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,814 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:54,819 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:54,831 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:54,835 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:54,839 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,839 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:54,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:54,844 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:54,848 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,848 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,848 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:54,854 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:54,858 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:54,862 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:54,866 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,866 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:54,867 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:54,871 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:54,875 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,875 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,875 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:54,880 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:54,884 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:54,888 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:54,892 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,893 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:54,894 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:54,898 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:54,901 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,902 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,902 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:54,907 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:54,911 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:54,915 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:54,919 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,919 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:54,920 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:54,924 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:54,927 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,928 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,928 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:54,933 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:54,937 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:54,941 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:54,945 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,945 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:54,946 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:54,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:54,953 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,954 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,954 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:54,959 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:54,963 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:54,967 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:54,971 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,971 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:54,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:54,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:54,980 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:54,980 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:54,980 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:54,986 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:54,990 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:54,994 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:54,998 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:54,998 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:54,999 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:55,003 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:55,007 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,007 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,007 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:55,013 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:55,017 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:55,021 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:55,025 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:55,026 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:55,027 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:55,031 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:55,034 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,034 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,034 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:55,039 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:55,045 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:55,049 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:55,053 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:55,053 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:55,055 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:55,058 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:55,062 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,063 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,063 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:55,068 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:55,072 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:55,078 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:55,089 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:55,089 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:55,091 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:55,094 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:55,095 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,095 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,095 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:55,100 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:55,104 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:55,108 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:55,112 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:43:55,113 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:55,114 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:55,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:55,115 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,115 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,115 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:55,118 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:55,123 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:55,126 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:55,128 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,128 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:55,128 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:55,129 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:55,129 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,129 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,129 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:55,139 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:55,148 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:55,158 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:55,167 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:55,168 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:55,174 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:55,175 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:55,175 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:55,175 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,175 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:55,176 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:55,177 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:55,177 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:55,178 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,178 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:55,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:55,179 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:55,183 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 15])", "<class 'int'>: 14")
2023-10-11 12:43:55,183 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,183 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:55,183 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:55,184 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:55,185 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:55,185 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,186 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:55,189 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:55,192 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:55,196 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,196 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,196 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:55,201 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:55,205 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:55,209 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:55,214 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,214 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:55,216 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:55,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:55,223 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,223 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,223 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:55,228 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:55,233 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:55,236 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:55,240 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,240 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:55,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:55,245 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:55,249 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,249 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,249 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:55,254 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:55,259 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:55,263 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:55,271 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,271 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:55,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:55,276 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:55,279 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,280 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,280 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:55,285 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:55,289 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:55,293 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:55,297 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,297 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:55,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:55,302 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:55,305 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,306 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,306 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:55,310 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:55,315 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:55,319 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:55,327 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,327 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:55,329 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:55,333 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:55,336 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,337 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,337 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:55,343 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:55,347 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:55,352 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:55,356 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,357 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:55,358 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:55,361 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:55,365 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,365 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,365 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:55,370 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:55,375 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:55,378 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:55,382 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,383 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:55,384 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:55,387 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:55,391 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,391 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,391 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:55,397 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:55,401 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:55,405 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:55,409 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,409 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:55,410 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:55,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:55,417 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,417 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,417 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:55,422 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:55,427 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:55,430 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:55,435 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,435 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:55,436 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:55,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:55,443 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,443 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,443 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:55,449 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:55,453 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:55,458 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:55,461 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,462 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:55,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:55,466 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:55,470 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,470 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,471 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:55,475 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:55,480 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:55,484 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:55,488 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,488 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:55,490 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:55,493 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:55,494 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,494 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,494 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:55,499 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:55,503 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:55,508 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:55,512 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:43:55,512 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:55,513 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:55,514 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:55,514 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,514 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,514 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:55,517 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:55,518 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:55,518 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:55,519 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,519 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:55,520 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:55,520 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:55,521 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,521 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,521 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:55,531 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:55,539 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:55,547 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:55,555 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:55,555 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:55,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:55,562 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:55,562 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:55,563 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,563 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:55,563 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:55,564 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:55,564 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:55,565 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,565 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:55,565 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:55,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:55,570 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 16])", "<class 'int'>: 15")
2023-10-11 12:43:55,570 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,570 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:55,571 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:55,571 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:55,572 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:55,572 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,573 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:55,576 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:55,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:55,583 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,583 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,583 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:55,593 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:55,597 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:55,601 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:55,605 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,605 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:55,607 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:55,610 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:55,613 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,613 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,614 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:55,620 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:55,624 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:55,628 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:55,632 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,632 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:55,633 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:55,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:55,640 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,641 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,641 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:55,646 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:55,650 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:55,654 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:55,658 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,658 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:55,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:55,662 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:55,666 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,666 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,666 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:55,671 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:55,675 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:55,679 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:55,686 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,686 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:55,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:55,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:55,694 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,694 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,694 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:55,699 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:55,708 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:55,712 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:55,716 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,716 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:55,718 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:55,721 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:55,725 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,725 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,725 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:55,730 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:55,734 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:55,738 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:55,742 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,742 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:55,743 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:55,746 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:55,750 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,750 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,751 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:55,756 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:55,760 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:55,764 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:55,767 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,768 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:55,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:55,773 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:55,776 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,776 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,776 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:55,781 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:55,785 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:55,790 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:55,794 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,794 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:55,795 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:55,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:55,803 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,803 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,803 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:55,808 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:55,812 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:55,816 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:55,820 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,821 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:55,822 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:55,826 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:55,830 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,830 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,830 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:55,836 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:55,840 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:55,845 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:55,849 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,849 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:55,850 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:55,854 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:55,858 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,858 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,858 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:55,863 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:55,868 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:55,872 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:55,877 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,877 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:55,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:55,882 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:55,883 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,883 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,883 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:55,888 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:55,893 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:55,897 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:55,900 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:43:55,901 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:55,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:55,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:55,903 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,903 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,903 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:55,904 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:55,907 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:55,907 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:55,908 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,908 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:55,909 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:55,909 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:55,910 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,910 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,910 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:55,919 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:55,928 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:55,936 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:55,944 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:55,945 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:55,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:55,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:55,951 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:55,951 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,952 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:55,952 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:55,953 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:55,953 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:55,954 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,954 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:55,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:55,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:55,959 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 17])", "<class 'int'>: 16")
2023-10-11 12:43:55,959 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:55,959 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:55,960 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:55,960 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:55,961 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:55,961 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:55,962 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:55,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:55,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:55,972 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:55,972 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:55,972 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:55,978 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:55,982 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:55,987 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:55,991 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:55,991 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:55,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:55,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:56,001 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,001 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,001 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:56,007 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:56,013 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:56,017 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:56,021 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,021 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:56,023 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:56,026 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:56,030 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,030 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,030 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:56,039 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:56,043 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:56,047 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:56,051 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,051 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:56,053 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:56,056 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:56,060 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,060 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,060 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:56,065 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:56,069 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:56,073 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:56,077 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,078 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:56,079 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:56,082 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:56,086 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,086 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,086 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:56,091 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:56,096 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:56,100 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:56,104 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,104 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:56,105 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:56,109 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:56,112 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,113 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,113 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:56,119 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:56,123 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:56,128 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:56,132 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,132 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:56,134 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:56,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:56,141 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,141 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,141 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:56,146 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:56,150 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:56,155 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:56,159 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,159 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:56,161 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:56,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:56,168 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,168 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,168 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:56,173 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:56,178 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:56,182 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:56,186 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,186 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:56,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:56,190 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:56,194 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,194 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,194 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:56,199 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:56,205 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:56,209 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:56,213 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,213 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:56,215 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:56,218 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:56,221 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,222 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,222 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:56,227 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:56,231 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:56,235 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:56,239 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,239 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:56,241 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:56,244 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:56,248 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,248 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,248 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:56,253 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:56,258 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:56,263 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:56,267 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,267 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:56,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:56,272 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:56,273 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,273 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,273 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:56,283 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:56,288 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:56,293 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:56,297 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:43:56,297 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:56,298 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:56,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:56,300 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,300 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,300 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:56,301 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:56,301 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:56,302 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:56,303 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:56,303 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:56,304 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:56,304 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:56,304 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,305 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,305 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:56,315 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:56,323 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:56,338 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:56,362 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:56,362 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:56,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:56,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:56,372 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:56,372 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,372 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:56,373 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:56,373 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:56,374 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:56,374 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:56,375 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:56,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:56,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:56,379 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 18])", "<class 'int'>: 17")
2023-10-11 12:43:56,379 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,379 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:56,380 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:56,380 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:56,381 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:56,382 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:56,382 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:56,385 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:56,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:56,392 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,392 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,392 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:56,398 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:56,403 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:56,408 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:56,413 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,413 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:56,414 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:56,418 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:56,421 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,422 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,422 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:56,427 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:56,432 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:56,436 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:56,440 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,441 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:56,442 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:56,445 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:56,449 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,449 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,450 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:56,455 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:56,460 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:56,465 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:56,470 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,470 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:56,471 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:56,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:56,479 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,479 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,479 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:56,484 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:56,494 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:56,498 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:56,503 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,503 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:56,505 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:56,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:56,512 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,512 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,512 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:56,517 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:56,521 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:56,525 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:56,530 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,530 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:56,531 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:56,535 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:56,538 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,539 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,539 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:56,543 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:56,548 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:56,552 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:56,557 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,557 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:56,558 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:56,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:56,565 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,565 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,566 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:56,570 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:56,575 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:56,579 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:56,596 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,596 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:56,598 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:56,601 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:56,605 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,605 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,605 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:56,615 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:56,621 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:56,625 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:56,629 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,630 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:56,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:56,634 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:56,638 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,638 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,638 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:56,643 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:56,648 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:56,653 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:56,657 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,657 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:56,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:56,662 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:56,666 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,666 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,666 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:56,671 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:56,676 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:56,680 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:56,683 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,684 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:56,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:56,688 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:56,692 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,692 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,692 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:56,698 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:56,702 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:56,707 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:56,711 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,711 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:56,712 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:56,716 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:56,716 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,716 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,717 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:56,722 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:56,726 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:56,731 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:56,735 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:43:56,735 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:56,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:56,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:56,738 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,738 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,738 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:56,739 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:56,740 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:56,741 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:56,742 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:56,742 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:56,742 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:56,743 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:56,743 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,743 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,743 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:56,754 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:56,762 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:56,771 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:56,779 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:56,779 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:56,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:56,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:56,787 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:56,787 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,787 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:56,787 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:56,788 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:56,789 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:56,789 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:56,790 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:56,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:56,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:56,794 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 19])", "<class 'int'>: 18")
2023-10-11 12:43:56,794 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:56,795 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:56,795 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:56,796 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:56,797 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:56,797 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:56,798 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:56,801 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:56,804 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:56,807 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,808 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,808 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:56,813 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:56,817 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:56,822 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:56,826 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:56,826 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:56,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:56,831 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:56,834 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,835 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,835 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:56,840 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:56,844 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:56,849 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:56,853 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:56,853 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:56,855 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:56,858 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:56,862 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,862 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,862 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:56,867 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:56,872 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:56,876 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:56,881 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:56,881 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:56,882 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:56,886 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:56,890 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,890 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,891 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:56,896 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:56,901 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:56,905 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:56,909 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:56,909 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:56,911 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:56,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:56,918 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,918 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,918 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:56,923 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:56,928 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:56,932 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:56,936 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:56,937 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:56,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:56,942 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:56,945 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,946 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,946 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:56,951 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:56,955 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:56,959 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:56,963 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:56,963 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:56,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:56,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:56,973 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:56,973 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:56,973 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:56,978 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:56,983 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:56,987 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:56,992 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:56,992 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:56,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:56,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:57,001 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,001 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,001 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:57,006 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:57,011 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:57,015 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:57,020 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:57,020 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:57,021 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:57,025 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:57,029 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,029 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,029 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:57,034 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:57,039 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:57,043 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:57,047 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:57,047 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:57,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:57,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:57,056 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,056 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,056 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:57,061 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:57,066 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:57,070 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:57,074 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:57,074 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:57,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:57,079 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:57,083 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,083 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,083 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:57,094 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:57,099 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:57,104 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:57,109 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:57,109 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:57,110 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:57,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:57,114 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,114 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,114 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:57,122 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:57,126 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:57,131 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:57,136 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:43:57,136 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:57,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:57,138 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:57,138 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,138 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,138 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:57,139 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:57,140 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:57,141 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:57,142 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:57,142 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:57,142 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:57,143 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:57,143 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,144 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,144 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:57,156 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:57,167 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:57,178 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:57,190 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:57,191 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:57,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:57,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:57,198 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:57,198 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,198 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:57,199 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:57,199 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:57,200 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:57,201 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:57,201 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:57,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:57,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:57,205 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 20])", "<class 'int'>: 19")
2023-10-11 12:43:57,205 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,205 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:57,206 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:57,207 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:57,207 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:57,208 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:57,208 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:57,211 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:57,214 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:57,218 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,218 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,218 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:57,224 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:57,229 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:57,234 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:57,238 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,239 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:57,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:57,243 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:57,247 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,247 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,247 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:57,253 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:57,258 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:57,262 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:57,267 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,268 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:57,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:57,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:57,277 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,277 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,277 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:57,283 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:57,288 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:57,293 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:57,298 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,298 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:57,300 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:57,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:57,307 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,307 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,307 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:57,314 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:57,319 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:57,323 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:57,334 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,334 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:57,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:57,339 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:57,343 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,344 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,344 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:57,349 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:57,354 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:57,358 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:57,363 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,363 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:57,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:57,369 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:57,373 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,373 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,373 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:57,379 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:57,383 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:57,388 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:57,393 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,393 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:57,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:57,397 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:57,401 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,401 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,402 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:57,407 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:57,411 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:57,415 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:57,420 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,420 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:57,421 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:57,425 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:57,429 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,429 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,429 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:57,434 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:57,439 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:57,443 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:57,447 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,448 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:57,449 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:57,452 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:57,456 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,456 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,456 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:57,461 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:57,465 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:57,469 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:57,473 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,474 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:57,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:57,479 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:57,482 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,482 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,482 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:57,488 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:57,492 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:57,496 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:57,500 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,500 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:57,501 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:57,505 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:57,508 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,508 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,509 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:57,514 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:57,518 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:57,525 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:57,529 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,529 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:57,530 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:57,534 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:57,534 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,534 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,535 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:57,540 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:57,544 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:57,549 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:57,553 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:43:57,553 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:57,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:57,555 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:57,555 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,556 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,556 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:57,556 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:57,557 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:57,558 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:57,559 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:57,559 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:57,559 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:57,560 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:57,560 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,560 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,560 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:57,571 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:57,580 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:57,594 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:57,602 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:57,603 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:57,609 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:57,609 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:57,610 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:57,610 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,610 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:57,610 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:57,611 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:57,611 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:57,612 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:57,612 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:57,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:57,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:57,617 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 21])", "<class 'int'>: 20")
2023-10-11 12:43:57,617 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:57,617 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:57,618 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:57,619 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:57,620 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:57,620 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:57,620 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:57,624 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:57,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:57,631 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,631 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,631 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:57,637 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:57,641 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:57,646 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:57,650 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,650 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:57,652 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:57,655 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:57,659 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,660 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,660 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:57,666 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:57,671 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:57,675 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:57,679 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,679 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:57,681 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:57,684 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:57,688 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,688 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,688 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:57,721 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:57,726 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:57,730 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:57,735 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,735 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:57,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:57,740 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:57,744 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,744 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,745 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:57,751 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:57,755 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:57,760 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:57,764 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,764 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:57,766 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:57,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:57,773 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,773 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,773 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:57,778 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:57,783 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:57,787 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:57,791 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,791 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:57,793 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:57,796 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:57,800 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,800 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,800 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:57,805 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:57,810 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:57,814 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:57,846 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,847 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:57,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:57,851 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:57,855 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,856 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,856 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:57,861 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:57,866 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:57,870 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:57,874 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,874 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:57,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:57,879 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:57,883 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,883 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,883 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:57,888 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:57,893 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:57,897 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:57,901 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,902 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:57,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:57,906 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:57,910 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,911 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,911 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:57,916 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:57,921 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:57,925 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:57,932 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,932 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:57,933 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:57,937 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:57,940 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,941 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,941 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:57,946 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:57,950 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:57,954 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:57,958 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,959 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:57,960 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:57,963 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:57,967 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,967 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,967 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:57,972 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:57,977 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:57,981 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:57,986 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:57,987 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:57,988 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:57,991 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:57,992 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:57,992 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:57,992 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:57,997 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:58,002 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:58,006 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:58,010 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:43:58,010 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:58,012 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:58,012 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:58,013 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,013 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,013 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:58,014 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:58,015 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:58,016 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:58,016 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,016 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:58,017 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:58,017 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:58,018 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,018 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,018 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:58,031 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:58,038 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:58,047 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:58,055 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:58,056 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:58,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:58,063 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:58,063 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:58,063 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,063 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:58,064 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:58,065 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:58,065 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:58,066 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,066 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:58,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:58,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:58,071 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 22])", "<class 'int'>: 21")
2023-10-11 12:43:58,071 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,071 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:58,072 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:58,072 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:58,073 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:58,074 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,074 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:58,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:58,080 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:58,084 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,084 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,085 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:58,092 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:58,098 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:58,104 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:58,109 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,109 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:58,111 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:58,114 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:58,117 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,118 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,118 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:58,123 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:58,127 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:58,132 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:58,136 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,136 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:58,138 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:58,141 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:58,145 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,145 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,145 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:58,151 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:58,155 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:58,159 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:58,164 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,164 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:58,165 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:58,169 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:58,172 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,173 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,173 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:58,178 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:58,187 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:58,191 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:58,195 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,196 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:58,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:58,200 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:58,204 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,204 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,204 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:58,210 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:58,215 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:58,219 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:58,223 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,223 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:58,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:58,227 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:58,231 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,231 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,231 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:58,236 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:58,241 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:58,244 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:58,249 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,249 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:58,250 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:58,253 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:58,257 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,257 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,257 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:58,262 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:58,266 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:58,270 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:58,275 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,275 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:58,277 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:58,280 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:58,284 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,284 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,284 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:58,289 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:58,293 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:58,297 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:58,302 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,302 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:58,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:58,306 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:58,309 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,310 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,310 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:58,315 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:58,319 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:58,323 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:58,327 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,327 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:58,328 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:58,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:58,335 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,335 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,335 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:58,340 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:58,346 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:58,350 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:58,354 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,354 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:58,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:58,359 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:58,362 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,363 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,363 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:58,368 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:58,372 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:58,377 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:58,381 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,381 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:58,382 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:58,385 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:58,386 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,386 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,386 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:58,392 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:58,396 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:58,400 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:58,404 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:43:58,404 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:58,405 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:58,406 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:58,406 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,407 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,407 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:58,407 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:58,408 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:58,409 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:58,410 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,410 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:58,410 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:58,411 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:58,411 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,411 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,411 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:58,420 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:58,428 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:58,438 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:58,450 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:58,450 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:58,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:58,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:58,459 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:58,459 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,459 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:58,460 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:58,460 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:58,461 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:58,462 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,462 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:58,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:58,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:58,467 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 23])", "<class 'int'>: 22")
2023-10-11 12:43:58,467 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,467 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:58,468 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:58,468 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:58,469 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:58,470 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,470 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:58,473 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:58,476 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:58,480 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,480 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,480 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:58,486 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:58,490 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:58,494 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:58,498 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,498 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:58,499 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:58,503 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:58,506 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,506 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,506 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:58,511 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:58,516 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:58,519 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:58,524 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,524 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:58,526 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:58,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:58,532 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,533 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,533 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:58,538 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:58,542 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:58,546 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:58,551 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,551 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:58,553 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:58,556 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:58,559 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,560 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,560 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:58,599 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:58,604 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:58,608 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:58,612 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,612 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:58,614 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:58,617 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:58,621 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,621 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,621 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:58,626 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:58,631 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:58,635 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:58,639 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,639 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:58,642 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:58,647 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:58,652 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,653 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,653 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:58,658 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:58,663 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:58,668 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:58,673 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,673 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:58,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:58,678 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:58,682 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,683 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,683 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:58,688 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:58,693 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:58,699 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:58,705 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,706 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:58,708 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:58,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:58,720 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,720 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,720 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:58,729 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:58,735 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:58,741 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:58,747 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,747 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:58,749 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:58,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:58,760 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,760 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,761 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:58,768 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:58,774 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:58,780 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:58,786 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,786 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:58,788 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:58,792 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:58,796 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,796 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,796 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:58,802 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:58,806 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:58,810 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:58,814 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,814 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:58,816 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:58,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:58,823 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,823 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,823 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:58,828 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:58,838 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:58,843 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:58,847 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,848 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:58,849 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:58,852 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:58,853 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,853 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,853 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:58,858 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:58,863 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:58,867 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:58,871 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:43:58,871 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:58,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:58,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:58,874 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,874 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,874 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:58,875 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:58,876 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:58,877 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:58,878 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,878 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:58,879 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:58,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:58,880 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,881 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,881 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:58,891 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:58,900 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:58,907 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:58,914 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:58,915 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:58,923 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:58,923 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:58,924 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:58,924 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,924 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:58,925 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:58,925 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:58,926 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:58,927 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,927 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:58,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:58,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:58,933 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 24])", "<class 'int'>: 23")
2023-10-11 12:43:58,933 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:58,933 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:58,934 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:58,935 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:58,935 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:58,936 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:58,936 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:58,940 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:58,943 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:58,947 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,947 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,947 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:58,953 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:58,957 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:58,962 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:58,966 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:58,966 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:58,967 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:58,971 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:58,974 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:58,974 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:58,974 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:58,979 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:58,984 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:58,988 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:58,993 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:58,993 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:58,994 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:58,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:59,001 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,001 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,002 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:59,011 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:59,016 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:59,020 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:59,024 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,024 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:59,025 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:59,029 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:59,032 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,032 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,032 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:59,037 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:59,042 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:59,046 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:59,050 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,050 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:59,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:59,055 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:59,059 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,059 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,059 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:59,065 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:59,068 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:59,073 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:59,082 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,082 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:59,083 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:59,087 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:59,090 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,090 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,090 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:59,107 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:59,111 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:59,116 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:59,120 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,120 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:59,121 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:59,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:59,128 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,128 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,128 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:59,140 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:59,145 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:59,149 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:59,154 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,154 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:59,155 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:59,159 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:59,162 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,162 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,162 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:59,168 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:59,176 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:59,180 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:59,184 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,184 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:59,185 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:59,188 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:59,192 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,192 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,192 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:59,198 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:59,202 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:59,207 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:59,211 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,211 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:59,212 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:59,215 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:59,219 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,219 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,219 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:59,224 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:59,228 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:59,232 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:59,236 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:59,237 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:59,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:59,244 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,244 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,244 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:59,249 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:59,253 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:59,257 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:59,262 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,262 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:59,263 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:59,266 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:59,267 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,267 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,267 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:59,272 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:59,276 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:59,280 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:59,284 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:43:59,285 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:59,286 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:59,286 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:59,286 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,287 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,287 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:59,288 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:59,288 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:59,289 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:59,290 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:59,290 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:59,290 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:59,291 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:59,291 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,291 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,291 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:59,301 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:59,308 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:59,316 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:59,323 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:59,323 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:59,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:59,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:59,332 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:59,333 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,333 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:59,334 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:59,334 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:59,335 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:59,335 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:59,335 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:59,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:59,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:59,340 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 25])", "<class 'int'>: 24")
2023-10-11 12:43:59,340 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,340 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:59,340 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:59,341 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:59,342 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:59,342 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:59,342 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:59,345 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:59,348 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:59,351 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,352 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,352 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:59,357 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:59,362 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:59,366 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:59,371 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,371 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:59,372 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:59,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:59,379 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,379 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,379 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:59,384 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:59,389 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:59,393 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:59,397 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,397 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:59,399 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:59,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:59,405 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,405 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,405 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:59,411 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:59,415 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:59,419 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:59,424 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,424 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:59,425 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:59,428 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:59,431 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,432 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,432 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:59,437 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:59,442 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:59,446 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:59,450 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,450 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:59,452 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:59,455 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:59,458 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,458 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,458 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:59,464 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:59,469 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:59,473 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:59,477 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,477 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:59,479 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:59,482 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:59,485 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,485 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,485 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:59,490 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:59,495 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:59,499 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:59,503 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,503 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:59,504 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:59,507 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:59,511 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,511 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,511 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:59,516 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:59,520 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:59,524 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:59,529 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,529 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:59,530 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:59,533 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:59,536 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,536 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,536 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:59,541 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:59,546 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:59,550 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:59,554 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,554 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:59,555 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:59,558 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:59,562 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,562 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,562 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:59,568 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:59,572 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:59,576 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:59,580 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,580 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:59,582 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:59,585 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:59,589 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,589 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,589 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:59,597 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:59,602 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:59,606 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:59,610 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,611 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:59,612 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:59,615 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:59,619 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,619 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,619 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:43:59,625 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:43:59,630 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:43:59,635 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:43:59,639 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,639 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:43:59,641 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:43:59,644 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:59,645 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,645 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,645 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:43:59,656 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:43:59,661 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:43:59,665 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:43:59,669 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:43:59,669 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:43:59,670 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:43:59,671 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:59,671 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,672 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,672 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:43:59,672 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:43:59,673 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:43:59,674 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:43:59,675 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:59,675 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:43:59,675 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:43:59,675 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:59,676 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,676 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,676 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:43:59,685 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:43:59,693 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:43:59,700 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:43:59,707 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:43:59,708 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:43:59,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:43:59,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:59,715 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:43:59,715 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,715 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:43:59,716 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:43:59,716 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:43:59,717 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:43:59,717 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:59,718 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:43:59,718 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:43:59,718 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:59,722 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 26])", "<class 'int'>: 25")
2023-10-11 12:43:59,722 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:43:59,722 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:43:59,723 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:43:59,724 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:43:59,724 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:43:59,725 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:43:59,725 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:43:59,728 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:43:59,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:59,735 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,735 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,735 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:43:59,740 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:43:59,745 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:43:59,749 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:43:59,753 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,753 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:43:59,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:43:59,758 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:59,761 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,761 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,762 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:43:59,767 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:43:59,771 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:43:59,775 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:43:59,779 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,779 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:43:59,781 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:43:59,784 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:59,787 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,788 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,788 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:43:59,793 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:43:59,797 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:43:59,801 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:43:59,805 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,805 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:43:59,807 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:43:59,810 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:59,813 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,813 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,813 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:43:59,818 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:43:59,823 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:43:59,827 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:43:59,831 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,831 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:43:59,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:43:59,835 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:59,839 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,839 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,839 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:43:59,844 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:43:59,849 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:43:59,853 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:43:59,857 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,858 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:43:59,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:43:59,862 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:59,865 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,865 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,865 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:43:59,871 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:43:59,875 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:43:59,879 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:43:59,884 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,884 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:43:59,885 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:43:59,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:59,892 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,892 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,892 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:43:59,897 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:43:59,902 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:43:59,906 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:43:59,910 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,910 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:43:59,911 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:43:59,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:59,918 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,918 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,918 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:43:59,923 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:43:59,927 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:43:59,932 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:43:59,937 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,937 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:43:59,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:43:59,941 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:59,945 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,945 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,945 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:43:59,951 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:43:59,956 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:43:59,960 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:43:59,966 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,966 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:43:59,967 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:43:59,971 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:59,974 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:43:59,975 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:43:59,975 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:43:59,980 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:43:59,985 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:43:59,989 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:43:59,994 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:43:59,994 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:43:59,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:43:59,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:00,002 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,002 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,002 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:00,007 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:00,011 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:00,016 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:00,020 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:44:00,021 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:00,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:00,025 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:00,026 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,026 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,026 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:00,031 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:00,036 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:00,040 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:00,045 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:44:00,045 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:00,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:00,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:00,047 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,047 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,047 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:00,048 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:00,048 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:00,049 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:00,050 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:00,050 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:00,050 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:00,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:00,051 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,051 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,051 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:00,061 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:00,070 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:00,078 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:00,085 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:00,085 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:00,097 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:00,098 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:00,098 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:00,098 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,099 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:00,099 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:00,100 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:00,100 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:00,101 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:00,101 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:00,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:00,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:00,106 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 27])", "<class 'int'>: 26")
2023-10-11 12:44:00,106 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,106 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:00,107 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:00,107 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:00,108 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:00,109 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:00,109 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:00,112 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:00,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:00,119 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,119 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,120 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:00,126 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:00,131 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:00,135 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:00,140 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,140 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:00,142 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:00,145 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:00,149 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,149 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,149 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:00,154 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:00,159 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:00,164 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:00,168 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,168 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:00,170 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:00,173 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:00,177 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,177 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,177 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:00,183 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:00,187 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:00,192 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:00,196 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,196 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:00,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:00,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:00,205 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,205 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,205 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:00,210 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:00,215 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:00,220 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:00,225 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,225 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:00,226 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:00,230 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:00,233 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,234 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,234 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:00,239 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:00,243 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:00,247 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:00,252 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,252 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:00,253 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:00,256 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:00,260 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,260 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,260 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:00,265 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:00,270 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:00,274 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:00,278 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,279 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:00,280 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:00,283 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:00,287 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,287 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,287 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:00,293 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:00,298 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:00,302 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:00,307 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,307 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:00,308 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:00,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:00,315 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,316 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,316 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:00,320 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:00,325 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:00,329 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:00,333 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,333 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:00,335 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:00,338 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:00,342 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,342 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,342 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:00,348 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:00,352 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:00,357 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:00,361 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,361 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:00,362 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:00,366 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:00,370 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,370 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,370 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:00,376 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:00,380 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:00,384 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:00,388 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,388 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:00,390 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:00,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:00,398 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,398 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,399 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:00,404 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:00,408 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:00,413 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:00,418 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,418 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:00,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:00,423 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:00,424 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,424 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,425 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:00,458 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:00,463 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:00,468 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:00,473 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:44:00,473 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:00,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:00,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:00,476 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,476 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,476 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:00,477 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:00,477 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:00,478 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:00,479 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:00,479 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:00,480 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:00,480 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:00,481 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,481 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,481 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:00,490 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:00,498 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:00,505 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:00,513 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:00,513 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:00,520 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:00,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:00,521 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:00,521 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,522 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:00,522 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:00,523 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:00,523 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:00,524 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:00,524 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:00,525 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:00,525 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:00,529 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 28])", "<class 'int'>: 27")
2023-10-11 12:44:00,529 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:00,529 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:00,530 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:00,531 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:00,531 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:00,532 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:00,532 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:00,536 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:00,539 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:00,543 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,543 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,543 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:00,549 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:00,554 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:00,559 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:00,563 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,563 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:00,565 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:00,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:00,573 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,573 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,573 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:00,578 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:00,583 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:00,587 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:00,596 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,596 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:00,598 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:00,602 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:00,605 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,606 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,606 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:00,612 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:00,616 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:00,621 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:00,626 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,626 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:00,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:00,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:00,635 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,636 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,636 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:00,641 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:00,646 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:00,650 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:00,655 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,655 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:00,657 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:00,660 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:00,664 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,665 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,665 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:00,671 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:00,675 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:00,680 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:00,685 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,685 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:00,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:00,690 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:00,694 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,694 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,694 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:00,699 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:00,705 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:00,710 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:00,715 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,715 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:00,716 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:00,720 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:00,724 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,724 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,724 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:00,730 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:00,734 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:00,739 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:00,743 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,743 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:00,745 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:00,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:00,752 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,753 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,753 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:00,758 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:00,763 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:00,767 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:00,771 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,771 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:00,772 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:00,776 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:00,779 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,780 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,780 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:00,785 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:00,790 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:00,794 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:00,798 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,799 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:00,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:00,803 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:00,807 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,807 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,807 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:00,813 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:00,817 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:00,822 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:00,826 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:00,827 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:00,828 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:00,831 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:00,835 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:00,835 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:00,835 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:00,845 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:00,850 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:01,040 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:01,045 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:01,045 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:01,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:01,050 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:01,051 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,051 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,051 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:01,056 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:01,060 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:01,065 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:01,069 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:44:01,069 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:01,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:01,071 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:01,072 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,072 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,072 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:01,072 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:01,073 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:01,074 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:01,075 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,075 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:01,075 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:01,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:01,076 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,076 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,076 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:01,086 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:01,098 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:01,105 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:01,112 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:01,113 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:01,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:01,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:01,121 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:01,121 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,121 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:01,122 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:01,122 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:01,123 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:01,123 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,124 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:01,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:01,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:01,128 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 29])", "<class 'int'>: 28")
2023-10-11 12:44:01,128 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,128 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:01,129 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:01,129 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:01,130 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:01,131 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,131 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:01,134 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:01,138 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:01,142 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,142 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,142 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:01,147 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:01,152 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:01,156 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:01,161 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,161 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:01,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:01,166 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:01,169 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,169 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,169 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:01,175 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:01,179 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:01,183 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:01,190 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,190 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:01,192 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:01,195 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:01,198 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,198 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,199 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:01,204 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:01,208 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:01,212 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:01,216 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,216 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:01,218 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:01,221 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:01,225 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,225 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,225 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:01,230 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:01,234 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:01,238 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:01,242 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,243 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:01,244 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:01,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:01,251 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,251 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,251 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:01,256 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:01,260 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:01,266 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:01,270 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,270 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:01,272 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:01,275 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:01,278 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,279 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,279 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:01,284 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:01,288 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:01,293 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:01,297 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,297 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:01,298 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:01,301 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:01,305 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,305 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,305 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:01,310 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:01,315 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:01,324 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:01,328 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,328 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:01,330 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:01,333 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:01,336 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,336 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,336 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:01,342 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:01,346 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:01,350 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:01,354 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,355 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:01,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:01,359 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:01,362 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,362 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,362 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:01,368 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:01,372 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:01,377 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:01,381 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,381 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:01,383 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:01,385 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:01,389 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,389 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,389 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:01,394 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:01,399 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:01,403 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:01,407 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,407 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:01,408 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:01,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:01,415 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,415 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,416 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:01,421 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:01,425 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:01,429 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:01,434 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,434 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:01,435 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:01,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:01,439 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,440 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,440 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:01,450 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:01,454 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:01,458 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:01,462 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:44:01,463 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:01,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:01,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:01,464 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,465 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,465 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:01,465 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:01,466 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:01,467 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:01,468 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,468 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:01,468 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:01,469 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:01,469 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,469 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,469 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:01,479 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:01,486 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:01,494 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:01,501 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:01,501 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:01,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:01,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:01,509 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:01,509 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,509 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:01,510 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:01,510 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:01,511 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:01,511 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,511 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:01,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:01,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:01,516 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 30])", "<class 'int'>: 29")
2023-10-11 12:44:01,516 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,516 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:01,517 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:01,517 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:01,518 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:01,518 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,518 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:01,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:01,525 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:01,528 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,528 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,528 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:01,534 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:01,538 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:01,543 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:01,547 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,547 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:01,548 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:01,551 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:01,555 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,555 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,555 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:01,561 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:01,565 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:01,570 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:01,574 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,574 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:01,575 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:01,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:01,582 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,582 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,582 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:01,588 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:01,592 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:01,597 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:01,601 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,601 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:01,602 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:01,606 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:01,609 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,609 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,609 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:01,614 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:01,618 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:01,622 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:01,627 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,627 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:01,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:01,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:01,635 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,635 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,635 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:01,640 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:01,645 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:01,650 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:01,654 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,654 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:01,656 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:01,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:01,662 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,662 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,662 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:01,667 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:01,672 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:01,676 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:01,681 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,681 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:01,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:01,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:01,689 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,689 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,689 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:01,695 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:01,700 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:01,704 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:01,708 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,708 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:01,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:01,713 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:01,716 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,716 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,716 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:01,721 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:01,726 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:01,730 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:01,735 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,735 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:01,736 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:01,739 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:01,742 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,743 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,743 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:01,747 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:01,752 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:01,756 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:01,760 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,760 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:01,761 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:01,765 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:01,768 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,768 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,768 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:01,773 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:01,777 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:01,782 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:01,786 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,786 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:01,787 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:01,791 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:01,794 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,794 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,795 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:01,800 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:01,804 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:01,809 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:01,813 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,813 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:01,815 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:01,818 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:01,818 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,818 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,819 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:01,824 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:01,828 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:01,833 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:01,837 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:44:01,837 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:01,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:01,839 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:01,839 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,839 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,839 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:01,840 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:01,841 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:01,842 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:01,846 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,846 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:01,847 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:01,847 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:01,847 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,848 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,848 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:01,859 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:01,866 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:01,873 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:01,881 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:01,881 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:01,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:01,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:01,888 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:01,889 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,889 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:01,889 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:01,890 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:01,890 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:01,891 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,891 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:01,891 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:01,892 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:01,895 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 31])", "<class 'int'>: 30")
2023-10-11 12:44:01,895 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:01,895 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:01,896 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:01,896 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:01,897 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:01,898 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:01,898 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:01,901 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:01,904 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:01,907 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,907 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,907 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:01,912 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:01,918 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:01,921 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:01,926 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:01,926 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:01,927 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:01,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:01,933 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,933 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,933 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:01,938 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:01,954 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:01,958 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:01,963 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:01,963 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:01,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:01,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:01,971 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:01,972 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:01,972 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:01,978 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:01,984 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:01,989 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:01,994 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:01,994 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:01,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:01,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:02,002 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,002 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,002 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:02,007 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:02,012 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:02,016 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:02,020 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,021 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:02,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:02,025 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:02,028 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,028 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,029 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:02,034 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:02,039 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:02,055 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:02,060 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,060 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:02,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:02,065 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:02,068 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,069 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,069 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:02,074 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:02,078 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:02,083 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:02,087 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,087 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:02,088 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:02,091 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:02,095 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,096 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,096 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:02,102 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:02,106 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:02,111 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:02,115 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,115 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:02,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:02,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:02,124 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,124 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,124 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:02,132 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:02,136 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:02,141 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:02,146 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,146 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:02,147 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:02,151 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:02,154 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,155 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,155 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:02,160 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:02,165 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:02,169 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:02,174 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,174 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:02,175 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:02,179 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:02,182 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,182 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,182 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:02,188 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:02,197 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:02,202 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:02,206 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,206 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:02,207 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:02,210 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:02,214 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,214 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,214 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:02,219 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:02,225 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:02,229 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:02,233 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,233 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:02,234 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:02,238 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:02,238 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,238 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,239 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:02,244 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:02,248 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:02,252 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:02,256 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:44:02,256 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:02,258 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:02,258 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:02,259 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,259 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,259 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:02,260 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:02,260 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:02,261 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:02,262 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:02,262 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:02,263 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:02,263 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:02,263 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,264 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,264 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:02,274 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:02,282 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:02,290 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:02,297 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:02,297 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:02,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:02,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:02,304 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:02,304 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,304 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:02,305 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:02,305 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:02,306 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:02,306 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:02,306 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:02,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:02,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:02,311 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 32])", "<class 'int'>: 31")
2023-10-11 12:44:02,311 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,311 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:02,312 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:02,312 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:02,313 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:02,313 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:02,314 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:02,316 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:02,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:02,323 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,323 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,323 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:02,329 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:02,333 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:02,337 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:02,342 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,342 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:02,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:02,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:02,350 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,351 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,351 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:02,356 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:02,361 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:02,365 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:02,369 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,370 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:02,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:02,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:02,378 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,378 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,379 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:02,384 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:02,388 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:02,393 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:02,397 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,397 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:02,399 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:02,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:02,405 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,406 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,406 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:02,411 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:02,416 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:02,421 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:02,424 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,425 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:02,426 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:02,430 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:02,433 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,433 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,433 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:02,439 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:02,445 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:02,452 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:02,456 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,456 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:02,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:02,461 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:02,465 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,465 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,465 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:02,470 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:02,476 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:02,481 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:02,496 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,496 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:02,497 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:02,500 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:02,504 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,504 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,504 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:02,509 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:02,553 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:02,560 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:02,564 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,565 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:02,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:02,570 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:02,574 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,574 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,574 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:02,579 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:02,584 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:02,588 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:02,592 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,593 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:02,594 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:02,597 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:02,600 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,600 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,601 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:02,607 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:02,611 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:02,616 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:02,620 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,620 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:02,622 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:02,625 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:02,628 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,628 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,629 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:02,634 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:02,638 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:02,643 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:02,648 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,648 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:02,649 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:02,653 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:02,656 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,656 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,656 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:02,662 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:02,666 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:02,671 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:02,681 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,681 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:02,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:02,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:02,686 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,686 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,686 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:02,715 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:02,720 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:02,725 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:02,730 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:44:02,730 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:02,732 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:02,732 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:02,733 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,733 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,733 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:02,734 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:02,735 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:02,736 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:02,737 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:02,737 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:02,738 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:02,738 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:02,739 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,739 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,739 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:02,749 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:02,757 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:02,766 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:02,774 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:02,774 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:02,781 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:02,782 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:02,782 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:02,783 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,783 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:02,784 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:02,785 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:02,786 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:02,787 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:02,788 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:02,788 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:02,789 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:02,792 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 33])", "<class 'int'>: 32")
2023-10-11 12:44:02,793 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:02,793 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:02,794 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:02,795 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:02,796 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:02,796 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:02,796 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:02,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:02,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:02,805 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,806 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,806 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:02,811 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:02,816 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:02,820 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:02,824 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:02,824 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:02,826 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:02,829 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:02,832 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,832 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,832 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:02,838 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:02,842 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:02,847 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:02,851 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:02,851 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:02,853 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:02,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:02,859 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,859 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,859 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:02,864 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:02,869 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:02,874 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:02,878 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:02,878 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:02,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:02,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:02,886 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,886 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,886 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:02,892 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:02,896 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:02,901 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:02,905 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:02,905 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:02,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:02,910 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:02,913 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,913 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,913 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:02,919 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:02,923 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:02,927 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:02,932 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:02,932 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:02,934 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:02,936 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:02,940 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,940 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,941 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:02,954 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:02,958 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:02,963 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:02,967 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:02,967 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:02,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:02,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:02,975 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:02,975 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:02,976 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:02,981 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:02,987 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:02,991 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:03,005 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:03,005 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:03,007 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:03,010 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:03,014 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,014 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,014 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:03,019 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:03,024 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:03,029 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:03,033 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:03,033 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:03,034 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:03,037 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:03,041 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,041 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,041 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:03,047 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:03,051 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:03,055 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:03,060 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:03,061 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:03,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:03,065 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:03,069 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,069 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,069 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:03,075 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:03,079 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:03,088 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:03,115 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:03,115 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:03,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:03,119 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:03,123 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,124 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,124 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:03,130 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:03,135 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:03,141 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:03,147 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:03,147 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:03,149 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:03,152 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:03,153 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,153 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,154 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:03,162 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:03,179 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:03,184 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:03,194 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:44:03,194 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:03,195 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:03,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:03,196 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,197 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,197 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:03,198 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:03,198 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:03,199 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:03,200 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:03,200 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:03,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:03,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:03,202 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,202 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,202 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:03,214 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:03,223 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:03,234 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:03,244 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:03,244 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:03,251 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:03,251 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:03,252 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:03,252 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,252 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:03,253 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:03,253 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:03,254 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:03,255 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:03,255 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:03,256 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:03,256 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:03,261 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 34])", "<class 'int'>: 33")
2023-10-11 12:44:03,261 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,261 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:03,262 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:03,263 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:03,263 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:03,264 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:03,264 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:03,267 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:03,270 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:03,274 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,274 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,274 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:03,280 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:03,284 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:03,289 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:03,294 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,294 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:03,296 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:03,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:03,302 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,302 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,302 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:03,308 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:03,312 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:03,317 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:03,321 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,321 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:03,323 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:03,326 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:03,329 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,329 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,329 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:03,335 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:03,340 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:03,344 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:03,348 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,348 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:03,350 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:03,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:03,356 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,356 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,356 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:03,362 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:03,366 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:03,371 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:03,375 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,375 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:03,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:03,380 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:03,383 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,384 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,384 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:03,389 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:03,394 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:03,398 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:03,403 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,403 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:03,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:03,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:03,411 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,411 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,411 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:03,416 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:03,421 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:03,425 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:03,430 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,430 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:03,431 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:03,434 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:03,438 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,438 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,438 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:03,443 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:03,453 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:03,457 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:03,462 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,462 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:03,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:03,467 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:03,470 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,470 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,470 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:03,476 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:03,480 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:03,484 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:03,489 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,489 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:03,490 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:03,494 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:03,497 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,497 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,498 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:03,503 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:03,507 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:03,513 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:03,517 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,517 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:03,518 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:03,522 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:03,525 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,525 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,525 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:03,530 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:03,535 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:03,540 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:03,544 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,544 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:03,546 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:03,549 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:03,552 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,553 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,553 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:03,558 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:03,563 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:03,567 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:03,572 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,572 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:03,573 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:03,576 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:03,577 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,577 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,577 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:03,584 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:03,588 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:03,593 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:03,597 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:44:03,597 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:03,598 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:03,599 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:03,599 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,599 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,600 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:03,600 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:03,601 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:03,602 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:03,603 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:03,603 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:03,603 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:03,604 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:03,604 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,604 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,604 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:03,613 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:03,621 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:03,628 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:03,636 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:03,636 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:03,642 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:03,642 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:03,643 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:03,643 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,643 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:03,644 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:03,644 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:03,645 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:03,645 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:03,646 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:03,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:03,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:03,650 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 35])", "<class 'int'>: 34")
2023-10-11 12:44:03,650 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,650 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:03,651 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:03,652 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:03,652 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:03,653 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:03,653 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:03,656 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:03,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:03,663 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,663 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,663 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:03,668 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:03,673 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:03,677 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:03,682 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,682 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:03,684 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:03,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:03,690 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,690 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,690 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:03,700 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:03,704 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:03,708 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:03,713 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,713 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:03,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:03,718 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:03,721 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,721 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,721 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:03,727 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:03,731 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:03,736 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:03,740 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,741 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:03,742 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:03,746 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:03,749 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,749 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,749 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:03,755 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:03,759 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:03,763 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:03,768 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,768 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:03,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:03,772 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:03,776 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,776 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,776 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:03,781 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:03,786 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:03,791 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:03,795 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,795 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:03,797 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:03,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:03,803 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,803 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,804 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:03,809 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:03,814 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:03,818 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:03,822 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,822 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:03,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:03,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:03,830 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,830 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,830 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:03,836 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:03,840 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:03,844 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:03,849 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,849 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:03,850 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:03,853 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:03,856 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,857 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,857 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:03,862 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:03,866 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:03,871 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:03,875 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,875 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:03,876 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:03,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:03,883 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,883 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,884 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:03,889 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:03,893 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:03,898 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:03,902 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,902 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:03,904 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:03,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:03,910 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,910 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,910 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:03,915 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:03,922 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:03,927 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:03,932 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,932 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:03,933 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:03,936 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:03,939 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,940 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,940 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:03,945 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:03,956 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:03,960 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:03,964 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,964 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:03,966 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:03,969 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:03,969 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,969 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:03,970 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:03,975 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:03,979 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:03,983 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:03,987 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:44:03,987 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:03,988 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:03,989 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:03,989 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,989 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,989 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:03,990 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:03,991 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:03,991 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:03,992 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:03,992 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:03,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:03,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:03,993 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:03,994 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:03,994 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:04,003 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:04,012 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:04,019 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:04,028 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:04,029 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:04,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:04,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:04,045 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:04,046 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,046 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:04,046 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:04,047 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:04,047 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:04,048 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,048 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:04,048 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:04,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:04,052 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 36])", "<class 'int'>: 35")
2023-10-11 12:44:04,052 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,053 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:04,053 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:04,054 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:04,054 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:04,055 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,055 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:04,058 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:04,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:04,065 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,065 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,065 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:04,070 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:04,075 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:04,079 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:04,083 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,084 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:04,085 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:04,088 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:04,091 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,091 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,091 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:04,097 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:04,101 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:04,106 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:04,111 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,112 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:04,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:04,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:04,119 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,120 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,120 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:04,125 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:04,130 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:04,134 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:04,138 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,139 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:04,140 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:04,144 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:04,147 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,147 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,147 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:04,153 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:04,157 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:04,162 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:04,166 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,167 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:04,168 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:04,171 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:04,175 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,175 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,175 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:04,181 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:04,186 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:04,190 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:04,194 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,195 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:04,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:04,199 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:04,203 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,203 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,203 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:04,208 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:04,213 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:04,218 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:04,222 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,222 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:04,223 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:04,227 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:04,230 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,230 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,230 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:04,237 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:04,242 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:04,246 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:04,250 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,250 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:04,252 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:04,255 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:04,258 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,259 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,259 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:04,264 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:04,268 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:04,273 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:04,279 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,279 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:04,280 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:04,283 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:04,286 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,287 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,287 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:04,292 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:04,298 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:04,303 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:04,307 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,307 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:04,309 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:04,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:04,315 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,315 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,315 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:04,321 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:04,325 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:04,330 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:04,334 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,335 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:04,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:04,339 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:04,342 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,342 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,343 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:04,349 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:04,353 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:04,358 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:04,362 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,362 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:04,364 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:04,367 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:04,367 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,367 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,368 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:04,373 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:04,378 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:04,382 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:04,387 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:44:04,387 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:04,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:04,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:04,389 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,389 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,389 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:04,390 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:04,391 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:04,391 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:04,392 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,392 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:04,393 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:04,393 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:04,393 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,394 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,394 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:04,403 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:04,411 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:04,419 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:04,427 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:04,427 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:04,434 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:04,434 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:04,434 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:04,434 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,435 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:04,435 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:04,436 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:04,436 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:04,437 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,437 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:04,438 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:04,438 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:04,441 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 37])", "<class 'int'>: 36")
2023-10-11 12:44:04,442 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,442 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:04,442 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:04,443 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:04,443 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:04,444 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,444 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:04,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:04,450 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:04,453 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,454 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,454 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:04,462 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:04,466 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:04,471 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:04,476 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,476 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:04,477 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:04,480 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:04,484 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,484 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,484 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:04,490 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:04,495 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:04,500 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:04,504 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,505 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:04,506 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:04,509 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:04,513 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,513 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,513 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:04,519 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:04,523 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:04,528 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:04,533 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,533 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:04,534 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:04,537 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:04,540 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,541 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,541 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:04,546 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:04,551 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:04,555 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:04,560 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,560 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:04,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:04,564 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:04,568 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,568 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,568 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:04,574 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:04,578 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:04,583 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:04,588 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,588 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:04,589 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:04,592 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:04,595 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,596 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,596 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:04,602 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:04,606 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:04,612 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:04,616 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,616 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:04,617 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:04,620 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:04,624 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,624 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,624 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:04,630 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:04,634 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:04,639 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:04,644 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,644 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:04,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:04,649 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:04,652 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,653 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,653 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:04,659 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:04,663 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:04,668 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:04,672 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,672 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:04,673 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:04,677 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:04,681 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,681 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,681 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:04,688 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:04,693 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:04,704 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:04,709 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,710 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:04,711 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:04,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:04,717 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,718 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,718 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:04,723 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:04,727 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:04,732 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:04,736 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,736 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:04,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:04,740 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:04,743 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,743 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,744 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:04,749 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:04,754 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:04,758 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:04,762 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,762 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:04,764 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:04,766 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:04,767 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,767 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,767 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:04,773 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:04,777 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:04,781 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:04,786 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:44:04,786 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:04,787 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:04,787 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:04,788 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,788 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,788 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:04,789 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:04,789 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:04,790 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:04,791 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,791 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:04,791 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:04,792 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:04,792 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,792 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,792 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:04,802 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:04,810 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:04,818 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:04,826 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:04,826 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:04,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:04,833 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:04,833 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:44:04,833 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,833 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:44:04,834 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:44:04,835 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:44:04,835 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:44:04,836 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,836 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:44:04,836 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:44:04,837 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:04,840 [forward.py:82 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 38])", "<class 'int'>: 37")
2023-10-11 12:44:04,840 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:04,840 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:44:04,841 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:44:04,841 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:44:04,842 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:44:04,842 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:04,843 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:44:04,845 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:44:04,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:04,852 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,852 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,852 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:44:04,859 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:44:04,863 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:44:04,867 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:44:04,872 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:04,872 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:44:04,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:44:04,876 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:04,879 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,880 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,880 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:44:04,885 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:44:04,889 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:44:04,894 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:44:04,898 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:04,898 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:44:04,899 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:44:04,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:04,907 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,908 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,908 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:44:04,918 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:44:04,923 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:44:04,928 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:44:04,933 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:04,933 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:44:04,935 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:44:04,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:04,941 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,941 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,942 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:44:04,948 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:44:04,962 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:44:04,968 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:44:04,973 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:04,973 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:44:04,974 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:44:04,977 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:04,981 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:04,981 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:04,981 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:44:04,987 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:44:04,991 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:44:04,998 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:44:05,003 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,003 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:44:05,005 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:44:05,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:05,012 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,012 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:05,012 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:44:05,018 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:44:05,023 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:44:05,034 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:44:05,039 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,039 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:44:05,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:44:05,044 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:05,048 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,049 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:05,049 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:44:05,054 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:44:05,059 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:44:05,064 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:44:05,069 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,069 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:44:05,071 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:44:05,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:05,077 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,078 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:05,078 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:44:05,083 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:44:05,088 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:44:05,093 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:44:05,097 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,098 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:44:05,099 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:44:05,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:05,106 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,106 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:05,106 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:44:05,112 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:44:05,116 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:44:05,121 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:44:05,125 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,125 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:44:05,127 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:44:05,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:05,133 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,133 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:05,133 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:44:05,139 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:44:05,144 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:44:05,149 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:44:05,153 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,154 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:44:05,155 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:44:05,158 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:05,161 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,162 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:05,162 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:44:05,167 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:44:05,173 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:44:05,178 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:44:05,182 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,182 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:44:05,184 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:44:05,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:05,187 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,188 [forward.py:83 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:44:05,188 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:44:05,194 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:44:05,198 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:44:05,210 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:44:05,214 [forward.py:101 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:44:05,214 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:44:05,215 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:44:05,216 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:05,216 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,217 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:05,217 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:44:05,217 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:44:05,218 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:44:05,219 [forward.py:87 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:44:05,220 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:44:05,220 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:44:05,220 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:44:05,221 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:44:05,221 [forward.py:82 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:44:05,221 [forward.py:83 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:44:05,221 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:44:05,231 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:44:05,240 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:44:05,248 [forward.py:87 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:44:05,256 [forward.py:101 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:44:05,256 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:44:05,264 [test.py:40 in test_hf_gen] INFO - for i in range(10):  ( ( (
,,,,,,,,,,,,,,,,, and and and and and and and and and
2023-10-11 12:44:05,264 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,264 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious???,...   ...                  
2023-10-11 12:44:05,264 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,264 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?ooooooooooooooooooo's,, and and and and and and and and
2023-10-11 12:44:05,264 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,264 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?oooooo   ...                  
2023-10-11 12:44:05,264 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,264 [test.py:40 in test_hf_gen] INFO - for i in range(10):  ( to
:::,,,,,,,,,,,,,,,, and and and and and and and and
2023-10-11 12:44:05,264 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,264 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious??                             
2023-10-11 12:44:05,265 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,265 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?oooooooooooooooooo's's, and and and and and and and and and
2023-10-11 12:44:05,265 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,265 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?ooooo                         
2023-10-11 12:44:05,265 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:44:05,274 [forward.py:21 in reset_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-11 12:44:05,275 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-11 12:44:05,276 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-11 12:44:05,276 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-11 12:44:05,276 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-11 12:44:05,276 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-11 12:44:05,276 [forward.py:21 in reset_forward] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-11 12:44:05,276 [forward.py:21 in reset_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-11 12:44:05,276 [forward.py:21 in reset_forward] DEBUG - lm_head from flexgen to old.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               2023-10-11 12:47:19,165 [connectionpool.py:273 in _get_conn] DEBUG - Resetting dropped connection: huggingface.co
2023-10-11 12:47:19,231 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:47:19,355 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:47:19,436 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:47:19,509 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:47:19,594 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:47:19,596 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'
2023-10-11 12:47:19,603 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-11 12:47:19,604 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-11 12:47:19,605 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-11 12:47:19,607 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-11 12:47:19,608 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-11 12:47:19,610 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-11 12:47:19,612 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-11 12:47:19,614 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-11 12:47:19,615 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-11 12:47:19,617 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-11 12:47:19,619 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-11 12:47:19,621 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-11 12:47:19,623 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-11 12:47:19,624 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-11 12:47:19,626 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:47:19,627 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:47:19,628 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-11 12:47:19,630 [model.py:148 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-11 12:47:19,633 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-11 12:47:19,661 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-11 12:47:19,662 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-11 12:47:19,663 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-11 12:47:19,664 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-11 12:47:19,665 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-11 12:47:19,666 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-11 12:47:19,667 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-11 12:47:19,668 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-11 12:47:19,669 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-11 12:47:19,669 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-11 12:47:19,670 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-11 12:47:19,672 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-11 12:47:19,673 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-11 12:47:19,674 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-11 12:47:19,675 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-11 12:47:19,676 [520681597.py:42 in to_test_forward] DEBUG - lm_head to test forward
2023-10-11 12:47:19,719 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:47:19,910 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:19,912 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:19,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:19,915 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:19,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:19,927 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:47:19,931 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:19,940 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:47:19,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:19,953 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:47:19,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:19,962 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:47:19,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:19,972 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:47:19,974 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:19,981 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:47:19,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:19,993 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:47:19,996 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:20,003 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:47:20,005 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:20,024 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:47:20,027 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:20,037 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:47:20,040 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:20,047 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:47:20,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:20,056 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:47:20,058 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:20,062 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:47:20,063 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:20,074 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:47:20,081 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-11 12:47:20,082 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:47:20,096 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-11 12:47:20,098 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-11 12:47:20,099 [520681597.py:22 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-11 12:47:20,100 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-11 12:47:20,100 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-11 12:47:20,102 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-11 12:47:20,103 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-11 12:47:20,103 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-11 12:47:20,105 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-11 12:47:20,106 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-11 12:47:20,106 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-11 12:47:20,107 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-11 12:47:20,108 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-11 12:47:20,108 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-11 12:47:20,109 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-11 12:47:20,110 [520681597.py:22 in reset_forward] DEBUG - lm_head from test to old.
2023-10-11 12:47:20,112 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-11 12:47:20,113 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-11 12:47:20,114 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-11 12:47:20,115 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-11 12:47:20,115 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-11 12:47:20,116 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-11 12:47:20,117 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-11 12:47:20,118 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-11 12:47:20,119 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-11 12:47:20,120 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-11 12:47:20,121 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-11 12:47:20,122 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-11 12:47:20,123 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-11 12:47:20,124 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-11 12:47:20,125 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-11 12:47:20,126 [2192271695.py:48 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-11 12:47:20,168 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:47:20,345 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:20,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:20,348 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])",)
2023-10-11 12:47:20,349 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:20,350 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:47:20,352 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:47:20,353 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:47:20,355 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:47:20,357 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:47:20,358 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:20,359 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:20,361 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:20,365 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])", "<class 'int'>: 0")
2023-10-11 12:47:20,366 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:20,367 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:47:20,369 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:47:20,370 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:47:20,372 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:47:20,373 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:47:20,374 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:20,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:20,383 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:20,388 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:20,389 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:20,389 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:47:20,401 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:47:20,406 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:47:20,415 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:47:20,424 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:20,425 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:47:20,428 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:20,432 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:20,437 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:20,438 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:20,438 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:47:20,451 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:47:20,459 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:47:20,464 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:47:20,470 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:20,471 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:47:20,473 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:20,478 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:20,483 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:20,483 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:20,484 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:47:20,496 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:47:20,507 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:47:20,515 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:47:20,522 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:20,523 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:47:20,525 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:20,530 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:20,534 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:20,535 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:20,536 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:47:20,545 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:47:20,551 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:47:20,560 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:47:20,567 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:20,568 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:47:20,571 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:20,576 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:20,580 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:20,581 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:20,582 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:47:20,592 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:47:20,599 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:47:20,605 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:47:20,615 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:20,616 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:47:20,619 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:20,624 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:20,629 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:20,630 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:20,631 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:47:20,643 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:47:20,650 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:47:20,657 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:47:20,663 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:20,663 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:47:20,666 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:20,670 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:20,675 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:20,676 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:20,677 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:47:20,686 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:47:20,692 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:47:20,699 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:47:20,719 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:20,720 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:47:20,722 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:20,727 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:20,732 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:20,733 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:20,734 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:47:20,759 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:47:20,772 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:47:20,780 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:47:20,786 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:20,787 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:47:20,789 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:20,794 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:20,799 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:20,800 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:20,800 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:47:20,807 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:47:20,815 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:47:20,825 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:47:20,835 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:20,836 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:47:20,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:20,843 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:20,848 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:20,848 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:20,850 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:47:20,856 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:47:20,864 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:47:20,870 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:47:20,880 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:20,881 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:47:20,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:20,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:20,893 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:20,894 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:20,895 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:47:20,902 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:47:20,911 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:47:20,919 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:47:20,943 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:20,944 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:47:20,947 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:20,952 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:20,954 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:20,955 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:20,956 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:47:20,968 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:47:20,976 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:47:20,985 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:47:20,992 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:20,993 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:47:20,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:20,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:20,998 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:20,999 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:21,000 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:47:21,003 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:47:21,004 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:47:21,006 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:47:21,008 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:47:21,009 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:47:21,011 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:21,012 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:21,013 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:21,014 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:21,015 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:47:21,036 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:47:21,053 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:47:21,073 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:47:21,090 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 25136])
2023-10-11 12:47:21,091 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:47:50,246 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:47:50,370 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:47:50,456 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:47:50,623 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:47:50,709 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:47:50,711 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'
2023-10-11 12:47:50,718 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-11 12:47:50,719 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-11 12:47:50,720 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-11 12:47:50,722 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-11 12:47:50,724 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-11 12:47:50,725 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-11 12:47:50,727 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-11 12:47:50,728 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-11 12:47:50,730 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-11 12:47:50,732 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-11 12:47:50,733 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-11 12:47:50,735 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-11 12:47:50,737 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-11 12:47:50,738 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-11 12:47:50,740 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:47:50,741 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:47:50,742 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-11 12:47:50,745 [model.py:148 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-11 12:47:50,749 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-11 12:47:50,790 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-11 12:47:50,791 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-11 12:47:50,792 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-11 12:47:50,793 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-11 12:47:50,794 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-11 12:47:50,795 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-11 12:47:50,796 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-11 12:47:50,797 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-11 12:47:50,797 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-11 12:47:50,798 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-11 12:47:50,800 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-11 12:47:50,801 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-11 12:47:50,801 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-11 12:47:50,802 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-11 12:47:50,803 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-11 12:47:50,804 [520681597.py:42 in to_test_forward] DEBUG - lm_head to test forward
2023-10-11 12:47:50,844 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:47:50,996 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:50,998 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:51,000 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:51,002 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:51,004 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:51,014 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:47:51,017 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:51,028 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:47:51,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:51,037 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:47:51,040 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:51,046 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:47:51,048 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:51,060 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:47:51,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:51,069 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:47:51,071 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:51,081 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:47:51,084 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:51,091 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:47:51,093 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:51,100 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:47:51,103 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:51,109 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:47:51,112 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:51,119 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:47:51,121 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:51,128 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:47:51,131 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:51,132 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:47:51,134 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:51,143 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:47:51,148 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-11 12:47:51,149 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:47:51,159 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-11 12:47:51,160 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-11 12:47:51,161 [520681597.py:22 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-11 12:47:51,162 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-11 12:47:51,163 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-11 12:47:51,164 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-11 12:47:51,165 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-11 12:47:51,166 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-11 12:47:51,167 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-11 12:47:51,168 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-11 12:47:51,169 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-11 12:47:51,170 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-11 12:47:51,171 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-11 12:47:51,171 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-11 12:47:51,173 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-11 12:47:51,174 [520681597.py:22 in reset_forward] DEBUG - lm_head from test to old.
2023-10-11 12:47:51,175 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-11 12:47:51,176 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-11 12:47:51,177 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-11 12:47:51,178 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-11 12:47:51,179 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-11 12:47:51,179 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-11 12:47:51,180 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-11 12:47:51,181 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-11 12:47:51,182 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-11 12:47:51,183 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-11 12:47:51,184 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-11 12:47:51,185 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-11 12:47:51,186 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-11 12:47:51,187 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-11 12:47:51,187 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-11 12:47:51,188 [2192271695.py:48 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-11 12:47:51,227 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:47:51,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:51,381 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:51,382 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])",)
2023-10-11 12:47:51,383 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:51,385 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:47:51,387 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:47:51,389 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:47:51,390 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:47:51,392 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:47:51,393 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:51,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:51,396 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:51,401 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])", "<class 'int'>: 0")
2023-10-11 12:47:51,401 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:51,402 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:47:51,404 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:47:51,406 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:47:51,408 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:47:51,410 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:47:51,410 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:51,415 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:51,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:51,425 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:51,426 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:51,427 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:47:51,434 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:47:51,440 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:47:51,447 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:47:51,458 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:51,459 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:47:51,462 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:51,466 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:51,471 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:51,472 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:51,473 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:47:51,482 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:47:51,489 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:47:51,499 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:47:51,507 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:51,508 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:47:51,511 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:51,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:51,521 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:51,523 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:51,524 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:47:51,548 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:47:51,586 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:47:51,623 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:47:51,633 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:51,634 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:47:51,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:51,641 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:51,646 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:51,647 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:51,648 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:47:51,660 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:47:51,669 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:47:51,676 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:47:51,684 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:51,685 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:47:51,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:51,692 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:51,697 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:51,698 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:51,699 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:47:51,719 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:47:51,728 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:47:51,737 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:47:51,745 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:51,746 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:47:51,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:51,753 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:51,758 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:51,759 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:51,760 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:47:51,773 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:47:51,779 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:47:51,792 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:47:51,798 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:51,800 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:47:51,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:51,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:51,813 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:51,814 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:51,815 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:47:51,825 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:47:51,835 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:47:51,846 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:47:51,855 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:51,855 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:47:51,858 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:51,862 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:51,867 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:51,868 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:51,869 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:47:51,883 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:47:51,906 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:47:51,912 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:47:51,922 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:51,923 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:47:51,925 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:51,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:51,935 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:51,937 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:51,937 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:47:51,948 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:47:51,955 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:47:51,962 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:47:51,970 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:51,971 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:47:51,974 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:51,978 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:51,983 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:51,984 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:51,985 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:47:51,994 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:47:52,003 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:47:52,008 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:47:52,016 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:52,017 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:47:52,020 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:52,025 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:52,030 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:52,031 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:52,031 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:47:52,038 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:47:52,045 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:47:52,054 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:47:52,063 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:52,064 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:47:52,066 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:52,071 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:52,072 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:52,073 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:52,074 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:47:52,089 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:47:52,095 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:47:52,101 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:47:52,106 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:47:52,107 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:47:52,110 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:52,111 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:52,112 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:52,113 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:52,114 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:47:52,117 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:47:52,122 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:47:52,127 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:47:52,131 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:47:52,132 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:47:52,134 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:52,135 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:52,136 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:47:52,137 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:52,138 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:47:52,157 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:47:52,180 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:47:52,197 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:47:52,212 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 25136])
2023-10-11 12:47:52,214 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:47:52,226 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:52,229 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:52,230 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:47:52,231 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:52,232 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:47:52,234 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:47:52,235 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:47:52,237 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:47:52,238 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:52,239 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:52,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:52,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:52,247 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 10])", "<class 'int'>: 9")
2023-10-11 12:47:52,248 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:52,249 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:47:52,251 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:47:52,252 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:47:52,254 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:47:52,255 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:52,256 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:52,261 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:52,265 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:52,270 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:52,271 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:52,272 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:47:52,279 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:47:52,284 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:47:52,294 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:47:52,300 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:47:52,301 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:47:52,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:52,308 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:52,313 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:52,314 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:52,315 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:47:52,322 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:47:52,327 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:47:52,332 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:47:52,343 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:47:52,344 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:47:52,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:52,352 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:52,356 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:52,357 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:52,358 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:47:52,365 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:47:52,370 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:47:52,381 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:47:52,386 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:47:52,387 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:47:52,390 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:52,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:52,399 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:52,400 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:52,401 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:47:52,407 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:47:52,413 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:47:52,418 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:47:52,426 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:47:52,427 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:47:52,429 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:52,434 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:52,439 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:52,440 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:52,441 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:47:52,449 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:47:52,457 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:47:52,464 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:47:52,472 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:47:52,472 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:47:52,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:52,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:52,490 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:52,492 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:52,494 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:47:52,532 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:47:52,540 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:47:52,546 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:47:52,554 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:47:52,555 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:47:52,558 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:52,565 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:52,573 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:52,575 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:52,576 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:47:52,592 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:47:52,651 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:47:52,674 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:47:52,680 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:47:52,680 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:47:52,683 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:52,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:52,691 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:52,692 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:52,693 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:47:52,699 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:47:52,707 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:47:52,717 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:47:52,723 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:47:52,724 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:47:52,726 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:52,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:52,736 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:52,737 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:52,738 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:47:52,748 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:47:52,755 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:47:52,761 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:47:52,766 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:47:52,767 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:47:52,770 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:52,775 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:52,780 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:52,780 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:52,781 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:47:52,789 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:47:52,794 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:47:52,803 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:47:52,811 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:47:52,812 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:47:52,814 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:52,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:52,824 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:52,825 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:52,826 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:47:52,865 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:47:52,898 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:47:52,909 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:47:52,918 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:47:52,920 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:47:52,923 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:52,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:52,932 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:52,933 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:52,934 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:47:52,942 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:47:52,957 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:47:52,966 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:47:52,972 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:47:52,972 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:47:52,975 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:52,977 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:52,978 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:52,979 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:52,980 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:47:52,983 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:47:52,986 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:47:52,988 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:47:52,990 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:52,991 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:47:52,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:52,994 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:52,995 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:52,996 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:52,997 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:47:53,008 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:47:53,016 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:47:53,027 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:47:53,040 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:47:53,041 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:47:53,055 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:53,056 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:53,058 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:47:53,059 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:53,060 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:47:53,062 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:47:53,064 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:47:53,065 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:47:53,067 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:53,067 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:53,069 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:53,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:53,075 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 11])", "<class 'int'>: 10")
2023-10-11 12:47:53,076 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:53,077 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:47:53,078 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:47:53,080 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:47:53,082 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:47:53,083 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:53,084 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:53,088 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:53,093 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:53,098 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,099 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,100 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:47:53,106 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:47:53,112 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:47:53,117 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:47:53,124 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:47:53,126 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:47:53,128 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:53,132 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:53,137 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,138 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,139 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:47:53,148 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:47:53,155 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:47:53,159 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:47:53,165 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:47:53,165 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:47:53,168 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:53,172 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:53,176 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,177 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,178 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:47:53,187 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:47:53,195 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:47:53,203 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:47:53,213 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:47:53,214 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:47:53,217 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:53,221 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:53,225 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,226 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,227 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:47:53,237 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:47:53,242 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:47:53,247 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:47:53,258 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:47:53,258 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:47:53,261 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:53,266 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:53,271 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,272 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,273 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:47:53,282 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:47:53,290 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:47:53,298 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:47:53,314 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:47:53,315 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:47:53,317 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:53,321 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:53,326 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,327 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,328 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:47:53,344 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:47:53,354 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:47:53,360 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:47:53,366 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:47:53,367 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:47:53,369 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:53,374 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:53,382 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,382 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,383 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:47:53,395 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:47:53,402 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:47:53,411 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:47:53,417 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:47:53,418 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:47:53,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:53,425 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:53,430 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,431 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,432 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:47:53,456 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:47:53,462 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:47:53,467 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:47:53,478 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:47:53,479 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:47:53,482 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:53,486 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:53,491 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,492 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,493 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:47:53,503 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:47:53,510 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:47:53,515 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:47:53,520 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:47:53,520 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:47:53,523 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:53,528 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:53,533 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,534 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,535 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:47:53,544 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:47:53,551 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:47:53,556 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:47:53,562 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:47:53,563 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:47:53,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:53,571 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:53,576 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,577 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,578 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:47:53,645 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:47:53,663 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:47:53,668 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:47:53,673 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:47:53,674 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:47:53,677 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:53,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:53,684 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,685 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,685 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:47:53,696 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:47:53,707 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:47:53,715 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:47:53,720 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:47:53,721 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:47:53,723 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:53,725 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:53,726 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,727 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:53,728 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:47:53,730 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:47:53,732 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:47:53,734 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:47:53,736 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:53,736 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:47:53,739 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:53,740 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:53,741 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,742 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:53,743 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:47:53,758 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:47:53,771 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:47:53,783 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:47:53,792 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:47:53,793 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:47:53,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:53,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:53,804 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:47:53,805 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:53,806 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:47:53,807 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:47:53,809 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:47:53,811 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:47:53,812 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:53,813 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:53,815 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:53,816 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:53,821 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 12])", "<class 'int'>: 11")
2023-10-11 12:47:53,822 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:53,822 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:47:53,824 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:47:53,825 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:47:53,827 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:47:53,828 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:53,829 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:53,834 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:53,839 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:53,844 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,845 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,845 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:47:53,852 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:47:53,861 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:47:53,872 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:47:53,877 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:47:53,878 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:47:53,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:53,885 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:53,889 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,890 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,891 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:47:53,897 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:47:53,902 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:47:53,908 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:47:53,914 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:47:53,914 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:47:53,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:53,921 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:53,926 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,927 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,928 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:47:53,936 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:47:53,942 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:47:53,950 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:47:53,956 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:47:53,957 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:47:53,960 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:53,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:53,970 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:53,971 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:53,971 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:47:53,977 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:47:53,983 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:47:53,988 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:47:53,993 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:47:53,993 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:47:53,996 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:54,001 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:54,006 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,007 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,007 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:47:54,014 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:47:54,020 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:47:54,027 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:47:54,034 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:47:54,035 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:47:54,037 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:54,042 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:54,047 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,047 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,049 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:47:54,055 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:47:54,060 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:47:54,066 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:47:54,071 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:47:54,072 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:47:54,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:54,079 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:54,084 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,085 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,085 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:47:54,093 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:47:54,099 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:47:54,106 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:47:54,112 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:47:54,113 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:47:54,115 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:54,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:54,124 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,125 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,126 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:47:54,133 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:47:54,141 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:47:54,151 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:47:54,185 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:47:54,186 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:47:54,189 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:54,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:54,198 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,199 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,200 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:47:54,213 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:47:54,222 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:47:54,227 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:47:54,236 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:47:54,237 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:47:54,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:54,244 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:54,249 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,250 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,251 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:47:54,263 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:47:54,269 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:47:54,283 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:47:54,324 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:47:54,325 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:47:54,327 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:54,333 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:54,339 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,340 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,341 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:47:54,361 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:47:54,424 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:47:54,457 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:47:54,467 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:47:54,468 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:47:54,471 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:54,476 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:54,478 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,479 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,480 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:47:54,494 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:47:54,500 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:47:54,505 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:47:54,510 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:47:54,512 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:47:54,514 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:54,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:54,517 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,518 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:54,518 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:47:54,520 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:47:54,522 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:47:54,524 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:47:54,526 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:54,527 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:47:54,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:54,530 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:54,531 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,532 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:54,533 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:47:54,546 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:47:54,557 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:47:54,566 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:47:54,574 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:47:54,576 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:47:54,583 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:54,585 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:54,586 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:47:54,587 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:54,588 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:47:54,590 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:47:54,592 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:47:54,593 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:47:54,595 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:54,596 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:54,597 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:54,599 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:54,603 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 13])", "<class 'int'>: 12")
2023-10-11 12:47:54,604 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:54,605 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:47:54,607 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:47:54,608 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:47:54,610 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:47:54,612 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:54,613 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:54,617 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:54,621 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:54,626 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,627 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,628 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:47:54,634 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:47:54,639 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:47:54,643 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:47:54,649 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:47:54,649 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:47:54,652 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:54,657 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:54,662 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,663 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,664 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:47:54,672 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:47:54,690 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:47:54,696 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:47:54,710 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:47:54,711 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:47:54,714 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:54,719 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:54,725 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,726 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,727 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:47:54,762 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:47:54,767 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:47:54,786 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:47:54,803 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:47:54,804 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:47:54,807 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:54,812 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:54,817 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,817 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,819 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:47:54,826 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:47:54,833 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:47:54,838 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:47:54,848 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:47:54,848 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:47:54,851 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:54,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:54,861 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,862 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,863 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:47:54,871 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:47:54,879 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:47:54,884 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:47:54,898 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:47:54,899 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:47:54,901 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:54,906 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:54,911 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,912 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,913 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:47:54,920 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:47:54,934 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:47:54,948 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:47:54,954 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:47:54,954 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:47:54,957 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:54,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:54,980 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:54,981 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:54,983 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:47:54,994 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:47:54,999 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:47:55,012 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:47:55,020 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:47:55,021 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:47:55,023 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:55,028 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:55,033 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,034 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,035 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:47:55,043 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:47:55,052 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:47:55,058 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:47:55,065 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:47:55,066 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:47:55,069 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:55,073 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:55,078 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,079 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,080 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:47:55,087 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:47:55,098 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:47:55,104 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:47:55,110 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:47:55,111 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:47:55,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:55,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:55,122 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,123 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,124 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:47:55,133 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:47:55,138 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:47:55,144 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:47:55,151 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:47:55,152 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:47:55,155 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:55,160 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:55,165 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,166 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,168 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:47:55,180 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:47:55,186 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:47:55,193 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:47:55,202 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:47:55,203 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:47:55,205 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:55,209 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:55,211 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,212 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,213 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:47:55,221 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:47:55,226 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:47:55,232 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:47:55,238 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:47:55,240 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:47:55,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:55,245 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:55,246 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,247 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:55,248 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:47:55,252 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:47:55,256 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:47:55,260 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:47:55,264 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:55,265 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:47:55,267 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:55,268 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:55,269 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,270 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:55,271 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:47:55,286 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:47:55,295 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:47:55,306 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:47:55,318 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:47:55,319 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:47:55,326 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:55,328 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:55,329 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:47:55,330 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:55,331 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:47:55,333 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:47:55,335 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:47:55,337 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:47:55,338 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:55,339 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:55,341 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:55,342 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:55,347 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 14])", "<class 'int'>: 13")
2023-10-11 12:47:55,347 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:55,348 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:47:55,350 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:47:55,352 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:47:55,353 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:47:55,355 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:55,355 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:55,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:55,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:55,370 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,370 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,371 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:47:55,378 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:47:55,386 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:47:55,391 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:47:55,400 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:47:55,402 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:47:55,405 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:55,410 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:55,416 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,416 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,417 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:47:55,425 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:47:55,432 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:47:55,450 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:47:55,466 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:47:55,467 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:47:55,470 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:55,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:55,479 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,480 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,481 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:47:55,491 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:47:55,499 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:47:55,507 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:47:55,516 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:47:55,517 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:47:55,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:55,524 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:55,530 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,530 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,531 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:47:55,539 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:47:55,545 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:47:55,551 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:47:55,557 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:47:55,557 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:47:55,560 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:55,565 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:55,571 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,572 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,572 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:47:55,579 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:47:55,585 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:47:55,591 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:47:55,598 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:47:55,599 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:47:55,602 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:55,606 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:55,612 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,613 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,614 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:47:55,621 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:47:55,627 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:47:55,632 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:47:55,638 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:47:55,639 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:47:55,641 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:55,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:55,652 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,653 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,653 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:47:55,663 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:47:55,674 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:47:55,680 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:47:55,686 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:47:55,687 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:47:55,690 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:55,694 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:55,699 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,700 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,700 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:47:55,707 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:47:55,715 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:47:55,734 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:47:55,739 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:47:55,740 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:47:55,743 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:55,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:55,754 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,755 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,755 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:47:55,762 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:47:55,768 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:47:55,774 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:47:55,784 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:47:55,785 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:47:55,788 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:55,792 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:55,797 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,798 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,798 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:47:55,806 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:47:55,811 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:47:55,818 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:47:55,825 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:47:55,826 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:47:55,829 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:55,833 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:55,838 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,839 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,840 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:47:55,846 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:47:55,852 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:47:55,859 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:47:55,870 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:47:55,871 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:47:55,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:55,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:55,880 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,881 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:55,882 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:47:55,899 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:47:55,906 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:47:55,919 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:47:55,924 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:47:55,926 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:47:55,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:55,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:55,931 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,932 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:55,933 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:47:55,935 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:47:55,937 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:47:55,939 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:47:55,942 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:55,943 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:47:55,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:55,946 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:55,947 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:55,948 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:55,948 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:47:55,960 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:47:55,976 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:47:55,987 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:47:55,999 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:47:56,000 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:47:56,007 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:56,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:56,010 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:47:56,011 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:56,011 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:47:56,013 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:47:56,015 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:47:56,016 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:47:56,018 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:56,018 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:56,020 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:56,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:56,028 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 15])", "<class 'int'>: 14")
2023-10-11 12:47:56,029 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:56,029 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:47:56,032 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:47:56,033 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:47:56,035 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:47:56,036 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:56,037 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:56,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:56,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:56,050 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,051 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,052 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:47:56,059 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:47:56,067 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:47:56,077 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:47:56,083 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:47:56,084 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:47:56,086 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:56,091 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:56,096 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,097 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,098 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:47:56,105 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:47:56,113 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:47:56,122 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:47:56,133 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:47:56,134 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:47:56,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:56,141 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:56,147 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,148 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,149 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:47:56,209 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:47:56,235 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:47:56,248 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:47:56,254 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:47:56,255 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:47:56,258 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:56,262 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:56,268 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,269 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,270 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:47:56,277 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:47:56,282 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:47:56,291 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:47:56,300 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:47:56,301 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:47:56,304 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:56,309 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:56,314 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,314 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,315 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:47:56,321 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:47:56,333 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:47:56,343 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:47:56,348 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:47:56,349 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:47:56,351 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:56,356 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:56,360 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,361 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,362 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:47:56,369 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:47:56,377 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:47:56,382 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:47:56,387 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:47:56,388 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:47:56,390 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:56,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:56,398 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,399 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,400 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:47:56,407 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:47:56,413 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:47:56,418 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:47:56,423 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:47:56,423 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:47:56,426 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:56,430 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:56,435 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,436 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,437 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:47:56,443 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:47:56,449 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:47:56,454 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:47:56,460 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:47:56,461 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:47:56,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:56,468 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:56,473 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,474 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,475 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:47:56,482 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:47:56,495 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:47:56,502 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:47:56,508 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:47:56,509 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:47:56,511 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:56,516 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:56,521 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,522 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,523 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:47:56,530 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:47:56,536 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:47:56,542 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:47:56,553 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:47:56,554 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:47:56,556 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:56,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:56,567 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,568 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,569 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:47:56,575 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:47:56,582 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:47:56,591 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:47:56,600 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:47:56,601 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:47:56,603 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:56,608 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:56,609 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,610 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,611 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:47:56,618 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:47:56,689 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:47:56,713 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:47:56,729 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:47:56,730 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:47:56,732 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:56,734 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:56,735 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,736 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:56,737 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:47:56,740 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:47:56,741 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:47:56,743 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:47:56,745 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:56,746 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:47:56,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:56,749 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:56,750 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,751 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:56,752 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:47:56,765 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:47:56,774 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:47:56,787 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:47:56,797 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:47:56,798 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:47:56,805 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:56,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:56,808 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:47:56,809 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:56,809 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:47:56,811 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:47:56,813 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:47:56,815 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:47:56,817 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:56,818 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:56,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:56,820 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:56,825 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 16])", "<class 'int'>: 15")
2023-10-11 12:47:56,826 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:56,827 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:47:56,829 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:47:56,830 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:47:56,832 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:47:56,833 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:56,834 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:56,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:56,843 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:56,847 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,848 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,849 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:47:56,856 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:47:56,861 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:47:56,865 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:47:56,870 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:47:56,871 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:47:56,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:56,877 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:56,882 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,883 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,884 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:47:56,890 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:47:56,895 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:47:56,900 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:47:56,904 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:47:56,905 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:47:56,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:56,912 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:56,916 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,917 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,918 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:47:56,924 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:47:56,932 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:47:56,942 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:47:56,947 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:47:56,948 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:47:56,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:56,956 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:56,961 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:56,962 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:56,962 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:47:56,971 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:47:56,980 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:47:56,986 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:47:56,994 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:47:56,995 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:47:56,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:57,002 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:57,007 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,008 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,009 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:47:57,016 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:47:57,027 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:47:57,033 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:47:57,038 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:47:57,038 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:47:57,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:57,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:57,050 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,051 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,052 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:47:57,058 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:47:57,063 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:47:57,071 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:47:57,155 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:47:57,156 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:47:57,159 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:57,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:57,169 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,170 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,171 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:47:57,177 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:47:57,183 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:47:57,188 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:47:57,193 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:47:57,194 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:47:57,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:57,200 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:57,205 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,206 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,207 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:47:57,213 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:47:57,218 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:47:57,238 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:47:57,243 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:47:57,245 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:47:57,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:57,251 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:57,256 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,257 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,258 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:47:57,265 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:47:57,278 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:47:57,283 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:47:57,289 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:47:57,290 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:47:57,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:57,297 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:57,301 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,302 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,303 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:47:57,309 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:47:57,314 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:47:57,322 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:47:57,329 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:47:57,330 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:47:57,333 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:57,338 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:57,345 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,345 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,346 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:47:57,353 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:47:57,358 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:47:57,363 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:47:57,369 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:47:57,370 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:47:57,373 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:57,378 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:57,380 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,380 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,381 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:47:57,388 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:47:57,395 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:47:57,400 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:47:57,411 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:47:57,412 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:47:57,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:57,415 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:57,416 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,417 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:57,417 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:47:57,421 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:47:57,424 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:47:57,426 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:47:57,429 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:57,430 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:47:57,432 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:57,433 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:57,434 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,435 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:57,436 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:47:57,452 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:47:57,464 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:47:57,480 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:47:57,490 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:47:57,492 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:47:57,499 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:57,501 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:57,502 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:47:57,503 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:57,503 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:47:57,505 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:47:57,507 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:47:57,508 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:47:57,510 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:57,511 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:57,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:57,514 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:57,519 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 17])", "<class 'int'>: 16")
2023-10-11 12:47:57,520 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:57,521 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:47:57,523 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:47:57,525 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:47:57,526 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:47:57,528 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:57,529 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:57,533 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:57,537 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:57,542 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,543 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,544 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:47:57,550 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:47:57,558 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:47:57,563 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:47:57,571 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:47:57,573 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:47:57,575 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:57,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:57,587 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,588 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,589 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:47:57,608 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:47:57,621 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:47:57,634 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:47:57,642 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:47:57,643 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:47:57,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:57,651 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:57,656 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,657 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,658 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:47:57,669 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:47:57,685 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:47:57,692 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:47:57,698 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:47:57,699 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:47:57,702 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:57,707 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:57,714 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,715 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,716 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:47:57,722 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:47:57,730 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:47:57,742 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:47:57,788 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:47:57,789 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:47:57,792 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:57,796 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:57,801 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,802 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,803 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:47:57,833 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:47:57,839 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:47:57,843 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:47:57,848 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:47:57,849 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:47:57,852 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:57,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:57,861 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,862 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,862 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:47:57,869 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:47:57,874 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:47:57,881 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:47:57,887 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:47:57,887 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:47:57,890 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:57,894 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:57,899 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,900 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,900 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:47:57,910 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:47:57,921 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:47:57,930 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:47:57,941 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:47:57,941 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:47:57,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:57,948 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:57,953 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:57,953 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:57,954 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:47:57,971 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:47:57,976 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:47:57,985 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:47:57,990 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:47:57,991 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:47:57,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:57,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:58,003 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,004 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,005 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:47:58,011 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:47:58,016 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:47:58,021 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:47:58,026 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:47:58,027 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:47:58,029 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:58,034 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:58,038 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,039 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,040 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:47:58,046 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:47:58,051 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:47:58,056 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:47:58,063 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:47:58,064 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:47:58,066 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:58,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:58,075 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,076 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,077 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:47:58,086 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:47:58,091 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:47:58,097 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:47:58,104 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:47:58,105 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:47:58,107 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:58,112 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:58,114 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,115 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,116 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:47:58,123 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:47:58,149 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:47:58,154 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:47:58,159 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:47:58,160 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:47:58,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:58,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:58,165 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,165 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:58,166 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:47:58,168 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:47:58,170 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:47:58,172 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:47:58,178 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:58,179 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:47:58,181 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:58,182 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:58,183 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,184 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:58,185 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:47:58,195 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:47:58,207 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:47:58,220 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:47:58,231 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:47:58,232 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:47:58,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:58,241 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:58,242 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:47:58,243 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:58,244 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:47:58,246 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:47:58,248 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:47:58,249 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:47:58,251 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:58,251 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:58,253 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:58,254 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:58,259 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 18])", "<class 'int'>: 17")
2023-10-11 12:47:58,260 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:58,261 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:47:58,263 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:47:58,264 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:47:58,266 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:47:58,268 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:58,268 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:58,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:58,277 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:58,282 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,283 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,284 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:47:58,291 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:47:58,297 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:47:58,301 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:47:58,307 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:47:58,308 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:47:58,310 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:58,314 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:58,319 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,320 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,321 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:47:58,327 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:47:58,333 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:47:58,338 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:47:58,343 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:47:58,344 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:47:58,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:58,352 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:58,356 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,357 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,358 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:47:58,372 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:47:58,377 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:47:58,383 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:47:58,453 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:47:58,455 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:47:58,457 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:58,462 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:58,468 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,469 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,470 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:47:58,484 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:47:58,491 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:47:58,499 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:47:58,505 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:47:58,506 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:47:58,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:58,513 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:58,518 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,519 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,520 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:47:58,527 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:47:58,532 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:47:58,537 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:47:58,542 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:47:58,543 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:47:58,546 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:58,550 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:58,555 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,556 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,556 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:47:58,563 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:47:58,568 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:47:58,578 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:47:58,585 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:47:58,586 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:47:58,588 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:58,593 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:58,598 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,599 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,600 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:47:58,607 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:47:58,612 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:47:58,619 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:47:58,626 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:47:58,626 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:47:58,629 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:58,633 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:58,638 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,639 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,639 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:47:58,646 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:47:58,651 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:47:58,656 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:47:58,667 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:47:58,667 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:47:58,670 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:58,675 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:58,680 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,681 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,682 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:47:58,694 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:47:58,701 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:47:58,706 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:47:58,711 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:47:58,712 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:47:58,715 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:58,720 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:58,725 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,726 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,727 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:47:58,804 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:47:58,820 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:47:58,825 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:47:58,835 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:47:58,836 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:47:58,839 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:58,844 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:58,849 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,850 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,850 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:47:58,860 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:47:58,866 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:47:58,883 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:47:58,894 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:47:58,895 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:47:58,898 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:58,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:58,904 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,905 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:58,907 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:47:58,922 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:47:58,928 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:47:58,934 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:47:58,939 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:47:58,940 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:47:58,943 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:58,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:58,945 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,946 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:58,947 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:47:58,948 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:47:58,950 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:47:58,952 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:47:58,954 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:58,955 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:47:58,957 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:58,958 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:58,959 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:58,960 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:58,961 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:47:58,974 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:47:58,985 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:47:58,994 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:47:59,003 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:47:59,004 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:47:59,012 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:59,014 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:59,015 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:47:59,016 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:59,017 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:47:59,019 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:47:59,021 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:47:59,022 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:47:59,025 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:59,026 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:59,031 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:59,033 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:59,038 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 19])", "<class 'int'>: 18")
2023-10-11 12:47:59,039 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:59,041 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:47:59,042 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:47:59,044 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:47:59,046 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:47:59,047 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:59,048 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:59,053 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:59,057 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:59,062 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,063 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:59,064 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:47:59,072 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:47:59,081 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:47:59,086 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:47:59,091 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:47:59,093 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:47:59,096 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:59,103 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:59,111 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,112 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:59,113 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:47:59,121 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:47:59,127 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:47:59,133 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:47:59,140 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:47:59,140 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:47:59,143 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:47:59,149 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:59,154 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,155 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:59,156 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:47:59,163 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:47:59,170 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:47:59,196 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:47:59,205 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:47:59,206 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:47:59,209 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:47:59,214 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:59,220 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,221 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:59,221 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:47:59,229 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:47:59,234 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:47:59,245 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:47:59,252 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:47:59,252 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:47:59,255 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:47:59,259 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:59,264 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,265 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:59,266 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:47:59,273 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:47:59,280 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:47:59,286 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:47:59,333 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:47:59,335 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:47:59,337 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:47:59,342 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:59,347 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,348 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:59,349 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:47:59,391 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:47:59,406 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:47:59,411 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:47:59,416 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:47:59,417 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:47:59,419 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:47:59,424 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:59,429 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,430 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:59,431 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:47:59,437 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:47:59,473 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:47:59,496 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:47:59,502 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:47:59,503 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:47:59,506 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:47:59,510 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:59,515 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,516 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:59,517 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:47:59,523 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:47:59,530 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:47:59,534 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:47:59,540 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:47:59,541 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:47:59,543 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:47:59,547 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:59,553 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,553 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:59,555 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:47:59,562 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:47:59,568 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:47:59,573 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:47:59,578 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:47:59,579 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:47:59,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:47:59,614 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:59,621 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,622 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:59,623 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:47:59,634 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:47:59,640 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:47:59,664 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:47:59,670 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:47:59,671 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:47:59,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:47:59,679 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:59,684 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,685 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:59,685 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:47:59,715 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:47:59,721 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:47:59,729 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:47:59,744 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:47:59,746 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:47:59,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:47:59,753 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:59,755 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,756 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:59,757 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:47:59,764 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:47:59,770 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:47:59,775 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:47:59,781 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:47:59,782 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:47:59,784 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:47:59,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:59,787 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,788 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:59,789 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:47:59,793 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:47:59,796 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:47:59,799 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:47:59,801 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:59,802 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:47:59,803 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:47:59,804 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:59,806 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,806 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:59,807 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:47:59,818 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:47:59,830 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:47:59,840 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:47:59,851 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:47:59,853 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:47:59,862 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:47:59,863 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:59,864 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:47:59,865 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:59,866 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:47:59,868 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:47:59,870 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:47:59,871 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:47:59,873 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:59,874 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:47:59,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:47:59,877 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:59,882 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 20])", "<class 'int'>: 19")
2023-10-11 12:47:59,883 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:47:59,886 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:47:59,888 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:47:59,889 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:47:59,891 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:47:59,893 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:47:59,893 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:47:59,897 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:47:59,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:47:59,910 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:47:59,911 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:47:59,912 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:47:59,969 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:47:59,986 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:00,000 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:00,011 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:48:00,012 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:00,014 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:00,019 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:00,023 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,024 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,025 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:00,035 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:00,051 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:00,059 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:00,066 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:48:00,067 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:00,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:00,075 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:00,080 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,081 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,082 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:00,092 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:00,099 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:00,103 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:00,108 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:48:00,109 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:00,112 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:00,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:00,121 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,122 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,123 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:00,132 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:00,138 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:00,144 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:00,151 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:48:00,152 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:00,155 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:00,159 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:00,164 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,165 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,165 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:00,174 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:00,185 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:00,190 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:00,203 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:48:00,204 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:00,206 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:00,211 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:00,216 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,217 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,218 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:00,225 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:00,232 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:00,238 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:00,243 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:48:00,244 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:00,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:00,251 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:00,256 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,257 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,258 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:00,264 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:00,269 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:00,275 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:00,280 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:48:00,281 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:00,284 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:00,289 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:00,294 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,294 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,296 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:00,302 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:00,310 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:00,319 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:00,324 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:48:00,325 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:00,327 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:00,332 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:00,336 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,337 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,339 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:00,346 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:00,352 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:00,357 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:00,363 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:48:00,364 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:00,367 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:00,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:00,375 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,376 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,377 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:00,383 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:00,452 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:00,478 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:00,484 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:48:00,485 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:00,487 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:00,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:00,497 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,498 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,499 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:00,505 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:00,522 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:00,530 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:00,540 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:48:00,541 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:00,543 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:00,547 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:00,549 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,550 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,551 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:00,559 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:00,564 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:00,573 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:00,579 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:48:00,580 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:00,583 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:00,584 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:00,586 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,586 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:00,587 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:00,589 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:00,591 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:00,595 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:00,597 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:00,597 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:00,599 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:00,600 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:00,602 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,602 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:00,603 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:00,614 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:00,624 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:00,635 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:00,650 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:00,651 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:00,658 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:00,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:00,661 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:00,662 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:00,663 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:00,664 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:00,666 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:00,667 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:00,668 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:00,669 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:00,670 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:00,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:00,676 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 21])", "<class 'int'>: 20")
2023-10-11 12:48:00,677 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:00,678 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:00,679 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:00,681 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:00,683 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:00,684 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:00,685 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:00,689 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:00,693 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:00,698 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,699 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,700 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:00,707 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:00,713 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:00,718 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:00,741 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:48:00,742 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:00,744 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:00,749 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:00,754 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,755 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,756 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:00,762 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:00,776 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:00,781 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:00,787 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:48:00,787 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:00,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:00,795 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:00,799 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,800 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,801 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:00,809 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:00,814 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:00,821 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:00,829 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:48:00,830 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:00,833 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:00,837 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:00,841 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,842 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,842 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:00,849 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:00,854 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:00,863 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:00,869 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:48:00,870 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:00,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:00,877 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:00,882 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,883 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,884 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:00,896 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:00,907 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:00,912 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:00,917 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:48:00,918 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:00,920 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:00,925 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:00,930 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,931 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,931 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:00,938 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:00,943 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:00,949 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:00,954 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:48:00,955 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:00,957 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:00,961 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:00,966 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:00,967 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:00,968 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:00,974 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:00,989 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:00,995 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:01,006 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:48:01,007 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:01,009 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:01,014 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:01,018 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,019 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,020 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:01,028 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:01,036 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:01,041 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:01,048 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:48:01,049 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:01,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:01,056 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:01,061 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,062 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,063 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:01,069 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:01,075 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:01,080 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:01,086 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:48:01,086 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:01,089 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:01,094 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:01,101 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,102 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,103 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:01,111 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:01,118 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:01,123 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:01,130 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:48:01,131 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:01,133 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:01,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:01,142 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,143 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,144 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:01,151 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:01,158 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:01,164 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:01,170 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:48:01,171 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:01,173 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:01,184 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:01,185 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,187 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,194 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:01,243 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:01,251 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:01,292 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:01,302 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:48:01,303 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:01,305 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:01,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:01,308 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,309 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:01,310 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:01,313 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:01,315 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:01,317 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:01,318 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:01,319 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:01,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:01,322 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:01,323 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,324 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:01,324 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:01,336 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:01,345 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:01,356 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:01,367 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:01,368 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:01,376 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:01,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:01,378 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:01,379 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:01,380 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:01,381 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:01,383 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:01,384 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:01,386 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:01,387 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:01,389 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:01,390 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:01,394 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 22])", "<class 'int'>: 21")
2023-10-11 12:48:01,395 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:01,396 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:01,398 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:01,399 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:01,401 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:01,403 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:01,404 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:01,408 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:01,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:01,416 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,417 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,418 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:01,427 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:01,433 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:01,443 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:01,448 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:48:01,449 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:01,452 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:01,456 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:01,461 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,462 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,463 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:01,471 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:01,478 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:01,485 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:01,491 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:48:01,491 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:01,494 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:01,498 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:01,503 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,504 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,504 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:01,511 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:01,527 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:01,542 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:01,610 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:48:01,612 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:01,615 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:01,620 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:01,625 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,626 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,627 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:01,637 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:01,642 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:01,649 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:01,655 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:48:01,656 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:01,658 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:01,662 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:01,667 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,668 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,669 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:01,682 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:01,690 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:01,696 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:01,702 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:48:01,703 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:01,705 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:01,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:01,714 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,715 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,716 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:01,722 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:01,730 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:01,735 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:01,742 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:48:01,743 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:01,745 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:01,750 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:01,755 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,756 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,756 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:01,764 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:01,769 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:01,776 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:01,793 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:48:01,794 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:01,796 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:01,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:01,805 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,806 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,806 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:01,813 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:01,818 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:01,823 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:01,829 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:48:01,829 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:01,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:01,836 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:01,841 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,842 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,843 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:01,849 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:01,855 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:01,861 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:01,866 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:48:01,867 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:01,869 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:01,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:01,878 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,879 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,880 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:01,890 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:01,897 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:01,903 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:01,908 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:48:01,909 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:01,911 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:01,915 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:01,920 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,921 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,923 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:01,929 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:01,935 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:01,940 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:01,947 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:48:01,947 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:01,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:01,954 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:01,956 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,957 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:01,958 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:01,964 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:01,970 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:01,975 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:01,982 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:48:01,983 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:01,985 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:01,987 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:01,988 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:01,989 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:01,990 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:01,993 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:01,995 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:01,998 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:02,000 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:02,001 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:02,002 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:02,004 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:02,005 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,006 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:02,006 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:02,019 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:02,029 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:02,045 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:02,059 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:02,060 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:02,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:02,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:02,078 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:02,079 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:02,080 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:02,081 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:02,083 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:02,084 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:02,085 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:02,086 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:02,087 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:02,089 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:02,093 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 23])", "<class 'int'>: 22")
2023-10-11 12:48:02,094 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:02,095 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:02,097 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:02,098 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:02,100 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:02,102 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:02,103 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:02,107 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:02,111 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:02,116 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,117 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,118 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:02,124 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:02,130 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:02,135 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:02,140 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:48:02,141 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:02,143 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:02,147 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:02,152 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,153 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,154 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:02,161 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:02,167 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:02,179 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:02,184 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:48:02,185 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:02,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:02,192 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:02,197 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,197 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,198 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:02,206 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:02,215 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:02,220 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:02,226 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:48:02,227 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:02,229 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:02,234 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:02,238 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,239 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,240 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:02,247 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:02,256 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:02,262 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:02,267 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:48:02,268 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:02,271 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:02,275 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:02,279 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,280 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,281 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:02,290 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:02,298 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:02,307 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:02,312 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:48:02,313 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:02,316 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:02,325 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:02,331 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,332 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,333 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:02,339 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:02,397 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:02,420 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:02,441 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:48:02,442 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:02,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:02,448 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:02,453 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,454 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,455 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:02,462 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:02,470 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:02,478 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:02,484 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:48:02,485 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:02,487 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:02,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:02,496 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,497 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,498 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:02,505 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:02,510 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:02,516 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:02,521 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:48:02,522 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:02,524 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:02,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:02,534 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,535 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,536 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:02,542 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:02,550 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:02,555 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:02,562 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:48:02,562 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:02,565 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:02,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:02,574 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,575 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,578 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:02,612 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:02,620 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:02,630 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:02,636 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:48:02,637 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:02,640 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:02,645 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:02,650 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,651 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,652 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:02,658 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:02,675 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:02,681 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:02,691 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:48:02,692 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:02,695 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:02,699 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:02,701 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,701 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,702 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:02,709 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:02,719 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:02,734 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:02,743 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:48:02,744 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:02,747 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:02,749 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:02,750 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,751 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:02,753 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:02,758 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:02,761 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:02,763 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:02,771 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:02,771 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:02,773 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:02,774 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:02,775 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,776 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:02,777 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:02,790 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:02,805 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:02,817 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:02,830 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:02,831 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:02,840 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:02,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:02,842 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:02,843 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:02,844 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:02,846 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:02,847 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:02,849 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:02,851 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:02,851 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:02,853 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:02,855 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:02,860 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 24])", "<class 'int'>: 23")
2023-10-11 12:48:02,861 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:02,862 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:02,863 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:02,865 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:02,867 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:02,868 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:02,869 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:02,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:02,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:02,883 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,884 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,884 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:02,892 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:02,897 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:02,906 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:02,915 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:48:02,915 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:02,918 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:02,923 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:02,927 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,928 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,929 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:02,943 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:02,958 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:02,967 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:02,979 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:48:02,980 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:02,983 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:02,987 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:02,992 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:02,993 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:02,994 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:03,001 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:03,008 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:03,014 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:03,021 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:48:03,022 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:03,024 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:03,028 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:03,033 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,034 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,035 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:03,041 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:03,106 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:03,112 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:03,117 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:48:03,118 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:03,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:03,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:03,129 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,129 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,130 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:03,142 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:03,149 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:03,155 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:03,161 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:48:03,161 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:03,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:03,168 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:03,172 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,173 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,173 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:03,180 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:03,185 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:03,190 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:03,195 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:48:03,196 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:03,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:03,202 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:03,206 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,207 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,211 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:03,291 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:03,297 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:03,302 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:03,308 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:48:03,309 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:03,311 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:03,315 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:03,320 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,321 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,322 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:03,337 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:03,343 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:03,348 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:03,353 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:48:03,353 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:03,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:03,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:03,365 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,366 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,367 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:03,373 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:03,379 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:03,387 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:03,392 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:48:03,393 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:03,395 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:03,400 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:03,404 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,405 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,406 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:03,413 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:03,419 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:03,424 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:03,430 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:48:03,431 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:03,433 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:03,438 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:03,442 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,443 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,444 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:03,451 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:03,457 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:03,466 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:03,471 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:48:03,472 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:03,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:03,478 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:03,479 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,486 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,487 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:03,559 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:03,565 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:03,572 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:03,578 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:48:03,579 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:03,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:03,583 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:03,584 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,585 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:03,586 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:03,588 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:03,590 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:03,592 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:03,594 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:03,595 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:03,596 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:03,597 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:03,599 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,599 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:03,600 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:03,613 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:03,627 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:03,638 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:03,648 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:03,649 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:03,657 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:03,658 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:03,659 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:03,660 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:03,661 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:03,662 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:03,664 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:03,665 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:03,667 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:03,667 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:03,669 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:03,671 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:03,676 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 25])", "<class 'int'>: 24")
2023-10-11 12:48:03,676 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:03,677 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:03,679 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:03,681 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:03,684 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:03,686 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:03,687 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:03,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:03,697 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:03,702 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,703 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,704 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:03,711 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:03,718 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:03,724 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:03,730 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:48:03,731 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:03,733 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:03,738 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:03,743 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,744 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,744 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:03,751 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:03,756 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:03,764 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:03,770 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:48:03,771 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:03,773 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:03,777 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:03,782 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,783 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,784 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:03,797 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:03,803 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:03,808 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:03,815 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:48:03,816 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:03,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:03,825 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:03,831 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,831 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,832 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:03,851 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:03,863 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:03,874 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:03,883 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:48:03,884 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:03,886 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:03,891 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:03,895 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,896 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,897 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:03,904 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:03,914 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:03,919 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:03,929 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:48:03,930 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:03,932 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:03,937 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:03,941 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,942 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,943 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:03,949 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:03,954 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:03,965 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:03,978 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:48:03,980 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:03,982 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:03,987 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:03,991 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:03,992 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:03,993 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:03,999 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:04,005 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:04,010 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:04,018 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:48:04,019 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:04,021 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:04,026 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:04,031 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,032 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,033 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:04,039 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:04,046 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:04,053 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:04,059 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:48:04,060 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:04,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:04,066 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:04,071 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,072 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,073 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:04,081 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:04,086 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:04,108 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:04,115 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:48:04,115 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:04,118 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:04,122 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:04,127 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,128 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,128 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:04,137 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:04,145 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:04,152 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:04,157 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:48:04,158 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:04,161 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:04,165 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:04,170 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,170 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,171 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:04,177 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:04,183 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:04,197 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:04,203 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:48:04,204 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:04,206 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:04,211 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:04,212 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,213 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,214 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:04,220 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:04,228 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:04,234 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:04,241 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:48:04,242 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:04,244 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:04,246 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:04,247 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,248 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:04,248 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:04,252 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:04,253 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:04,255 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:04,257 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:04,258 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:04,259 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:04,260 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:04,262 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,263 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:04,263 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:04,275 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:04,288 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:04,299 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:04,311 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:04,312 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:04,319 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:04,321 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:04,322 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:04,323 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:04,323 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:04,325 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:04,327 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:04,328 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:04,329 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:04,331 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:04,332 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:04,333 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:04,338 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 26])", "<class 'int'>: 25")
2023-10-11 12:48:04,338 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:04,340 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:04,341 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:04,343 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:04,344 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:04,346 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:04,347 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:04,351 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:04,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:04,360 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,360 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,361 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:04,369 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:04,375 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:04,382 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:04,391 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:48:04,391 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:04,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:04,398 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:04,402 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,403 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,404 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:04,412 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:04,420 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:04,426 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:04,433 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:48:04,433 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:04,436 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:04,440 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:04,444 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,445 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,446 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:04,454 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:04,460 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:04,467 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:04,472 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:48:04,473 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:04,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:04,479 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:04,484 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,485 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,485 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:04,497 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:04,502 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:04,507 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:04,515 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:48:04,515 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:04,518 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:04,523 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:04,528 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,529 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,530 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:04,537 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:04,549 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:04,562 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:04,567 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:48:04,568 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:04,571 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:04,575 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:04,580 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,581 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,582 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:04,588 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:04,593 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:04,599 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:04,606 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:48:04,607 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:04,609 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:04,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:04,618 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,619 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,620 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:04,631 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:04,636 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:04,642 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:04,647 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:48:04,648 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:04,651 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:04,655 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:04,660 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,661 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,662 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:04,679 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:04,686 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:04,692 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:04,698 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:48:04,698 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:04,701 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:04,705 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:04,711 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,712 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,713 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:04,736 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:04,743 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:04,812 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:04,845 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:48:04,846 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:04,850 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:04,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:04,863 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,865 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,866 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:04,875 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:04,880 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:04,890 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:04,897 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:48:04,897 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:04,900 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:04,905 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:04,910 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,911 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,912 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:04,919 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:04,926 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:04,933 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:04,940 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:48:04,941 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:04,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:04,948 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:04,950 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,951 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:04,952 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:04,959 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:04,964 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:04,970 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:04,975 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:48:04,977 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:04,979 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:04,981 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:04,982 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,983 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:04,984 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:04,986 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:04,988 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:04,990 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:04,992 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:04,992 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:04,994 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:04,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:04,997 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:04,997 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:04,998 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:05,014 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:05,027 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:05,038 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:05,047 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:05,049 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:05,055 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:05,057 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:05,058 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:05,058 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:05,059 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:05,061 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:05,062 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:05,063 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:05,065 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:05,065 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:05,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:05,068 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:05,073 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 27])", "<class 'int'>: 26")
2023-10-11 12:48:05,074 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:05,074 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:05,076 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:05,078 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:05,080 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:05,081 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:05,082 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:05,086 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:05,090 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:05,095 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,096 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:05,097 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:05,104 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:05,110 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:05,115 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:05,120 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:48:05,121 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:05,123 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:05,127 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:05,132 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,133 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:05,134 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:05,155 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:05,166 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:05,175 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:05,189 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:48:05,190 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:05,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:05,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:05,202 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,203 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:05,204 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:05,211 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:05,217 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:05,230 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:05,238 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:48:05,239 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:05,243 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:05,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:05,253 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,254 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:05,254 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:05,262 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:05,285 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:05,298 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:05,304 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:48:05,305 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:05,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:05,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:05,318 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,319 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:05,320 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:05,327 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:05,333 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:05,346 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:05,353 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:48:05,354 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:05,356 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:05,361 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:05,366 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,366 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:05,367 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:05,377 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:05,386 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:05,395 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:05,403 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:48:05,404 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:05,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:05,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:05,416 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,417 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:05,418 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:05,426 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:05,477 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:05,531 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:05,538 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:48:05,539 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:05,541 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:05,545 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:05,549 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,550 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:05,551 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:05,559 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:05,564 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:05,570 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:05,575 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:48:05,576 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:05,578 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:05,583 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:05,588 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,589 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:05,589 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:05,597 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:05,604 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:05,610 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:05,615 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:48:05,616 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:05,619 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:05,623 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:05,628 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,629 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:05,630 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:05,636 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:05,641 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:05,647 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:05,652 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:48:05,652 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:05,655 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:05,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:05,664 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,665 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:05,665 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:05,672 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:05,679 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:05,684 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:05,690 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:48:05,691 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:05,693 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:05,697 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:05,699 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,700 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:05,701 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:05,707 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:05,713 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:05,718 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:05,723 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:48:05,724 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:05,726 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:05,728 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:05,729 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,730 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:05,731 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:05,734 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:05,736 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:05,738 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:05,740 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:05,741 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:05,742 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:05,744 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:05,745 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,745 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:05,746 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:05,762 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:05,771 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:05,784 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:05,794 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:05,796 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:05,803 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:05,805 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:05,806 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:05,807 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:05,807 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:05,809 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:05,810 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:05,812 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:05,814 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:05,815 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:05,816 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:05,817 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:05,822 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 28])", "<class 'int'>: 27")
2023-10-11 12:48:05,823 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:05,824 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:05,826 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:05,827 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:05,829 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:05,830 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:05,831 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:05,835 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:05,840 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:05,845 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,846 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:05,847 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:05,853 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:05,859 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:05,923 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:05,968 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:48:05,969 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:05,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:05,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:05,981 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:05,982 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:05,982 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:05,993 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:06,002 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:06,024 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:06,037 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:48:06,039 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:06,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:06,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:06,050 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,050 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,051 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:06,058 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:06,063 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:06,070 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:06,076 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:48:06,076 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:06,078 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:06,082 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:06,086 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,087 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,088 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:06,103 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:06,111 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:06,116 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:06,121 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:48:06,122 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:06,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:06,128 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:06,133 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,133 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,134 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:06,143 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:06,149 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:06,155 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:06,163 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:48:06,164 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:06,166 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:06,170 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:06,174 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,175 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,176 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:06,182 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:06,188 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:06,210 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:06,216 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:48:06,217 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:06,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:06,223 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:06,227 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,228 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,229 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:06,235 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:06,241 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:06,245 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:06,251 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:48:06,251 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:06,254 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:06,258 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:06,262 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,263 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,264 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:06,280 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:06,287 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:06,294 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:06,302 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:48:06,303 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:06,305 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:06,310 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:06,314 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,315 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,316 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:06,323 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:06,330 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:06,337 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:06,349 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:48:06,351 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:06,353 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:06,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:06,362 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,362 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,363 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:06,372 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:06,383 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:06,390 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:06,397 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:48:06,398 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:06,400 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:06,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:06,410 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,411 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,411 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:06,420 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:06,427 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:06,434 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:06,453 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:48:06,454 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:06,456 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:06,461 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:06,462 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,463 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,463 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:06,473 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:06,478 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:06,483 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:06,494 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:48:06,495 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:06,497 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:06,499 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:06,500 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,501 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:06,502 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:06,505 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:06,509 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:06,513 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:06,515 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:06,516 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:06,517 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:06,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:06,520 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,521 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:06,522 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:06,540 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:06,556 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:06,574 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:06,596 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:06,597 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:06,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:06,615 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:06,616 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:06,617 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:06,618 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:06,619 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:06,620 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:06,622 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:06,623 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:06,624 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:06,625 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:06,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:06,631 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 29])", "<class 'int'>: 28")
2023-10-11 12:48:06,632 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:06,633 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:06,635 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:06,636 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:06,638 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:06,640 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:06,640 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:06,645 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:06,649 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:06,654 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,655 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,656 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:06,663 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:06,670 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:06,675 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:06,701 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:48:06,702 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:06,704 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:06,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:06,714 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,715 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,716 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:06,728 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:06,734 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:06,739 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:06,785 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:48:06,786 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:06,789 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:06,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:06,806 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,807 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,808 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:06,832 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:06,851 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:06,857 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:06,863 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:48:06,865 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:06,868 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:06,872 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:06,877 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,878 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,879 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:06,890 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:06,899 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:06,910 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:06,919 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:48:06,920 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:06,922 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:06,926 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:06,931 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:06,932 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:06,933 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:06,993 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:07,004 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:07,011 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:07,022 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:48:07,027 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:07,032 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:07,040 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:07,048 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,049 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,050 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:07,057 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:07,063 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:07,075 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:07,082 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:48:07,083 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:07,085 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:07,089 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:07,094 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,095 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,096 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:07,102 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:07,110 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:07,118 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:07,146 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:48:07,147 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:07,150 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:07,154 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:07,159 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,160 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,161 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:07,168 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:07,174 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:07,181 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:07,186 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:48:07,187 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:07,189 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:07,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:07,198 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,199 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,200 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:07,206 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:07,219 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:07,224 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:07,230 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:48:07,231 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:07,233 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:07,238 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:07,242 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,243 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,244 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:07,254 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:07,260 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:07,265 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:07,275 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:48:07,276 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:07,278 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:07,283 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:07,287 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,288 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,289 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:07,295 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:07,300 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:07,305 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:07,313 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:48:07,314 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:07,316 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:07,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:07,322 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,323 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,324 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:07,356 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:07,363 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:07,376 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:07,384 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:48:07,385 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:07,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:07,389 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:07,390 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,391 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:07,392 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:07,394 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:07,396 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:07,399 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:07,401 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:07,402 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:07,403 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:07,405 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:07,406 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,407 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:07,407 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:07,423 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:07,432 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:07,442 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:07,452 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:07,453 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:07,467 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:07,468 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:07,469 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:07,471 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:07,472 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:07,473 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:07,475 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:07,476 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:07,477 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:07,478 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:07,480 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:07,481 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:07,486 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 30])", "<class 'int'>: 29")
2023-10-11 12:48:07,487 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:07,488 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:07,490 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:07,491 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:07,493 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:07,494 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:07,496 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:07,500 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:07,504 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:07,509 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,510 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,510 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:07,519 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:07,534 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:07,542 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:07,548 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:48:07,549 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:07,552 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:07,557 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:07,562 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,563 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,563 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:07,571 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:07,577 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:07,584 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:07,603 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:48:07,604 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:07,608 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:07,612 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:07,617 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,618 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,618 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:07,625 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:07,630 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:07,637 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:07,642 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:48:07,643 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:07,645 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:07,650 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:07,655 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,656 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,656 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:07,663 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:07,672 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:07,678 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:07,684 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:48:07,685 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:07,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:07,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:07,696 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,697 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,698 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:07,707 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:07,714 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:07,726 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:07,733 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:48:07,733 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:07,736 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:07,740 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:07,745 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,746 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,747 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:07,754 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:07,761 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:07,766 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:07,773 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:48:07,774 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:07,776 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:07,780 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:07,785 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,786 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,787 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:07,796 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:07,801 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:07,807 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:07,819 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:48:07,820 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:07,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:07,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:07,831 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,832 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,833 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:07,839 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:07,846 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:07,852 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:07,857 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:48:07,858 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:07,860 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:07,864 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:07,868 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,869 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,870 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:07,891 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:07,904 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:07,910 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:07,918 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:48:07,919 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:07,921 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:07,925 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:07,930 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,930 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,931 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:07,940 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:07,947 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:07,961 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:07,967 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:48:07,968 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:07,971 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:07,975 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:07,980 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:07,981 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:07,982 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:07,989 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:07,995 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:08,002 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:08,010 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:48:08,011 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:08,014 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:08,020 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:08,022 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,023 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:08,024 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:08,032 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:08,043 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:08,048 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:08,055 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:48:08,055 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:08,057 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:08,059 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:08,061 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,061 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:08,062 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:08,066 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:08,077 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:08,079 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:08,082 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:08,083 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:08,084 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:08,085 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:08,087 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,088 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:08,089 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:08,104 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:08,117 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:08,127 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:08,136 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:08,138 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:08,145 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:08,147 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:08,148 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:08,149 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:08,149 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:08,151 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:08,153 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:08,154 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:08,156 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:08,157 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:08,158 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:08,160 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:08,164 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 31])", "<class 'int'>: 30")
2023-10-11 12:48:08,165 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:08,166 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:08,167 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:08,169 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:08,171 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:08,172 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:08,173 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:08,177 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:08,182 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:08,186 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,187 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:08,188 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:08,195 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:08,200 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:08,207 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:08,212 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:48:08,213 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:08,215 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:08,220 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:08,224 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,225 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:08,226 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:08,232 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:08,242 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:08,267 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:08,275 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:48:08,276 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:08,278 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:08,282 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:08,287 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,288 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:08,292 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:08,366 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:08,387 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:08,394 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:08,400 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:48:08,401 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:08,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:08,408 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:08,413 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,414 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:08,415 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:08,425 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:08,436 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:08,446 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:08,452 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:48:08,453 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:08,455 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:08,459 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:08,464 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,464 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:08,465 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:08,489 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:08,498 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:08,508 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:08,516 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:48:08,517 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:08,520 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:08,524 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:08,529 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,530 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:08,531 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:08,537 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:08,551 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:08,558 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:08,564 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:48:08,564 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:08,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:08,570 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:08,575 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,575 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:08,576 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:08,583 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:08,591 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:08,596 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:08,601 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:48:08,602 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:08,605 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:08,610 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:08,614 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,615 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:08,616 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:08,623 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:08,629 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:08,634 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:08,650 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:48:08,651 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:08,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:08,658 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:08,663 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,664 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:08,666 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:08,673 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:08,682 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:08,692 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:08,699 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:48:08,700 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:08,703 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:08,707 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:08,712 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,713 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:08,714 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:08,723 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:08,738 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:08,744 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:08,750 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:48:08,751 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:08,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:08,759 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:08,764 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,765 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:08,766 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:08,773 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:08,781 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:08,807 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:08,815 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:48:08,816 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:08,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:08,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:08,825 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,826 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:08,827 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:08,835 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:08,908 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:08,926 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:08,946 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:48:08,947 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:08,950 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:08,952 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:08,953 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,953 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:08,955 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:08,958 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:08,960 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:08,965 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:08,968 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:08,968 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:08,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:08,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:08,973 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:08,974 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:08,974 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:08,991 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:09,002 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:09,013 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:09,023 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:09,024 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:09,034 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:09,035 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:09,036 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:09,037 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:09,038 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:09,040 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:09,041 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:09,043 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:09,044 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:09,045 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:09,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:09,047 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:09,052 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 32])", "<class 'int'>: 31")
2023-10-11 12:48:09,053 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:09,053 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:09,056 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:09,057 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:09,059 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:09,060 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:09,061 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:09,065 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:09,069 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:09,074 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,075 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,076 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:09,085 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:09,092 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:09,099 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:09,105 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:48:09,105 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:09,108 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:09,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:09,118 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,118 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,119 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:09,127 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:09,138 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:09,144 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:09,149 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:48:09,150 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:09,153 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:09,157 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:09,162 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,163 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,164 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:09,171 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:09,177 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:09,182 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:09,197 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:48:09,198 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:09,200 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:09,204 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:09,210 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,210 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,211 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:09,218 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:09,228 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:09,234 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:09,242 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:48:09,243 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:09,245 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:09,250 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:09,255 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,256 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,257 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:09,265 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:09,271 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:09,277 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:09,283 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:48:09,284 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:09,286 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:09,291 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:09,296 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,296 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,297 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:09,304 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:09,311 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:09,317 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:09,323 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:48:09,324 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:09,327 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:09,332 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:09,337 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,338 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,339 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:09,346 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:09,369 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:09,377 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:09,386 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:48:09,387 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:09,389 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:09,395 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:09,401 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,402 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,402 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:09,410 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:09,416 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:09,431 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:09,441 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:48:09,442 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:09,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:09,449 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:09,454 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,454 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,455 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:09,471 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:09,477 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:09,482 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:09,488 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:48:09,489 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:09,491 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:09,496 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:09,501 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,502 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,503 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:09,510 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:09,515 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:09,526 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:09,534 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:48:09,535 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:09,537 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:09,541 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:09,546 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,547 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,548 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:09,556 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:09,562 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:09,575 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:09,583 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:48:09,583 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:09,585 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:09,590 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:09,592 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,593 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,593 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:09,673 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:09,681 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:09,687 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:09,694 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:48:09,695 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:09,698 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:09,701 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:09,702 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,703 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:09,704 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:09,707 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:09,711 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:09,714 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:09,717 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:09,719 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:09,720 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:09,721 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:09,723 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,724 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:09,725 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:09,738 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:09,753 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:09,765 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:09,775 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:09,777 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:09,787 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:09,788 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:09,790 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:09,790 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:09,791 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:09,793 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:09,794 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:09,795 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:09,797 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:09,798 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:09,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:09,801 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:09,806 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 33])", "<class 'int'>: 32")
2023-10-11 12:48:09,807 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:09,808 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:09,810 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:09,812 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:09,813 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:09,815 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:09,815 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:09,820 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:09,825 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:09,830 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,831 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,832 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:09,839 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:09,847 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:09,852 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:09,930 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:48:09,931 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:09,934 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:09,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:09,943 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,944 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,945 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:09,952 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:09,957 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:09,965 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:09,972 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:48:09,973 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:09,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:09,980 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:09,985 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:09,986 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:09,986 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:09,993 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:10,006 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:10,012 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:10,018 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:48:10,019 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:10,021 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:10,026 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:10,031 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:10,032 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:10,033 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:10,042 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:10,051 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:10,058 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:10,067 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:48:10,068 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:10,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:10,075 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:10,079 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:10,080 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:10,081 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:10,087 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:10,096 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:10,104 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:10,122 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:48:10,123 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:10,125 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:10,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:10,136 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:10,137 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:10,138 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:10,191 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:10,198 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:10,204 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:10,210 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:48:10,211 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:10,214 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:10,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:10,224 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:10,225 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:10,226 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:10,233 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:10,241 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:10,246 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:10,252 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:48:10,253 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:10,255 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:10,262 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:10,268 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:10,269 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:10,269 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:10,276 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:10,282 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:10,287 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:10,295 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:48:10,296 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:10,298 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:10,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:10,307 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:10,308 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:10,309 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:10,332 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:10,342 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:10,350 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:10,355 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:48:10,356 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:10,358 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:10,363 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:10,368 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:10,369 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:10,370 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:10,376 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:10,429 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:10,488 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:10,497 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:48:10,498 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:10,501 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:10,507 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:10,512 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:10,513 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:10,513 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:10,525 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:10,533 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:10,541 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:10,557 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:48:10,558 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:10,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:10,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:10,567 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:10,568 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:10,569 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:10,651 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:10,658 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:10,664 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:10,670 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:48:10,671 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:10,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:10,675 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:10,676 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:10,678 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:10,679 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:10,681 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:10,683 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:10,685 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:10,687 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:10,688 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:10,690 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:10,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:10,692 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:10,693 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:10,694 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:10,706 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:10,717 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:10,731 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:10,745 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:10,746 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:10,752 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:10,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:10,755 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:10,756 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:10,757 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:10,759 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:10,760 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:10,762 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:10,764 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:10,764 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:10,766 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:10,767 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:10,772 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 34])", "<class 'int'>: 33")
2023-10-11 12:48:10,773 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:10,774 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:10,775 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:10,777 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:10,779 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:10,780 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:10,781 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:10,785 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:10,789 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:10,794 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:10,795 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:10,796 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:10,803 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:10,810 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:10,816 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:10,822 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:48:10,823 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:10,825 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:10,830 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:10,834 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:10,835 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:10,836 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:10,843 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:10,851 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:10,856 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:10,862 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:48:10,863 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:10,865 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:10,870 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:10,876 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:10,877 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:10,877 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:10,902 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:10,971 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:11,000 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:11,007 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:48:11,008 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:11,010 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:11,014 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:11,019 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,020 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,020 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:11,040 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:11,048 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:11,055 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:11,063 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:48:11,064 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:11,066 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:11,071 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:11,076 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,077 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,078 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:11,088 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:11,103 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:11,116 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:11,123 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:48:11,124 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:11,126 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:11,131 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:11,137 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,137 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,138 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:11,146 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:11,152 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:11,157 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:11,164 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:48:11,165 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:11,168 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:11,172 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:11,177 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,178 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,179 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:11,198 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:11,213 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:11,220 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:11,227 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:48:11,228 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:11,230 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:11,235 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:11,239 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,240 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,241 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:11,251 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:11,259 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:11,268 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:11,282 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:48:11,284 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:11,288 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:11,296 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:11,301 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,302 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,303 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:11,312 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:11,319 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:11,325 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:11,332 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:48:11,333 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:11,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:11,340 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:11,345 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,346 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,347 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:11,355 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:11,361 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:11,370 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:11,380 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:48:11,381 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:11,383 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:11,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:11,393 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,393 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,394 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:11,402 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:11,408 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:11,416 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:11,425 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:48:11,426 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:11,428 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:11,433 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:11,434 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,435 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,436 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:11,446 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:11,452 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:11,460 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:11,476 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:48:11,477 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:11,480 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:11,481 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:11,482 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,483 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:11,484 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:11,486 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:11,488 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:11,489 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:11,491 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:11,492 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:11,493 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:11,495 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:11,496 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,497 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:11,497 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:11,514 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:11,528 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:11,541 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:11,551 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:11,552 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:11,560 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:11,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:11,563 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:11,564 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:11,564 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:11,566 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:11,568 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:11,570 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:11,572 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:11,573 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:11,574 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:11,576 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:11,581 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 35])", "<class 'int'>: 34")
2023-10-11 12:48:11,581 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:11,582 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:11,584 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:11,586 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:11,587 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:11,589 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:11,590 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:11,596 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:11,600 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:11,606 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,607 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,608 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:11,616 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:11,622 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:11,630 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:11,638 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:48:11,639 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:11,642 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:11,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:11,651 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,652 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,653 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:11,660 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:11,669 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:11,675 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:11,684 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:48:11,685 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:11,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:11,692 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:11,697 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,697 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,699 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:11,712 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:11,718 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:11,725 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:11,731 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:48:11,732 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:11,735 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:11,740 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:11,746 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,746 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,747 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:11,754 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:11,766 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:11,775 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:11,783 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:48:11,784 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:11,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:11,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:11,795 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,796 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,797 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:11,819 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:11,826 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:11,831 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:11,840 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:48:11,841 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:11,843 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:11,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:11,854 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,856 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,857 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:11,916 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:11,932 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:11,959 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:11,969 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:48:11,970 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:11,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:11,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:11,981 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:11,982 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:11,983 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:12,001 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:12,007 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:12,013 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:12,019 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:48:12,020 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:12,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:12,026 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:12,031 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,032 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,033 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:12,043 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:12,049 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:12,058 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:12,070 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:48:12,071 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:12,073 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:12,078 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:12,082 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,083 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,084 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:12,100 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:12,108 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:12,114 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:12,121 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:48:12,122 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:12,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:12,129 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:12,134 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,134 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,135 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:12,150 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:12,207 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:12,259 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:12,265 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:48:12,266 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:12,268 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:12,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:12,278 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,279 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,280 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:12,297 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:12,303 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:12,309 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:12,316 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:48:12,317 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:12,319 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:12,324 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:12,325 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,326 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,327 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:12,335 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:12,343 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:12,349 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:12,355 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:48:12,356 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:12,359 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:12,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:12,362 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,362 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:12,363 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:12,367 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:12,369 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:12,371 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:12,375 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:12,375 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:12,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:12,378 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:12,380 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,381 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:12,382 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:12,394 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:12,409 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:12,419 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:12,429 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:12,431 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:12,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:12,441 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:12,442 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:12,443 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:12,444 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:12,445 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:12,447 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:12,448 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:12,450 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:12,450 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:12,452 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:12,453 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:12,458 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 36])", "<class 'int'>: 35")
2023-10-11 12:48:12,459 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:12,460 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:12,462 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:12,463 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:12,464 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:12,466 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:12,467 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:12,471 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:12,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:12,480 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,481 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,482 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:12,493 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:12,501 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:12,511 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:12,520 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:48:12,521 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:12,523 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:12,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:12,534 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,535 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,535 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:12,543 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:12,549 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:12,554 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:12,561 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:48:12,562 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:12,564 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:12,570 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:12,575 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,576 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,576 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:12,583 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:12,593 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:12,599 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:12,605 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:48:12,606 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:12,609 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:12,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:12,618 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,619 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,620 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:12,627 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:12,634 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:12,642 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:12,651 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:48:12,652 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:12,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:12,658 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:12,664 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,664 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,665 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:12,676 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:12,682 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:12,689 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:12,695 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:48:12,696 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:12,699 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:12,703 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:12,708 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,709 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,710 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:12,717 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:12,733 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:12,740 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:12,748 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:48:12,749 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:12,751 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:12,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:12,760 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,761 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,762 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:12,769 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:12,776 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:12,787 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:12,812 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:48:12,813 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:12,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:12,824 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:12,830 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,831 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,831 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:12,901 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:12,911 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:12,918 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:12,926 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:48:12,927 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:12,929 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:12,934 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:12,939 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,940 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,940 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:12,950 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:12,957 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:12,966 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:12,977 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:48:12,978 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:12,981 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:12,986 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:12,991 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:12,992 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:12,993 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:13,077 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:13,111 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:13,123 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:13,128 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:48:13,129 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:13,131 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:13,135 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:13,140 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:13,141 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:13,142 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:13,152 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:13,159 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:13,166 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:13,174 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:48:13,175 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:13,177 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:13,181 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:13,183 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:13,184 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:13,185 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:13,193 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:13,199 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:13,205 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:13,214 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:48:13,215 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:13,218 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:13,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:13,220 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:13,221 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:13,222 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:13,224 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:13,225 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:13,227 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:13,230 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:13,231 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:13,233 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:13,234 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:13,235 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:13,236 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:13,236 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:13,250 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:13,263 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:13,275 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:13,299 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:13,300 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:13,341 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:13,343 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:13,344 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:13,346 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:13,346 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:13,349 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:13,350 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:13,352 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:13,354 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:13,355 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:13,356 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:13,358 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:13,363 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 37])", "<class 'int'>: 36")
2023-10-11 12:48:13,363 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:13,364 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:13,367 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:13,369 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:13,371 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:13,373 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:13,373 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:13,378 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:13,383 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:13,388 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:13,389 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:13,390 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:13,400 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:13,407 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:13,421 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:13,439 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:48:13,440 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:13,442 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:13,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:13,451 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:13,452 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:13,452 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:13,459 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:13,467 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:13,472 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:13,482 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:48:13,482 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:13,485 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:13,489 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:13,494 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:13,494 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:13,495 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:13,575 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:13,598 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:13,605 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:13,633 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:48:13,634 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:13,637 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:13,642 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:13,646 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:13,647 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:13,648 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:13,655 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:13,661 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:13,666 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:13,677 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:48:13,678 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:13,681 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:13,689 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:13,700 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:13,702 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:13,711 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:13,803 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:13,832 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:13,843 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:13,849 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:48:13,850 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:13,853 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:13,857 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:13,861 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:13,863 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:13,864 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:13,872 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:13,877 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:13,883 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:13,889 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:48:13,890 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:13,892 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:13,897 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:13,901 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:13,902 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:13,904 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:13,913 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:13,923 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:13,929 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:13,936 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:48:13,937 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:13,939 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:13,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:13,948 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:13,949 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:13,950 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:13,957 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:13,965 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:13,971 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:13,978 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:48:13,979 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:13,981 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:13,987 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:13,993 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:13,993 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:13,994 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:14,003 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:14,009 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:14,015 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:14,023 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:48:14,024 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:14,026 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:14,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:14,035 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,036 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,036 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:14,051 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:14,062 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:14,067 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:14,074 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:48:14,075 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:14,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:14,082 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:14,087 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,087 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,088 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:14,098 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:14,104 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:14,110 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:14,120 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:48:14,120 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:14,123 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:14,127 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:14,129 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,129 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,130 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:14,137 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:14,147 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:14,157 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:14,169 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:48:14,170 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:14,172 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:14,174 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:14,176 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,177 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:14,177 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:14,182 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:14,184 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:14,187 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:14,189 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:14,190 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:14,192 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:14,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:14,194 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,195 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:14,196 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:14,209 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:14,220 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:14,233 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:14,243 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:14,244 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:14,252 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:14,254 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:14,256 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:48:14,256 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:14,257 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:48:14,259 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:48:14,260 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:48:14,261 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:48:14,263 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:14,263 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:48:14,265 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:48:14,266 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:14,271 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 38])", "<class 'int'>: 37")
2023-10-11 12:48:14,271 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:14,272 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:48:14,274 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:48:14,275 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:48:14,277 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:48:14,278 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:14,279 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:48:14,283 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:48:14,288 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:14,292 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,293 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,294 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:48:14,301 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:48:14,310 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:48:14,317 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:48:14,323 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:48:14,324 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:48:14,326 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:48:14,330 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:14,335 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,336 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,336 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:48:14,352 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:48:14,359 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:48:14,364 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:48:14,370 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:48:14,371 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:48:14,374 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:48:14,378 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:14,383 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,384 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,386 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:48:14,395 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:48:14,420 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:48:14,435 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:48:14,442 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:48:14,443 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:48:14,446 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:48:14,451 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:14,456 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,457 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,458 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:48:14,491 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:48:14,500 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:48:14,505 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:48:14,512 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:48:14,513 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:48:14,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:48:14,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:14,524 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,525 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,526 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:48:14,538 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:48:14,544 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:48:14,549 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:48:14,555 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:48:14,555 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:48:14,558 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:48:14,562 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:14,567 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,571 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,573 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:48:14,654 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:48:14,662 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:48:14,668 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:48:14,676 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:48:14,677 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:48:14,680 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:48:14,684 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:14,689 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,690 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,691 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:48:14,700 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:48:14,707 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:48:14,713 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:48:14,721 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:48:14,722 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:48:14,725 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:48:14,730 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:14,735 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,736 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,737 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:48:14,747 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:48:14,752 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:48:14,759 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:48:14,764 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:48:14,765 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:48:14,768 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:48:14,772 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:14,777 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,778 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,780 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:48:14,787 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:48:14,794 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:48:14,801 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:48:14,808 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:48:14,810 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:48:14,812 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:48:14,817 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:14,822 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,823 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,824 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:48:14,833 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:48:14,839 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:48:14,848 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:48:14,855 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:48:14,856 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:48:14,858 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:48:14,863 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:14,868 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,869 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,869 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:48:14,876 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:48:14,882 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:48:14,894 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:48:14,900 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:48:14,901 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:48:14,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:48:14,908 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:14,909 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,910 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:48:14,911 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:48:14,918 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:48:14,926 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:48:14,931 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:48:14,940 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:48:14,940 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:48:14,943 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:48:14,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:14,945 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,946 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:14,947 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:48:14,950 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:48:14,952 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:48:14,954 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:48:14,956 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:48:14,956 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:48:14,958 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:48:14,959 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:48:14,960 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:48:14,961 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:48:14,962 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:48:14,973 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:48:14,987 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:48:15,001 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:48:15,014 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:48:15,016 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:48:15,026 [test.py:40 in test_hf_gen] INFO - for i in range(10):  ( ( (
,,,,,,,,,,,,,,,,, and and and and and and and and and
2023-10-11 12:48:15,027 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:48:15,028 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious???,...   ...                  
2023-10-11 12:48:15,029 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:48:15,030 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?ooooooooooooooooooo's,, and and and and and and and and
2023-10-11 12:48:15,031 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:48:15,032 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?oooooo   ...                  
2023-10-11 12:48:15,033 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:48:15,033 [test.py:40 in test_hf_gen] INFO - for i in range(10):  ( to
:::,,,,,,,,,,,,,,,, and and and and and and and and
2023-10-11 12:48:15,034 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:48:15,035 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious??                             
2023-10-11 12:48:15,036 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:48:15,037 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?oooooooooooooooooo's's, and and and and and and and and and
2023-10-11 12:48:15,038 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:48:15,039 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?ooooo                         
2023-10-11 12:48:15,041 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:48:15,052 [520681597.py:17 in reset_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-11 12:48:15,053 [520681597.py:17 in reset_forward] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-11 12:48:15,054 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-11 12:48:15,055 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-11 12:48:15,056 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-11 12:48:15,057 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-11 12:48:15,059 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-11 12:48:15,059 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-11 12:48:15,061 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-11 12:48:15,061 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-11 12:48:15,062 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-11 12:48:15,063 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-11 12:48:15,064 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-11 12:48:15,064 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-11 12:48:15,065 [520681597.py:17 in reset_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-11 12:48:15,066 [520681597.py:17 in reset_forward] DEBUG - lm_head from flexgen to old.
2023-10-11 12:50:33,701 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:50:33,829 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:50:33,913 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:50:33,955 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:50:34,036 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:50:34,037 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'
2023-10-11 12:50:34,044 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-11 12:50:34,045 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-11 12:50:34,046 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-11 12:50:34,048 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-11 12:50:34,050 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-11 12:50:34,051 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-11 12:50:34,053 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-11 12:50:34,055 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-11 12:50:34,057 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-11 12:50:34,059 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-11 12:50:34,061 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-11 12:50:34,062 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-11 12:50:34,063 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-11 12:50:34,065 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-11 12:50:34,067 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:50:34,068 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:50:34,069 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-11 12:50:34,072 [model.py:148 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-11 12:50:34,076 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-11 12:50:34,104 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-11 12:50:34,105 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-11 12:50:34,106 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-11 12:50:34,107 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-11 12:50:34,107 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-11 12:50:34,108 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-11 12:50:34,109 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-11 12:50:34,110 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-11 12:50:34,111 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-11 12:50:34,112 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-11 12:50:34,113 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-11 12:50:34,114 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-11 12:50:34,115 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-11 12:50:34,116 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-11 12:50:34,117 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-11 12:50:34,118 [520681597.py:42 in to_test_forward] DEBUG - lm_head to test forward
2023-10-11 12:50:34,185 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:50:34,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:34,345 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:34,347 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:34,349 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:34,350 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:34,361 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:34,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:34,373 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:34,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:34,387 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:34,389 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:34,397 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:34,399 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:34,408 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:34,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:34,420 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:34,423 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:34,430 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:34,433 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:34,439 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:34,442 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:34,450 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:34,452 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:34,459 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:34,462 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:34,470 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:34,473 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:34,481 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:34,485 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:34,487 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:34,488 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:34,500 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:34,506 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-11 12:50:34,507 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:50:34,516 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-11 12:50:34,517 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-11 12:50:34,518 [520681597.py:22 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-11 12:50:34,520 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-11 12:50:34,520 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-11 12:50:34,521 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-11 12:50:34,522 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-11 12:50:34,523 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-11 12:50:34,524 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-11 12:50:34,525 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-11 12:50:34,526 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-11 12:50:34,527 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-11 12:50:34,528 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-11 12:50:34,529 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-11 12:50:34,531 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-11 12:50:34,531 [520681597.py:22 in reset_forward] DEBUG - lm_head from test to old.
2023-10-11 12:50:34,532 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-11 12:50:34,534 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-11 12:50:34,534 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-11 12:50:34,536 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-11 12:50:34,537 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-11 12:50:34,538 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-11 12:50:34,539 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-11 12:50:34,540 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-11 12:50:34,540 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-11 12:50:34,542 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-11 12:50:34,543 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-11 12:50:34,544 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-11 12:50:34,545 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-11 12:50:34,546 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-11 12:50:34,547 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-11 12:50:34,549 [2192271695.py:48 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-11 12:50:34,594 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:50:35,306 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:35,308 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:35,310 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])",)
2023-10-11 12:50:35,311 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:35,312 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:35,314 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:35,316 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:35,318 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:35,320 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:50:35,320 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:35,322 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:35,324 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:35,329 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])", "<class 'int'>: 0")
2023-10-11 12:50:35,330 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:35,331 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:35,334 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:35,335 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:35,337 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:35,339 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:50:35,340 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:35,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:35,349 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:35,353 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:50:35,354 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:35,355 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:35,366 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:35,376 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:35,384 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:35,395 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:50:35,396 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:35,399 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:35,403 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:35,408 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:50:35,409 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:35,409 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:35,420 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:35,428 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:35,437 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:35,444 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:50:35,444 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:35,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:35,451 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:35,456 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:50:35,457 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:35,457 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:35,467 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:35,474 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:35,482 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:35,491 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:50:35,492 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:35,495 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:35,501 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:35,506 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:50:35,507 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:35,509 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:35,518 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:35,525 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:35,534 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:35,553 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:50:35,554 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:35,557 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:35,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:35,566 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:50:35,567 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:35,568 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:35,600 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:35,607 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:35,616 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:35,623 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:50:35,624 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:35,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:35,632 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:35,637 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:50:35,638 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:35,639 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:35,647 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:35,657 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:35,664 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:35,674 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:50:35,675 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:35,677 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:35,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:35,687 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:50:35,687 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:35,688 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:35,705 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:35,715 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:35,729 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:35,745 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:50:35,746 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:35,749 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:35,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:35,759 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:50:35,760 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:35,761 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:35,768 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:35,775 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:35,788 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:35,798 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:50:35,799 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:35,801 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:35,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:35,811 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:50:35,811 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:35,812 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:35,820 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:35,827 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:35,835 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:35,843 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:50:35,844 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:35,846 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:35,851 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:35,856 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:50:35,857 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:35,858 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:35,865 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:35,873 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:35,879 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:35,888 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:50:35,889 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:35,892 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:35,897 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:35,903 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:50:35,903 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:35,904 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:35,912 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:35,918 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:35,932 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:35,940 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:50:35,940 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:35,943 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:35,948 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:35,949 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:50:35,950 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:35,951 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:35,959 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:35,968 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:35,976 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:35,983 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:50:35,984 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:35,986 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:35,988 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:35,989 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:50:35,990 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:35,991 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:35,994 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:35,998 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:36,002 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:36,005 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:50:36,006 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:36,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:36,010 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:36,012 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:50:36,013 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:36,013 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:36,029 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:36,047 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:36,062 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:36,078 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 25136])
2023-10-11 12:50:36,080 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:36,094 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:36,096 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:36,097 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:36,098 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:36,099 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:36,101 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:36,103 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:36,104 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:36,106 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:36,107 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:36,108 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:36,110 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:36,115 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 10])", "<class 'int'>: 9")
2023-10-11 12:50:36,115 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:36,116 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:36,118 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:36,120 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:36,121 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:36,123 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:36,124 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:36,128 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:36,133 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:36,137 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,138 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:36,139 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:36,148 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:36,159 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:36,165 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:36,172 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:50:36,173 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:36,175 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:36,180 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:36,185 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,186 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:36,187 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:36,193 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:36,210 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:36,218 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:36,224 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:50:36,225 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:36,227 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:36,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:36,237 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,238 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:36,239 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:36,247 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:36,262 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:36,275 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:36,285 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:50:36,286 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:36,289 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:36,295 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:36,300 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,300 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:36,301 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:36,308 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:36,313 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:36,317 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:36,322 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:50:36,323 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:36,326 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:36,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:36,335 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,336 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:36,337 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:36,349 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:36,356 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:36,361 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:36,370 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:50:36,371 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:36,373 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:36,378 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:36,383 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,384 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:36,385 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:36,396 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:36,402 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:36,406 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:36,449 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:50:36,450 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:36,453 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:36,457 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:36,461 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,462 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:36,463 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:36,515 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:36,553 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:36,595 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:36,630 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:50:36,632 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:36,635 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:36,640 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:36,648 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,649 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:36,650 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:36,659 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:36,664 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:36,670 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:36,685 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:50:36,686 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:36,689 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:36,693 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:36,697 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,698 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:36,699 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:36,705 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:36,711 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:36,716 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:36,722 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:50:36,723 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:36,725 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:36,729 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:36,734 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,735 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:36,736 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:36,747 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:36,752 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:36,757 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:36,764 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:50:36,765 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:36,767 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:36,772 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:36,776 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,777 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:36,778 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:36,784 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:36,792 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:36,798 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:36,802 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:50:36,803 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:36,805 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:36,810 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:36,811 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,812 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:36,813 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:36,821 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:36,826 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:36,831 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:36,836 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:50:36,837 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:36,839 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:36,840 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:36,841 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,842 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:36,843 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:36,848 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:36,851 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:36,854 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:36,856 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:36,857 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:36,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:36,860 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:36,861 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,862 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:36,862 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:36,878 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:36,890 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:36,904 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:36,917 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:36,918 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:36,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:36,929 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:36,931 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:36,932 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:36,933 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:36,935 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:36,936 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:36,940 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:36,942 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:36,943 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:36,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:36,946 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:36,951 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 11])", "<class 'int'>: 10")
2023-10-11 12:50:36,952 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:36,952 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:36,954 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:36,956 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:36,957 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:36,959 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:36,960 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:36,964 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:36,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:36,977 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:36,978 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:36,979 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:36,986 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:36,993 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:36,998 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:37,005 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:50:37,006 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:37,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:37,013 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:37,018 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,019 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,020 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:37,029 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:37,038 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:37,044 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:37,053 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:50:37,053 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:37,056 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:37,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:37,065 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,066 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,067 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:37,073 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:37,081 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:37,090 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:37,098 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:50:37,099 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:37,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:37,106 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:37,111 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,112 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,112 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:37,124 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:37,129 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:37,134 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:37,139 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:50:37,140 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:37,143 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:37,147 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:37,152 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,152 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,153 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:37,161 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:37,185 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:37,193 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:37,202 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:50:37,203 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:37,205 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:37,210 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:37,215 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,216 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,217 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:37,225 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:37,241 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:37,245 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:37,259 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:50:37,260 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:37,263 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:37,268 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:37,273 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,274 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,275 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:37,290 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:37,297 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:37,310 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:37,318 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:50:37,319 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:37,322 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:37,326 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:37,331 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,332 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,333 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:37,371 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:37,413 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:37,433 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:37,440 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:50:37,441 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:37,443 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:37,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:37,452 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,453 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,454 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:37,460 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:37,465 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:37,471 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:37,475 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:50:37,476 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:37,479 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:37,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:37,488 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,488 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,489 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:37,496 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:37,502 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:37,507 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:37,512 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:50:37,513 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:37,516 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:37,520 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:37,525 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,528 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,529 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:37,592 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:37,623 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:37,634 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:37,639 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:50:37,641 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:37,644 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:37,648 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:37,650 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,651 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,651 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:37,660 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:37,665 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:37,670 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:37,683 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:50:37,684 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:37,686 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:37,688 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:37,689 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,689 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:37,690 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:37,694 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:37,697 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:37,701 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:37,703 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:37,705 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:37,706 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:37,707 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:37,709 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,710 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:37,710 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:37,722 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:37,734 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:37,745 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:37,758 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:37,760 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:37,768 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:37,770 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:37,771 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:37,772 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:37,773 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:37,775 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:37,776 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:37,778 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:37,779 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:37,780 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:37,782 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:37,783 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:37,788 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 12])", "<class 'int'>: 11")
2023-10-11 12:50:37,789 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:37,790 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:37,791 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:37,793 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:37,795 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:37,796 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:37,797 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:37,801 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:37,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:37,811 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,812 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,812 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:37,820 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:37,826 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:37,831 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:37,837 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:50:37,837 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:37,840 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:37,844 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:37,849 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,850 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,851 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:37,857 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:37,862 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:37,879 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:37,889 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:50:37,889 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:37,892 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:37,896 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:37,900 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,901 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,902 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:37,910 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:37,918 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:37,929 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:37,941 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:50:37,942 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:37,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:37,949 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:37,953 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,953 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,954 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:37,960 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:37,965 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:37,970 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:37,979 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:50:37,980 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:37,982 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:37,987 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:37,992 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:37,993 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:37,993 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:38,001 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:38,007 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:38,012 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:38,024 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:50:38,025 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:38,027 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:38,031 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:38,036 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,036 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,037 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:38,044 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:38,050 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:38,063 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:38,074 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:50:38,076 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:38,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:38,082 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:38,087 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,088 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,089 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:38,095 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:38,100 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:38,106 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:38,121 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:50:38,122 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:38,125 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:38,129 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:38,134 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,135 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,136 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:38,142 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:38,147 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:38,175 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:38,211 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:50:38,212 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:38,214 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:38,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:38,224 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,225 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,226 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:38,267 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:38,286 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:38,313 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:38,319 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:50:38,320 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:38,323 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:38,327 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:38,332 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,333 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,334 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:38,341 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:38,348 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:38,354 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:38,358 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:50:38,359 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:38,361 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:38,366 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:38,371 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,372 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,372 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:38,378 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:38,389 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:38,394 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:38,399 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:50:38,400 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:38,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:38,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:38,409 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,409 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,410 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:38,417 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:38,424 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:38,430 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:38,442 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:50:38,442 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:38,445 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:38,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:38,448 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,448 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:38,449 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:38,451 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:38,454 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:38,456 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:38,458 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:38,459 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:38,462 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:38,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:38,464 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,465 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:38,466 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:38,481 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:38,492 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:38,507 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:38,517 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:38,519 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:38,528 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:38,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:38,531 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:38,531 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:38,532 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:38,534 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:38,535 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:38,537 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:38,538 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:38,539 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:38,541 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:38,542 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:38,548 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 13])", "<class 'int'>: 12")
2023-10-11 12:50:38,549 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:38,551 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:38,552 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:38,554 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:38,556 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:38,557 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:38,558 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:38,562 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:38,567 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:38,572 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,573 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,574 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:38,580 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:38,594 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:38,601 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:38,640 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:50:38,641 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:38,645 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:38,652 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:38,659 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,660 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,661 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:38,670 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:38,677 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:38,687 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:38,694 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:50:38,695 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:38,698 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:38,705 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:38,712 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,712 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,713 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:38,721 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:38,729 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:38,735 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:38,740 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:50:38,741 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:38,744 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:38,749 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:38,755 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,756 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,757 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:38,774 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:38,779 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:38,784 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:38,802 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:50:38,803 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:38,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:38,811 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:38,816 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,817 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,819 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:38,825 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:38,830 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:38,835 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:38,845 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:50:38,847 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:38,852 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:38,865 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:38,873 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,874 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,875 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:38,895 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:38,902 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:38,910 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:38,916 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:50:38,917 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:38,919 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:38,924 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:38,929 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,930 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,931 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:38,940 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:38,953 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:38,969 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:38,974 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:50:38,975 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:38,977 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:38,981 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:38,986 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:38,987 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:38,987 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:38,998 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:39,047 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:39,097 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:39,108 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:50:39,109 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:39,111 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:39,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:39,121 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,122 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,122 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:39,129 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:39,140 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:39,145 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:39,153 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:50:39,155 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:39,157 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:39,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:39,167 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,168 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,169 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:39,180 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:39,185 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:39,191 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:39,196 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:50:39,197 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:39,199 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:39,203 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:39,208 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,208 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,209 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:39,215 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:39,222 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:39,227 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:39,234 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:50:39,235 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:39,238 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:39,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:39,243 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,244 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,245 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:39,252 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:39,258 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:39,292 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:39,302 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:50:39,303 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:39,306 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:39,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:39,308 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,309 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:39,310 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:39,313 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:39,316 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:39,318 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:39,320 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:39,321 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:39,322 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:39,324 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:39,325 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,326 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:39,327 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:39,345 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:39,357 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:39,369 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:39,379 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:39,380 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:39,387 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:39,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:39,390 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:39,391 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:39,392 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:39,394 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:39,395 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:39,397 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:39,398 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:39,399 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:39,401 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:39,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:39,407 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 14])", "<class 'int'>: 13")
2023-10-11 12:50:39,408 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:39,409 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:39,411 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:39,413 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:39,414 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:39,416 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:39,417 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:39,421 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:39,426 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:39,430 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,431 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,432 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:39,438 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:39,447 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:39,459 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:39,468 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:50:39,469 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:39,471 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:39,476 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:39,480 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,481 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,482 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:39,491 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:39,496 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:39,504 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:39,509 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:50:39,510 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:39,513 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:39,517 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:39,522 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,523 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,523 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:39,556 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:39,562 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:39,569 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:39,576 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:50:39,577 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:39,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:39,584 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:39,589 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,589 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,590 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:39,604 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:39,610 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:39,618 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:39,627 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:50:39,628 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:39,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:39,635 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:39,641 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,642 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,642 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:39,650 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:39,656 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:39,669 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:39,685 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:50:39,686 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:39,688 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:39,699 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:39,704 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,705 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,706 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:39,752 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:39,759 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:39,775 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:39,781 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:50:39,783 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:39,785 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:39,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:39,795 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,796 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,797 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:39,804 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:39,810 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:39,826 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:39,831 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:50:39,832 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:39,834 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:39,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:39,843 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,844 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,845 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:39,853 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:39,861 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:39,868 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:39,879 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:50:39,880 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:39,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:39,888 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:39,893 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,894 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,894 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:39,901 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:39,922 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:39,927 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:39,932 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:50:39,934 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:39,936 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:39,940 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:39,945 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,945 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,946 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:39,952 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:39,959 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:39,964 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:39,970 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:50:39,970 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:39,973 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:39,977 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:39,982 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:39,983 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:39,984 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:39,990 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:39,995 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:40,003 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:40,009 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:50:40,010 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:40,013 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:40,017 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:40,019 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,020 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:40,021 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:40,029 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:40,035 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:40,040 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:40,050 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:50:40,051 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:40,054 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:40,055 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:40,057 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,058 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:40,058 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:40,061 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:40,063 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:40,065 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:40,067 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:40,067 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:40,069 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:40,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:40,072 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,072 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:40,073 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:40,090 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:40,102 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:40,116 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:40,126 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:40,128 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:40,138 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:40,141 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:40,142 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:40,143 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:40,144 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:40,146 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:40,147 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:40,149 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:40,150 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:40,151 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:40,153 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:40,154 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:40,159 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 15])", "<class 'int'>: 14")
2023-10-11 12:50:40,160 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:40,161 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:40,163 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:40,165 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:40,166 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:40,168 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:40,169 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:40,173 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:40,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:40,182 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,183 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:40,184 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:40,193 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:40,200 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:40,206 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:40,211 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:50:40,211 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:40,214 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:40,218 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:40,223 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,224 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:40,225 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:40,232 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:40,237 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:40,242 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:40,247 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:50:40,248 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:40,250 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:40,255 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:40,260 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,261 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:40,262 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:40,268 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:40,295 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:40,321 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:40,327 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:50:40,327 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:40,330 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:40,334 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:40,339 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,340 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:40,340 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:40,349 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:40,354 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:40,360 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:40,373 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:50:40,374 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:40,376 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:40,381 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:40,386 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,387 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:40,388 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:40,395 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:40,410 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:40,414 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:40,456 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:50:40,457 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:40,459 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:40,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:40,468 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,469 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:40,470 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:40,510 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:40,522 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:40,528 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:40,533 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:50:40,534 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:40,536 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:40,540 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:40,545 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,546 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:40,547 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:40,554 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:40,561 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:40,577 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:40,582 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:50:40,583 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:40,585 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:40,590 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:40,594 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,595 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:40,596 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:40,602 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:40,610 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:40,615 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:40,620 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:50:40,621 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:40,623 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:40,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:40,631 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,632 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:40,633 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:40,639 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:40,644 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:40,649 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:40,657 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:50:40,659 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:40,662 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:40,666 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:40,671 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,672 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:40,673 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:40,682 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:40,692 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:40,698 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:40,703 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:50:40,705 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:40,707 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:40,711 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:40,716 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,717 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:40,718 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:40,725 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:40,730 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:40,738 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:40,746 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:50:40,747 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:40,750 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:40,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:40,756 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,757 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:40,758 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:40,770 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:40,779 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:40,786 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:40,808 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:50:40,809 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:40,811 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:40,812 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:40,813 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,814 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:40,815 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:40,818 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:40,824 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:40,832 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:40,846 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:40,849 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:40,850 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:40,852 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:40,853 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,854 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:40,855 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:40,872 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:40,891 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:40,908 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:40,923 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:40,924 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:40,937 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:40,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:40,939 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:40,940 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:40,941 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:40,943 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:40,944 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:40,945 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:40,947 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:40,948 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:40,949 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:40,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:40,955 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 16])", "<class 'int'>: 15")
2023-10-11 12:50:40,956 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:40,957 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:40,959 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:40,960 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:40,962 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:40,964 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:40,965 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:40,969 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:40,974 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:40,979 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:40,980 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:40,981 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:40,991 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:40,996 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:41,001 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:41,009 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:50:41,010 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:41,013 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:41,017 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:41,022 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,023 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,024 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:41,034 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:41,039 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:41,045 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:41,065 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:50:41,066 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:41,069 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:41,073 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:41,078 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,078 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,079 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:41,085 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:41,090 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:41,095 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:41,100 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:50:41,101 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:41,103 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:41,108 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:41,112 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,113 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,114 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:41,120 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:41,125 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:41,136 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:41,141 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:50:41,142 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:41,145 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:41,150 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:41,155 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,156 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,157 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:41,177 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:41,183 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:41,188 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:41,195 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:50:41,196 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:41,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:41,209 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:41,214 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,215 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,216 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:41,271 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:41,277 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:41,315 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:41,322 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:50:41,323 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:41,325 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:41,329 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:41,334 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,335 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,335 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:41,342 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:41,349 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:41,355 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:41,367 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:50:41,368 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:41,370 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:41,375 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:41,380 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,381 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,382 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:41,388 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:41,394 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:41,406 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:41,413 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:50:41,414 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:41,417 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:41,422 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:41,428 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,429 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,430 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:41,436 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:41,442 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:41,446 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:41,451 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:50:41,452 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:41,455 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:41,459 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:41,464 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,465 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,466 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:41,472 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:41,477 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:41,482 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:41,488 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:50:41,488 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:41,491 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:41,495 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:41,501 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,501 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,502 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:41,510 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:41,517 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:41,529 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:41,542 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:50:41,544 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:41,546 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:41,551 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:41,553 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,554 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,555 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:41,569 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:41,577 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:41,583 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:41,588 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:50:41,589 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:41,592 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:41,593 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:41,594 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,595 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:41,596 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:41,601 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:41,604 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:41,606 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:41,610 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:41,611 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:41,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:41,614 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:41,615 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,616 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:41,617 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:41,631 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:41,645 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:41,655 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:41,664 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:41,666 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:41,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:41,675 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:41,676 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:41,677 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:41,678 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:41,680 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:41,682 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:41,683 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:41,685 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:41,685 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:41,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:41,689 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:41,693 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 17])", "<class 'int'>: 16")
2023-10-11 12:50:41,694 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:41,695 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:41,697 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:41,699 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:41,703 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:41,705 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:41,706 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:41,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:41,715 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:41,721 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,722 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,723 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:41,757 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:41,766 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:41,773 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:41,792 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:50:41,793 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:41,796 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:41,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:41,807 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,808 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,809 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:41,816 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:41,821 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:41,827 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:41,835 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:50:41,836 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:41,838 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:41,842 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:41,847 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,848 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,849 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:41,857 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:41,863 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:41,868 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:41,873 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:50:41,874 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:41,877 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:41,881 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:41,886 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,887 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,888 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:41,898 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:41,910 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:41,918 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:41,926 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:50:41,927 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:41,929 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:41,933 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:41,938 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,939 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,939 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:41,946 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:41,952 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:41,961 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:41,966 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:50:41,967 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:41,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:41,975 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:41,980 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:41,980 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:41,981 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:41,988 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:41,994 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:42,000 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:42,006 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:50:42,007 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:42,009 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:42,014 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:42,019 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,020 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,021 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:42,028 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:42,039 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:42,055 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:42,062 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:50:42,063 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:42,065 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:42,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:42,075 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,076 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,077 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:42,090 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:42,110 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:42,115 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:42,120 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:50:42,121 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:42,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:42,128 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:42,134 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,134 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,135 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:42,141 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:42,149 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:42,153 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:42,159 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:50:42,160 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:42,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:42,167 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:42,172 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,173 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,174 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:42,180 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:42,185 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:42,189 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:42,195 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:50:42,195 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:42,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:42,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:42,206 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,206 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,207 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:42,214 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:42,220 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:42,226 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:42,230 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:50:42,231 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:42,233 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:42,238 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:42,239 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,240 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,241 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:42,248 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:42,253 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:42,263 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:42,272 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:50:42,275 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:42,278 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:42,280 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:42,281 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,282 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:42,283 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:42,290 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:42,295 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:42,298 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:42,302 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:42,303 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:42,304 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:42,305 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:42,306 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,308 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:42,308 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:42,328 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:42,353 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:42,369 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:42,384 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:42,385 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:42,432 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:42,433 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:42,435 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:42,436 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:42,437 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:42,438 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:42,440 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:42,442 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:42,443 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:42,444 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:42,446 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:42,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:42,452 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 18])", "<class 'int'>: 17")
2023-10-11 12:50:42,453 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:42,454 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:42,456 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:42,457 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:42,459 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:42,461 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:42,462 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:42,466 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:42,470 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:42,475 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,476 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,477 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:42,483 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:42,489 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:42,494 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:42,502 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:50:42,503 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:42,505 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:42,509 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:42,514 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,515 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,516 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:42,522 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:42,527 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:42,534 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:42,549 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:50:42,551 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:42,553 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:42,558 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:42,562 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,563 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,564 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:42,574 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:42,584 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:42,592 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:42,597 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:50:42,598 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:42,601 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:42,605 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:42,610 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,610 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,611 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:42,617 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:42,630 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:42,635 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:42,642 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:50:42,643 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:42,645 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:42,650 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:42,656 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,657 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,658 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:42,666 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:42,675 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:42,681 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:42,688 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:50:42,689 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:42,692 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:42,697 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:42,702 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,703 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,704 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:42,713 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:42,725 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:42,734 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:42,742 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:50:42,743 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:42,745 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:42,750 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:42,754 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,755 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,756 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:42,763 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:42,774 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:42,779 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:42,784 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:50:42,786 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:42,788 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:42,792 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:42,797 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,798 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,799 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:42,873 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:42,905 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:42,913 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:42,919 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:50:42,920 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:42,923 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:42,928 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:42,933 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,934 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,935 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:42,941 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:42,949 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:42,956 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:42,962 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:50:42,963 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:42,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:42,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:42,974 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:42,975 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:42,976 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:42,983 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:42,989 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:42,996 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:43,003 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:50:43,004 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:43,007 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:43,011 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:43,016 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,017 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:43,019 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:43,026 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:43,031 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:43,038 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:43,044 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:50:43,045 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:43,047 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:43,052 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:43,053 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,054 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:43,055 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:43,062 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:43,067 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:43,073 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:43,078 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:50:43,079 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:43,081 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:43,082 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:43,083 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,084 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:43,085 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:43,087 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:43,092 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:43,094 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:43,095 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:43,096 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:43,098 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:43,099 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:43,100 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,101 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:43,102 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:43,118 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:43,129 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:43,140 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:43,150 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:43,152 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:43,164 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:43,165 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:43,167 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:43,168 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:43,168 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:43,171 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:43,172 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:43,174 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:43,175 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:43,176 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:43,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:43,179 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:43,184 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 19])", "<class 'int'>: 18")
2023-10-11 12:50:43,185 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:43,186 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:43,188 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:43,189 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:43,191 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:43,193 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:43,193 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:43,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:43,202 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:43,207 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,208 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:43,209 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:43,216 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:43,223 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:43,230 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:43,238 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:50:43,239 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:43,241 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:43,245 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:43,250 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,251 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:43,251 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:43,261 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:43,271 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:43,279 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:43,286 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:50:43,287 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:43,290 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:43,295 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:43,300 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,300 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:43,301 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:43,310 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:43,316 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:43,322 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:43,334 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:50:43,335 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:43,337 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:43,342 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:43,346 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,347 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:43,348 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:43,354 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:43,359 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:43,368 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:43,374 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:50:43,375 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:43,378 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:43,382 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:43,387 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,393 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:43,394 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:43,401 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:43,410 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:43,462 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:43,498 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:50:43,499 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:43,502 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:43,507 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:43,511 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,512 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:43,513 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:43,519 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:43,525 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:43,530 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:43,535 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:50:43,538 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:43,542 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:43,551 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:43,560 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,562 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:43,563 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:43,573 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:43,586 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:43,594 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:43,599 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:50:43,600 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:43,602 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:43,607 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:43,611 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,612 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:43,613 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:43,620 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:43,630 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:43,637 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:43,645 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:50:43,645 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:43,648 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:43,652 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:43,657 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,658 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:43,659 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:43,666 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:43,672 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:43,677 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:43,687 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:50:43,688 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:43,690 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:43,694 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:43,699 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,699 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:43,700 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:43,711 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:43,751 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:43,767 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:43,776 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:50:43,777 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:43,780 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:43,786 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:43,790 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,791 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:43,792 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:43,799 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:43,813 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:43,818 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:43,824 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:50:43,826 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:43,828 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:43,834 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:43,835 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,837 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:43,837 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:43,874 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:43,881 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:43,887 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:43,896 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:50:43,897 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:43,900 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:43,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:43,903 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,904 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:43,904 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:43,908 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:43,911 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:43,913 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:43,914 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:43,916 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:43,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:43,918 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:43,919 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:43,920 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:43,921 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:43,932 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:43,942 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:43,955 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:43,965 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:43,966 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:43,982 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:43,983 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:43,984 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:43,985 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:43,986 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:43,988 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:43,989 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:43,990 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:43,992 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:43,992 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:43,994 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:43,995 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:44,000 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 20])", "<class 'int'>: 19")
2023-10-11 12:50:44,001 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:44,001 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:44,003 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:44,005 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:44,006 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:44,008 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:44,008 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:44,012 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:44,016 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:44,021 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,022 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,023 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:44,029 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:44,038 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:44,043 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:44,049 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:50:44,049 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:44,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:44,056 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:44,060 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,061 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,062 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:44,068 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:44,074 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:44,079 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:44,083 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:50:44,084 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:44,086 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:44,090 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:44,094 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,095 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,096 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:44,103 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:44,114 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:44,121 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:44,130 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:50:44,131 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:44,133 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:44,138 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:44,142 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,143 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,144 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:44,150 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:44,160 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:44,168 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:44,177 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:50:44,178 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:44,180 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:44,185 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:44,190 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,191 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,192 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:44,198 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:44,203 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:44,209 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:44,216 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:50:44,217 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:44,220 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:44,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:44,229 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,229 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,230 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:44,237 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:44,254 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:44,268 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:44,275 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:50:44,275 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:44,278 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:44,282 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:44,287 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,288 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,289 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:44,300 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:44,305 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:44,311 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:44,317 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:50:44,318 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:44,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:44,325 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:44,329 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,330 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,331 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:44,337 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:44,346 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:44,351 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:44,356 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:50:44,357 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:44,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:44,364 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:44,369 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,370 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,370 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:44,377 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:44,385 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:44,392 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:44,402 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:50:44,404 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:44,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:44,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:44,417 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,418 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,419 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:44,443 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:44,450 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:44,459 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:44,465 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:50:44,466 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:44,468 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:44,473 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:44,478 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,479 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,480 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:44,489 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:44,508 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:44,513 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:44,518 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:50:44,519 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:44,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:44,526 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:44,527 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,528 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,529 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:44,536 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:44,543 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:44,551 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:44,556 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:50:44,557 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:44,560 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:44,562 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:44,563 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,564 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:44,565 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:44,567 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:44,570 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:44,573 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:44,575 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:44,576 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:44,578 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:44,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:44,581 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,581 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:44,582 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:44,600 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:44,610 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:44,620 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:44,632 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:44,634 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:44,642 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:44,643 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:44,645 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:44,645 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:44,646 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:44,648 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:44,649 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:44,650 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:44,652 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:44,652 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:44,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:44,655 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:44,660 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 21])", "<class 'int'>: 20")
2023-10-11 12:50:44,660 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:44,661 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:44,663 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:44,665 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:44,666 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:44,668 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:44,669 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:44,673 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:44,678 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:44,683 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,684 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,685 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:44,697 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:44,703 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:44,709 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:44,716 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:50:44,717 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:44,720 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:44,725 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:44,730 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,731 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,732 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:44,744 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:44,751 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:44,762 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:44,768 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:50:44,769 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:44,772 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:44,777 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:44,782 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,783 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,784 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:44,795 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:44,804 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:44,816 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:44,825 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:50:44,826 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:44,830 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:44,835 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:44,840 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,841 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,842 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:44,855 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:44,870 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:44,886 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:44,893 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:50:44,894 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:44,897 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:44,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:44,907 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,909 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,909 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:44,927 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:44,933 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:44,942 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:44,947 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:50:44,948 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:44,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:44,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:44,960 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:44,961 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:44,962 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:44,969 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:45,004 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:45,010 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:45,016 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:50:45,017 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:45,019 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:45,024 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:45,029 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,030 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,031 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:45,037 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:45,043 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:45,058 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:45,063 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:50:45,064 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:45,066 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:45,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:45,075 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,076 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,076 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:45,083 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:45,088 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:45,093 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:45,099 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:50:45,100 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:45,103 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:45,107 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:45,112 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,113 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,114 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:45,120 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:45,128 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:45,134 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:45,141 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:50:45,141 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:45,144 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:45,148 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:45,153 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,154 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,155 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:45,162 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:45,167 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:45,177 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:45,186 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:50:45,186 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:45,189 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:45,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:45,198 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,199 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,200 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:45,209 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:45,215 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:45,220 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:45,225 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:50:45,226 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:45,228 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:45,233 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:45,235 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,235 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,236 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:45,250 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:45,261 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:45,266 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:45,272 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:50:45,273 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:45,276 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:45,278 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:45,280 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,291 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:45,291 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:45,295 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:45,302 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:45,311 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:45,318 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:45,319 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:45,321 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:45,322 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:45,324 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,324 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:45,325 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:45,346 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:45,360 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:45,376 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:45,390 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:45,391 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:45,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:45,415 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:45,416 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:45,417 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:45,418 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:45,420 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:45,422 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:45,423 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:45,425 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:45,425 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:45,427 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:45,429 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:45,434 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 22])", "<class 'int'>: 21")
2023-10-11 12:50:45,435 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:45,436 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:45,437 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:45,439 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:45,441 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:45,442 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:45,443 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:45,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:45,452 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:45,456 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,457 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,458 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:45,467 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:45,472 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:45,477 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:45,485 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:50:45,486 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:45,488 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:45,493 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:45,498 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,498 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,499 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:45,513 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:45,519 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:45,528 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:45,535 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:50:45,536 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:45,539 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:45,544 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:45,549 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,550 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,550 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:45,557 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:45,563 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:45,571 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:45,580 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:50:45,580 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:45,583 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:45,587 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:45,592 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,593 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,594 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:45,603 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:45,612 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:45,619 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:45,625 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:50:45,626 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:45,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:45,633 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:45,638 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,639 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,640 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:45,655 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:45,670 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:45,676 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:45,747 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:50:45,748 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:45,750 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:45,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:45,764 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,765 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,766 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:45,796 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:45,811 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:45,817 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:45,827 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:50:45,828 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:45,830 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:45,835 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:45,840 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,841 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,842 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:45,849 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:45,858 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:45,866 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:45,871 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:50:45,872 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:45,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:45,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:45,885 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,885 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,886 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:45,893 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:45,902 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:45,908 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:45,913 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:50:45,914 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:45,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:45,921 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:45,926 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,927 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,928 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:45,939 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:45,946 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:45,952 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:45,962 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:50:45,963 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:45,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:45,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:45,974 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:45,975 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:45,976 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:45,988 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:45,993 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:45,998 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:46,004 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:50:46,005 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:46,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:46,013 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:46,017 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,018 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:46,019 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:46,034 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:46,040 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:46,045 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:46,050 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:50:46,051 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:46,053 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:46,057 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:46,059 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,060 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:46,061 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:46,071 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:46,076 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:46,083 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:46,088 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:50:46,088 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:46,091 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:46,092 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:46,094 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,095 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:46,095 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:46,098 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:46,102 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:46,105 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:46,108 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:46,109 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:46,111 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:46,112 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:46,113 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,114 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:46,115 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:46,127 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:46,137 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:46,149 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:46,160 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:46,161 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:46,216 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:46,218 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:46,220 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:46,220 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:46,221 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:46,223 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:46,225 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:46,226 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:46,228 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:46,228 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:46,230 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:46,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:46,236 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 23])", "<class 'int'>: 22")
2023-10-11 12:50:46,237 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:46,238 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:46,240 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:46,242 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:46,244 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:46,248 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:46,249 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:46,254 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:46,259 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:46,264 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,264 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:46,265 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:46,284 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:46,292 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:46,297 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:46,304 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:50:46,305 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:46,307 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:46,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:46,317 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,318 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:46,319 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:46,326 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:46,336 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:46,343 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:46,351 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:50:46,352 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:46,354 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:46,359 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:46,364 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,365 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:46,365 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:46,432 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:46,448 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:46,455 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:46,461 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:50:46,462 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:46,464 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:46,468 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:46,473 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,474 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:46,475 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:46,488 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:46,496 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:46,501 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:46,510 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:50:46,511 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:46,513 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:46,517 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:46,522 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,523 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:46,524 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:46,531 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:46,542 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:46,547 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:46,555 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:50:46,556 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:46,559 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:46,563 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:46,567 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,568 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:46,569 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:46,575 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:46,592 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:46,597 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:46,603 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:50:46,604 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:46,607 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:46,611 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:46,616 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,617 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:46,618 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:46,624 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:46,630 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:46,637 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:46,642 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:50:46,643 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:46,645 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:46,651 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:46,656 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,657 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:46,658 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:46,665 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:46,672 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:46,677 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:46,682 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:50:46,683 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:46,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:46,690 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:46,695 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,696 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:46,697 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:46,705 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:46,717 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:46,724 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:46,734 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:50:46,735 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:46,738 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:46,742 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:46,747 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,748 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:46,749 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:46,756 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:46,774 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:46,789 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:46,795 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:50:46,796 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:46,798 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:46,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:46,807 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,808 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:46,809 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:46,817 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:46,822 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:46,828 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:46,838 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:50:46,839 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:46,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:46,846 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:46,848 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,848 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:46,849 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:46,862 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:46,869 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:46,874 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:46,879 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:50:46,880 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:46,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:46,884 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:46,886 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,886 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:46,887 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:46,889 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:46,891 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:46,892 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:46,894 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:46,895 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:46,897 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:46,898 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:46,899 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,900 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:46,901 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:46,911 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:46,923 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:46,935 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:46,946 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:46,947 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:46,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:46,957 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:46,958 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:46,959 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:46,960 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:46,962 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:46,963 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:46,965 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:46,966 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:46,967 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:46,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:46,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:46,975 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 24])", "<class 'int'>: 23")
2023-10-11 12:50:46,976 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:46,976 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:46,978 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:46,980 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:46,982 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:46,983 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:46,984 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:46,988 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:46,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:46,998 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:46,999 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,000 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:47,012 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:47,029 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:47,036 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:47,041 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:50:47,042 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:47,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:47,050 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:47,054 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,055 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,056 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:47,067 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:47,080 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:47,085 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:47,090 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:50:47,091 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:47,093 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:47,098 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:47,104 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,104 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,105 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:47,113 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:47,122 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:47,161 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:47,227 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:50:47,228 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:47,230 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:47,234 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:47,238 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,239 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,240 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:47,247 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:47,253 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:47,269 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:47,278 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:50:47,279 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:47,281 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:47,285 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:47,291 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,292 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,292 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:47,299 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:47,306 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:47,314 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:47,320 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:50:47,320 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:47,323 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:47,327 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:47,332 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,333 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,334 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:47,342 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:47,347 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:47,352 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:47,358 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:50:47,359 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:47,361 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:47,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:47,370 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,371 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,372 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:47,382 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:47,387 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:47,394 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:47,400 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:50:47,400 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:47,403 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:47,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:47,412 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,413 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,414 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:47,420 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:47,426 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:47,431 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:47,437 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:50:47,439 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:47,440 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:47,445 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:47,450 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,451 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,452 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:47,458 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:47,464 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:47,472 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:47,477 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:50:47,479 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:47,481 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:47,485 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:47,490 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,498 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,499 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:47,506 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:47,548 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:47,562 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:47,593 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:50:47,594 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:47,596 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:47,600 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:47,605 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,606 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,607 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:47,615 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:47,620 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:47,624 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:47,629 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:50:47,630 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:47,632 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:47,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:47,638 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,639 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,640 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:47,646 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:47,651 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:47,661 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:47,666 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:50:47,666 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:47,668 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:47,670 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:47,671 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,672 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:47,673 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:47,676 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:47,678 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:47,680 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:47,682 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:47,683 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:47,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:47,686 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:47,688 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,689 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:47,689 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:47,707 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:47,717 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:47,731 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:47,745 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:47,746 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:47,753 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:47,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:47,756 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:47,757 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:47,758 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:47,759 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:47,764 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:47,765 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:47,767 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:47,768 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:47,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:47,770 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:47,775 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 25])", "<class 'int'>: 24")
2023-10-11 12:50:47,776 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:47,777 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:47,779 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:47,780 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:47,782 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:47,783 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:47,784 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:47,788 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:47,793 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:47,797 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,799 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,800 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:47,850 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:47,861 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:47,866 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:47,872 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:50:47,873 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:47,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:47,879 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:47,884 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,885 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,886 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:47,899 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:47,907 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:47,913 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:47,927 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:50:47,928 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:47,931 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:47,935 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:47,940 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,941 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,942 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:47,956 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:47,962 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:47,972 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:47,980 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:50:47,981 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:47,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:47,989 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:47,993 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:47,994 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:47,995 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:48,003 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:48,008 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:48,014 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:48,030 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:50:48,031 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:48,033 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:48,038 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:48,043 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,044 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,044 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:48,051 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:48,057 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:48,062 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:48,080 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:50:48,081 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:48,084 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:48,089 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:48,094 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,095 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,096 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:48,103 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:48,111 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:48,116 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:48,122 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:50:48,123 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:48,125 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:48,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:48,135 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,136 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,137 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:48,144 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:48,153 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:48,184 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:48,199 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:50:48,200 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:48,203 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:48,208 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:48,212 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,213 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,214 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:48,225 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:48,233 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:48,239 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:48,246 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:50:48,247 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:48,249 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:48,254 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:48,260 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,261 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,262 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:48,270 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:48,299 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:48,315 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:48,329 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:50:48,330 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:48,332 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:48,337 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:48,342 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,343 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,344 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:48,355 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:48,364 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:48,376 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:48,382 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:50:48,383 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:48,386 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:48,391 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:48,396 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,396 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,397 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:48,405 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:48,414 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:48,424 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:48,433 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:50:48,433 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:48,436 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:48,440 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:48,442 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,443 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,444 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:48,451 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:48,458 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:48,464 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:48,471 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:50:48,472 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:48,475 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:48,476 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:48,477 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,478 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:48,479 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:48,483 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:48,486 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:48,488 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:48,490 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:48,491 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:48,492 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:48,494 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:48,495 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,496 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:48,496 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:48,514 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:48,528 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:48,538 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:48,549 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:48,551 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:48,560 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:48,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:48,563 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:48,564 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:48,564 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:48,566 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:48,568 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:48,569 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:48,571 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:48,572 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:48,573 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:48,574 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:48,579 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 26])", "<class 'int'>: 25")
2023-10-11 12:50:48,580 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:48,581 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:48,583 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:48,584 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:48,586 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:48,587 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:48,588 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:48,592 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:48,596 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:48,601 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,602 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,603 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:48,609 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:48,614 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:48,622 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:48,630 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:50:48,631 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:48,634 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:48,638 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:48,643 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,644 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,645 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:48,655 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:48,662 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:48,678 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:48,686 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:50:48,687 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:48,689 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:48,693 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:48,698 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,699 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,700 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:48,711 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:48,718 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:48,728 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:48,733 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:50:48,734 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:48,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:48,741 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:48,746 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,746 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,747 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:48,759 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:48,770 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:48,788 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:48,794 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:50:48,795 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:48,797 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:48,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:48,807 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,808 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,809 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:48,816 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:48,826 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:48,847 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:48,852 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:50:48,853 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:48,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:48,860 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:48,865 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,866 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,867 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:48,875 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:48,882 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:48,888 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:48,893 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:50:48,894 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:48,896 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:48,901 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:48,906 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,906 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,907 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:48,915 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:48,926 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:48,931 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:48,937 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:50:48,938 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:48,941 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:48,946 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:48,951 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,952 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,953 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:48,960 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:48,965 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:48,977 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:48,983 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:50:48,983 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:48,986 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:48,990 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:48,995 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:48,996 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:48,997 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:49,003 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:49,009 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:49,022 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:49,027 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:50:49,029 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:49,031 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:49,036 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:49,040 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,041 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,042 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:49,049 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:49,056 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:49,061 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:49,066 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:50:49,067 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:49,069 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:49,073 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:49,078 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,079 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,080 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:49,086 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:49,098 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:49,103 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:49,110 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:50:49,111 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:49,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:49,118 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:49,120 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,120 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,121 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:49,128 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:49,133 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:49,141 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:49,151 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:50:49,152 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:49,154 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:49,156 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:49,157 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,158 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:49,159 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:49,161 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:49,167 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:49,170 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:49,172 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:49,173 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:49,175 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:49,177 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:49,178 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,179 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:49,179 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:49,190 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:49,205 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:49,217 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:49,227 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:49,229 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:49,235 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:49,236 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:49,238 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:49,239 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:49,240 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:49,241 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:49,243 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:49,244 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:49,245 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:49,246 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:49,248 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:49,249 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:49,254 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 27])", "<class 'int'>: 26")
2023-10-11 12:50:49,255 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:49,256 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:49,257 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:49,259 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:49,261 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:49,263 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:49,264 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:49,268 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:49,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:49,278 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,279 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,280 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:49,324 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:49,334 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:49,339 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:49,345 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:50:49,346 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:49,348 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:49,352 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:49,359 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,361 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,362 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:49,372 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:49,381 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:49,386 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:49,396 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:50:49,397 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:49,400 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:49,405 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:49,409 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,410 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,411 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:49,418 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:49,424 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:49,429 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:49,435 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:50:49,436 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:49,438 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:49,442 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:49,447 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,448 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,448 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:49,464 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:49,469 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:49,474 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:49,480 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:50:49,481 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:49,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:49,488 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:49,493 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,494 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,494 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:49,502 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:49,547 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:49,610 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:49,626 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:50:49,628 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:49,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:49,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:49,641 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,642 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,642 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:49,655 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:49,660 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:49,665 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:49,671 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:50:49,672 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:49,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:49,678 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:49,683 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,684 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,686 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:49,692 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:49,702 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:49,708 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:49,714 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:50:49,714 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:49,717 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:49,721 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:49,726 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,727 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,727 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:49,734 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:49,739 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:49,745 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:49,751 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:50:49,752 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:49,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:49,758 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:49,763 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,764 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,765 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:49,776 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:49,782 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:49,787 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:49,792 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:50:49,793 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:49,795 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:49,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:49,804 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,805 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,806 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:49,813 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:49,820 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:49,826 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:49,833 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:50:49,833 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:49,836 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:49,840 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:49,845 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,846 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,847 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:49,860 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:49,868 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:49,877 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:49,888 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:50:49,888 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:49,891 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:49,896 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:49,897 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:49,898 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:49,899 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:49,944 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:49,999 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:50,015 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:50,039 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:50:50,040 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:50,043 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:50,044 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:50,045 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,046 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:50,047 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:50,051 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:50,053 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:50,055 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:50,058 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:50,059 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:50,060 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:50,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:50,063 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,064 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:50,065 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:50,076 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:50,086 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:50,101 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:50,117 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:50,118 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:50,126 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:50,127 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:50,128 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:50,129 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:50,130 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:50,132 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:50,134 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:50,135 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:50,137 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:50,137 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:50,139 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:50,140 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:50,145 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 28])", "<class 'int'>: 27")
2023-10-11 12:50:50,146 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:50,147 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:50,149 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:50,151 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:50,152 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:50,153 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:50,154 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:50,159 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:50,163 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:50,168 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,169 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:50,170 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:50,177 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:50,190 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:50,197 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:50,203 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:50:50,204 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:50,207 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:50,211 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:50,215 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,216 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:50,217 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:50,225 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:50,232 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:50,238 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:50,244 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:50:50,245 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:50,248 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:50,252 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:50,257 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,258 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:50,259 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:50,267 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:50,278 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:50,291 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:50,370 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:50:50,371 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:50,374 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:50,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:50,384 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,385 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:50,386 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:50,393 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:50,399 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:50,418 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:50,423 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:50:50,424 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:50,427 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:50,431 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:50,436 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,437 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:50,438 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:50,444 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:50,450 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:50,462 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:50,470 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:50:50,471 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:50,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:50,478 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:50,483 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,483 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:50,484 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:50,492 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:50,503 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:50,560 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:50,587 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:50:50,589 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:50,591 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:50,595 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:50,600 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,601 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:50,602 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:50,612 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:50,622 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:50,640 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:50,647 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:50:50,648 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:50,651 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:50,656 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:50,660 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,661 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:50,662 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:50,669 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:50,674 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:50,681 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:50,688 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:50:50,689 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:50,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:50,695 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:50,700 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,701 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:50,701 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:50,708 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:50,714 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:50,719 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:50,725 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:50:50,726 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:50,728 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:50,733 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:50,737 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,738 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:50,739 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:50,747 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:50,753 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:50,759 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:50,765 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:50:50,766 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:50,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:50,774 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:50,779 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,780 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:50,781 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:50,787 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:50,796 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:50,807 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:50,816 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:50:50,817 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:50,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:50,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:50,825 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,826 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:50,827 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:50,833 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:50,839 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:50,848 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:50,853 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:50:50,854 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:50,857 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:50,858 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:50,859 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,860 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:50,861 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:50,863 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:50,865 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:50,867 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:50,869 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:50,869 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:50,871 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:50,872 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:50,873 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,874 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:50,875 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:50,893 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:50,905 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:50,915 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:50,926 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:50,927 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:50,937 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:50,939 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:50,941 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:50,942 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:50,942 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:50,944 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:50,946 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:50,947 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:50,949 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:50,949 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:50,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:50,952 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:50,957 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 29])", "<class 'int'>: 28")
2023-10-11 12:50:50,958 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:50,959 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:50,961 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:50,962 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:50,964 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:50,965 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:50,966 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:50,970 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:50,975 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:50,980 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:50,981 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:50,982 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:50,994 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:51,021 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:51,033 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:51,043 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:50:51,044 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:51,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:51,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:51,056 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,057 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:51,058 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:51,066 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:51,078 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:51,086 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:51,102 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:50:51,103 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:51,105 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:51,109 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:51,114 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,115 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:51,115 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:51,122 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:51,150 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:51,156 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:51,165 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:50:51,166 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:51,168 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:51,173 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:51,178 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,179 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:51,180 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:51,187 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:51,195 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:51,202 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:51,210 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:50:51,212 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:51,214 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:51,219 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:51,224 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,226 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:51,227 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:51,237 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:51,256 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:51,269 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:51,274 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:50:51,276 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:51,278 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:51,282 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:51,287 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,288 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:51,289 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:51,297 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:51,303 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:51,309 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:51,327 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:50:51,328 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:51,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:51,338 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:51,344 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,345 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:51,346 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:51,356 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:51,363 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:51,379 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:51,386 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:50:51,387 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:51,390 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:51,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:51,399 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,400 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:51,401 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:51,418 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:51,427 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:51,435 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:51,441 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:50:51,442 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:51,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:51,448 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:51,453 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,454 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:51,455 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:51,463 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:51,471 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:51,477 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:51,482 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:50:51,483 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:51,485 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:51,490 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:51,494 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,496 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:51,496 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:51,503 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:51,511 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:51,517 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:51,562 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:50:51,564 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:51,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:51,576 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:51,581 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,582 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:51,584 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:51,638 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:51,646 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:51,658 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:51,664 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:50:51,665 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:51,667 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:51,671 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:51,673 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,674 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:51,675 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:51,682 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:51,691 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:51,697 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:51,710 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:50:51,711 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:51,713 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:51,715 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:51,716 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,717 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:51,717 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:51,720 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:51,721 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:51,724 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:51,726 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:51,727 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:51,728 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:51,729 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:51,731 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,732 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:51,732 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:51,745 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:51,756 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:51,770 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:51,781 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:51,782 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:51,792 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:51,794 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:51,795 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:51,796 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:51,797 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:51,798 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:51,800 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:51,801 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:51,803 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:51,803 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:51,805 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:51,807 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:51,811 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 30])", "<class 'int'>: 29")
2023-10-11 12:50:51,812 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:51,813 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:51,815 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:51,816 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:51,818 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:51,819 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:51,820 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:51,824 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:51,829 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:51,834 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,834 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:51,835 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:51,842 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:51,848 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:51,878 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:51,887 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:50:51,888 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:51,891 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:51,895 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:51,900 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,901 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:51,902 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:51,913 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:51,925 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:51,934 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:51,940 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:50:51,941 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:51,943 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:51,948 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:51,953 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:51,953 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:51,954 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:51,967 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:51,975 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:51,982 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:51,988 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:50:51,989 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:51,992 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:51,996 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:52,002 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,003 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:52,004 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:52,011 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:52,020 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:52,027 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:52,106 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:50:52,108 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:52,111 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:52,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:52,122 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,123 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:52,124 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:52,135 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:52,146 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:52,155 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:52,165 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:50:52,166 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:52,169 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:52,173 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:52,178 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,179 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:52,180 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:52,195 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:52,202 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:52,213 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:52,220 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:50:52,221 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:52,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:52,228 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:52,233 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,234 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:52,235 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:52,242 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:52,250 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:52,264 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:52,270 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:50:52,271 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:52,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:52,278 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:52,282 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,283 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:52,284 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:52,291 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:52,296 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:52,302 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:52,309 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:50:52,310 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:52,313 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:52,317 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:52,322 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,323 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:52,324 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:52,330 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:52,337 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:52,343 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:52,349 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:50:52,350 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:52,352 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:52,356 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:52,361 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,362 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:52,363 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:52,394 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:52,403 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:52,410 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:52,423 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:50:52,424 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:52,426 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:52,431 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:52,435 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,436 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:52,437 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:52,508 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:52,515 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:52,520 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:52,525 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:50:52,526 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:52,528 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:52,533 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:52,535 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,536 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:52,537 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:52,544 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:52,551 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:52,559 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:52,566 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:50:52,567 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:52,569 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:52,571 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:52,573 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,573 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:52,574 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:52,576 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:52,578 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:52,579 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:52,581 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:52,582 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:52,584 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:52,585 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:52,586 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,587 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:52,587 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:52,599 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:52,613 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:52,625 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:52,635 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:52,636 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:52,645 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:52,646 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:52,648 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:52,649 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:52,649 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:52,651 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:52,652 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:52,654 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:52,655 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:52,656 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:52,658 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:52,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:52,664 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 31])", "<class 'int'>: 30")
2023-10-11 12:50:52,665 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:52,666 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:52,668 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:52,669 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:52,671 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:52,672 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:52,673 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:52,677 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:52,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:52,686 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,687 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:52,688 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:52,695 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:52,702 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:52,707 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:52,714 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:50:52,715 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:52,718 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:52,723 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:52,728 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,728 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:52,729 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:52,739 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:52,746 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:52,755 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:52,761 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:50:52,761 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:52,766 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:52,775 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:52,788 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,789 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:52,790 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:52,830 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:52,836 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:52,841 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:52,897 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:50:52,898 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:52,901 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:52,905 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:52,910 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,911 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:52,912 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:52,920 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:52,932 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:52,939 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:52,945 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:50:52,946 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:52,949 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:52,954 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:52,959 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:52,960 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:52,961 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:52,968 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:52,975 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:52,980 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:52,990 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:50:52,991 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:52,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:52,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:53,002 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,003 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,004 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:53,014 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:53,019 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:53,025 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:53,031 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:50:53,032 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:53,034 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:53,039 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:53,044 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,045 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,045 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:53,052 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:53,058 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:53,074 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:53,080 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:50:53,081 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:53,083 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:53,088 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:53,094 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,095 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,096 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:53,119 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:53,127 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:53,134 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:53,139 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:50:53,140 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:53,142 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:53,147 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:53,151 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,152 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,153 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:53,160 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:53,166 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:53,172 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:53,178 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:50:53,180 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:53,182 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:53,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:53,191 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,192 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,193 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:53,202 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:53,208 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:53,221 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:53,227 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:50:53,229 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:53,231 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:53,236 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:53,241 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,242 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,243 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:53,250 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:53,262 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:53,274 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:53,280 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:50:53,281 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:53,283 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:53,288 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:53,290 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,291 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,291 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:53,301 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:53,307 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:53,315 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:53,322 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:50:53,323 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:53,325 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:53,327 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:53,328 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,329 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:53,330 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:53,332 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:53,334 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:53,338 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:53,340 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:53,341 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:53,342 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:53,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:53,345 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,346 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:53,347 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:53,361 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:53,373 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:53,383 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:53,394 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:53,395 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:53,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:53,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:53,405 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:53,407 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:53,407 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:53,409 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:53,410 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:53,412 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:53,413 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:53,414 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:53,415 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:53,416 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:53,421 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 32])", "<class 'int'>: 31")
2023-10-11 12:50:53,422 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:53,423 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:53,424 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:53,426 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:53,427 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:53,429 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:53,431 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:53,436 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:53,440 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:53,445 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,446 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,447 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:53,454 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:53,460 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:53,470 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:53,480 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:50:53,480 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:53,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:53,487 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:53,492 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,493 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,494 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:53,502 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:53,515 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:53,521 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:53,527 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:50:53,527 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:53,530 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:53,535 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:53,540 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,540 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,541 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:53,548 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:53,556 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:53,562 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:53,571 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:50:53,572 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:53,574 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:53,580 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:53,585 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,587 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,593 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:53,618 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:53,630 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:53,636 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:53,648 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:50:53,649 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:53,652 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:53,657 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:53,662 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,663 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,664 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:53,740 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:53,791 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:53,797 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:53,806 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:50:53,807 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:53,809 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:53,813 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:53,818 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,819 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,820 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:53,827 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:53,834 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:53,840 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:53,845 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:50:53,846 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:53,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:53,853 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:53,858 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,859 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,860 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:53,867 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:53,873 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:53,882 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:53,888 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:50:53,889 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:53,891 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:53,896 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:53,900 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,901 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,902 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:53,909 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:53,915 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:53,921 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:53,926 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:50:53,927 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:53,929 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:53,933 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:53,938 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:53,939 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:53,940 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:54,019 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:54,029 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:54,035 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:54,041 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:50:54,041 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:54,044 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:54,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:54,054 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,055 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,055 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:54,071 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:54,079 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:54,086 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:54,092 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:50:54,093 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:54,095 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:54,099 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:54,105 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,105 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,106 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:54,113 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:54,119 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:54,126 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:54,134 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:50:54,135 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:54,137 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:54,141 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:54,143 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,144 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,145 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:54,151 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:54,157 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:54,162 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:54,168 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:50:54,169 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:54,171 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:54,173 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:54,174 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,175 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:54,175 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:54,178 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:54,182 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:54,186 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:54,188 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:54,189 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:54,190 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:54,192 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:54,193 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,194 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:54,194 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:54,210 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:54,223 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:54,232 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:54,244 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:54,246 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:54,254 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:54,255 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:54,257 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:54,258 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:54,259 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:54,261 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:54,263 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:54,267 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:54,269 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:54,270 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:54,271 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:54,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:54,277 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 33])", "<class 'int'>: 32")
2023-10-11 12:50:54,278 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:54,279 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:54,281 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:54,283 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:54,285 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:54,286 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:54,287 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:54,291 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:54,295 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:54,300 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,301 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,302 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:54,312 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:54,318 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:54,326 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:54,332 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:50:54,333 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:54,335 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:54,340 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:54,345 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,346 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,347 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:54,358 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:54,366 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:54,381 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:54,390 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:50:54,391 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:54,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:54,399 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:54,403 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,404 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,405 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:54,416 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:54,423 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:54,429 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:54,436 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:50:54,437 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:54,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:54,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:54,449 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,450 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,450 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:54,467 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:54,475 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:54,494 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:54,502 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:50:54,502 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:54,505 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:54,509 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:54,514 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,515 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,516 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:54,523 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:54,530 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:54,566 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:54,583 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:50:54,584 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:54,586 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:54,591 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:54,596 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,597 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,597 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:54,608 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:54,615 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:54,621 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:54,627 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:50:54,628 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:54,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:54,638 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:54,643 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,644 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,645 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:54,659 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:54,671 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:54,676 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:54,682 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:50:54,683 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:54,685 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:54,690 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:54,696 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,697 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,698 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:54,705 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:54,713 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:54,723 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:54,730 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:50:54,731 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:54,733 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:54,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:54,742 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,743 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,744 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:54,750 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:54,756 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:54,762 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:54,769 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:50:54,770 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:54,773 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:54,778 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:54,783 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,784 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,786 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:54,797 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:54,821 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:54,832 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:54,839 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:50:54,840 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:54,843 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:54,849 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:54,854 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,855 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,856 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:54,868 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:54,878 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:54,886 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:54,893 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:50:54,894 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:54,898 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:54,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:54,905 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,906 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:54,907 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:54,915 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:54,927 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:54,938 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:54,946 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:50:54,947 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:54,949 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:54,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:54,952 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,953 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:54,954 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:54,958 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:54,977 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:54,982 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:54,986 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:54,988 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:54,989 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:54,991 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:54,992 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:54,993 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:54,994 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:55,012 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:55,028 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:55,037 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:55,056 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:55,058 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:55,088 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:55,089 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:55,091 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:55,092 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:55,093 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:55,095 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:55,096 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:55,098 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:55,100 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:55,101 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:55,103 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:55,104 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:55,110 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 34])", "<class 'int'>: 33")
2023-10-11 12:50:55,110 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:55,111 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:55,113 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:55,115 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:55,117 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:55,118 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:55,119 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:55,123 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:55,128 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:55,133 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,134 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,134 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:55,142 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:55,150 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:55,157 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:55,166 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:50:55,167 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:55,169 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:55,174 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:55,179 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,180 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,181 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:55,188 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:55,195 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:55,203 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:55,208 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:50:55,209 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:55,212 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:55,216 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:55,221 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,222 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,223 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:55,230 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:55,238 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:55,244 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:55,251 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:50:55,252 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:55,254 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:55,259 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:55,263 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,264 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,265 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:55,272 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:55,278 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:55,287 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:55,295 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:50:55,296 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:55,299 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:55,303 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:55,308 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,309 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,310 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:55,324 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:55,332 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:55,338 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:55,350 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:50:55,351 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:55,354 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:55,358 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:55,363 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,364 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,365 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:55,372 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:55,381 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:55,388 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:55,394 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:50:55,395 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:55,397 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:55,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:55,408 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,409 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,409 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:55,422 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:55,431 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:55,437 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:55,516 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:50:55,517 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:55,519 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:55,524 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:55,529 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,530 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,531 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:55,543 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:55,550 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:55,556 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:55,568 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:50:55,569 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:55,572 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:55,576 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:55,582 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,583 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,584 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:55,600 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:55,606 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:55,614 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:55,620 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:50:55,621 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:55,623 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:55,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:55,633 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,634 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,635 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:55,642 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:55,649 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:55,656 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:55,662 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:50:55,663 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:55,665 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:55,669 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:55,674 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,675 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,676 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:55,683 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:55,691 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:55,696 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:55,702 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:50:55,703 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:55,705 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:55,710 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:55,711 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,712 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,713 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:55,722 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:55,728 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:55,734 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:55,740 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:50:55,740 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:55,743 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:55,745 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:55,746 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,747 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:55,748 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:55,750 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:55,753 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:55,756 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:55,760 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:55,761 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:55,763 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:55,764 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:55,765 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,766 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:55,767 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:55,780 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:55,793 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:55,804 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:55,814 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:55,815 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:55,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:55,825 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:55,826 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:55,827 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:55,827 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:55,829 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:55,831 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:55,832 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:55,833 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:55,834 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:55,835 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:55,837 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:55,841 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 35])", "<class 'int'>: 34")
2023-10-11 12:50:55,842 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:55,843 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:55,845 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:55,847 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:55,848 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:55,850 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:55,851 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:55,855 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:55,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:55,864 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,865 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,866 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:55,881 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:55,886 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:55,894 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:55,907 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:50:55,908 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:55,912 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:55,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:55,922 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,924 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,925 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:55,933 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:55,943 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:55,951 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:55,957 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:50:55,958 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:55,960 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:55,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:55,970 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:55,971 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:55,972 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:56,046 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:56,059 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:56,097 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:56,107 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:50:56,108 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:56,111 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:56,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:56,121 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:56,122 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:56,123 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:56,138 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:56,147 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:56,208 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:56,278 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:50:56,280 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:56,282 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:56,287 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:56,292 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:56,293 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:56,294 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:56,302 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:56,307 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:56,313 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:56,319 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:50:56,320 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:56,323 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:56,328 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:56,333 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:56,333 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:56,334 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:56,342 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:56,352 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:56,360 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:56,365 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:50:56,366 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:56,368 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:56,373 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:56,377 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:56,378 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:56,379 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:56,386 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:56,391 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:56,397 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:56,403 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:50:56,404 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:56,406 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:56,411 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:56,416 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:56,416 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:56,417 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:56,423 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:56,429 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:56,436 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:56,443 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:50:56,444 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:56,446 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:56,450 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:56,455 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:56,462 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:56,463 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:56,537 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:56,547 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:56,570 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:56,578 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:50:56,579 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:56,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:56,586 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:56,591 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:56,592 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:56,592 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:56,599 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:56,606 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:56,613 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:56,627 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:50:56,628 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:56,630 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:56,634 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:56,639 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:56,640 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:56,641 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:56,651 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:56,658 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:56,668 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:56,674 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:50:56,674 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:56,677 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:56,681 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:56,683 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:56,684 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:56,685 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:56,764 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:56,800 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:56,818 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:56,824 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:50:56,825 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:56,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:56,828 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:56,829 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:56,830 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:56,831 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:56,833 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:56,836 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:56,838 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:56,841 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:56,842 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:56,843 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:56,845 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:56,846 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:56,847 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:56,847 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:56,863 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:56,874 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:56,884 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:56,893 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:56,894 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:56,901 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:56,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:56,904 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:56,905 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:56,906 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:56,908 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:56,909 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:56,910 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:56,912 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:56,912 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:56,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:56,915 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:56,920 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 36])", "<class 'int'>: 35")
2023-10-11 12:50:56,921 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:56,922 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:56,924 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:56,926 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:56,927 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:56,928 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:56,929 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:56,933 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:56,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:56,942 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:56,943 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:56,944 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:56,951 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:56,958 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:57,032 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:57,054 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:50:57,055 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:57,059 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:57,064 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:57,068 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:57,069 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:57,070 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:57,079 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:57,088 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:57,095 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:57,106 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:50:57,106 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:57,109 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:57,114 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:57,119 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:57,120 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:57,121 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:57,134 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:57,143 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:57,198 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:57,250 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:50:57,251 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:57,254 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:57,258 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:57,263 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:57,264 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:57,265 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:57,273 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:57,282 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:57,301 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:57,310 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:50:57,310 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:57,313 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:57,318 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:57,323 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:57,324 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:57,324 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:57,333 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:57,338 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:57,348 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:57,354 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:50:57,355 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:57,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:57,362 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:57,366 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:57,367 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:57,368 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:57,377 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:57,384 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:57,394 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:57,400 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:50:57,401 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:57,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:57,409 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:57,415 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:57,416 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:57,417 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:57,426 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:57,432 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:57,439 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:57,512 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:50:57,513 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:57,516 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:57,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:57,527 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:57,527 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:57,528 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:57,539 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:57,545 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:57,577 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:57,583 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:50:57,584 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:57,586 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:57,590 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:57,595 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:57,596 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:57,596 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:57,606 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:57,613 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:57,623 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:57,631 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:50:57,632 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:57,634 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:57,639 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:57,644 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:57,644 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:57,645 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:57,652 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:57,658 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:57,666 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:57,672 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:50:57,673 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:57,675 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:57,680 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:57,685 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:57,686 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:57,687 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:57,694 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:57,700 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:57,705 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:57,717 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:50:57,718 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:57,720 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:57,725 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:57,726 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:57,727 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:57,728 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:57,741 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:57,751 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:57,768 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:57,778 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:50:57,779 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:57,781 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:57,790 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:57,791 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:57,792 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:57,793 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:57,798 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:57,803 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:57,811 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:57,824 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:57,825 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:57,829 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:57,831 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:57,833 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:57,835 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:57,836 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:57,864 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:57,880 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:57,895 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:57,911 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:57,912 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:57,921 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:57,923 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:57,924 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:57,925 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:57,926 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:57,928 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:57,929 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:57,931 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:57,933 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:57,934 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:57,935 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:57,937 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:57,942 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 37])", "<class 'int'>: 36")
2023-10-11 12:50:57,942 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:57,943 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:57,945 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:57,947 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:57,948 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:57,950 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:57,951 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:57,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:57,960 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:57,965 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:57,966 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:57,967 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:57,974 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:57,980 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:57,986 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:57,991 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:50:57,992 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:57,994 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:57,998 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:58,003 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,004 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,005 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:58,015 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:58,026 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:58,111 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:58,119 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:50:58,120 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:58,123 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:58,128 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:58,133 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,134 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,135 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:58,163 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:58,169 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:58,176 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:58,187 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:50:58,188 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:58,191 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:58,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:58,201 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,201 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,202 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:58,212 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:58,228 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:58,234 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:58,253 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:50:58,254 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:58,256 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:58,261 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:58,267 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,268 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,269 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:58,279 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:58,297 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:58,305 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:58,311 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:50:58,312 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:58,315 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:58,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:58,325 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,326 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,327 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:58,336 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:58,343 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:58,349 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:58,355 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:50:58,356 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:58,358 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:58,363 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:58,368 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,369 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,370 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:58,382 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:58,388 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:58,395 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:58,405 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:50:58,406 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:58,408 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:58,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:58,417 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,418 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,419 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:58,428 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:58,461 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:58,467 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:58,473 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:50:58,474 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:58,476 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:58,481 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:58,486 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,487 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,488 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:58,496 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:58,509 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:58,517 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:58,526 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:50:58,527 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:58,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:58,534 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:58,539 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,540 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,541 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:58,551 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:58,559 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:58,570 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:58,582 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:50:58,583 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:58,586 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:58,590 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:58,595 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,596 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,596 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:58,604 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:58,612 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:58,626 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:58,632 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:50:58,633 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:58,635 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:58,640 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:58,641 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,642 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,643 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:58,652 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:58,661 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:58,667 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:58,673 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:50:58,674 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:58,676 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:58,678 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:58,679 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,680 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:58,680 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:58,683 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:58,687 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:58,690 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:58,694 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:58,695 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:58,696 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:58,698 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:58,700 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,702 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:58,704 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:58,722 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:58,735 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:58,746 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:58,755 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:58,756 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:58,764 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:58,765 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:58,767 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:50:58,767 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:58,768 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:50:58,770 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:50:58,772 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:50:58,773 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:50:58,774 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:58,775 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:50:58,777 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:50:58,778 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:58,783 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 38])", "<class 'int'>: 37")
2023-10-11 12:50:58,784 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:58,784 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:50:58,786 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:50:58,788 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:50:58,789 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:50:58,791 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:58,792 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:50:58,797 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:50:58,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:58,807 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,808 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,809 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:50:58,818 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:50:58,825 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:50:58,833 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:50:58,840 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:50:58,841 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:50:58,844 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:50:58,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:58,853 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,854 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,855 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:50:58,862 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:50:58,868 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:50:58,880 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:50:58,888 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:50:58,888 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:50:58,892 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:50:58,896 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:58,902 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,903 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,904 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:50:58,911 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:50:58,917 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:50:58,927 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:50:58,942 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:50:58,943 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:50:58,946 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:50:58,951 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:58,956 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:58,956 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:58,957 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:50:58,965 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:50:59,002 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:50:59,010 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:50:59,024 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:50:59,025 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:50:59,028 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:50:59,033 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:59,038 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:59,039 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:59,040 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:50:59,105 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:50:59,123 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:50:59,134 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:50:59,140 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:50:59,141 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:50:59,144 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:50:59,148 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:59,153 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:59,154 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:59,155 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:50:59,163 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:50:59,171 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:50:59,180 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:50:59,191 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:50:59,192 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:50:59,195 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:50:59,200 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:59,205 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:59,206 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:59,207 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:50:59,215 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:50:59,221 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:50:59,226 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:50:59,249 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:50:59,251 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:50:59,253 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:50:59,258 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:59,263 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:59,264 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:59,265 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:50:59,272 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:50:59,281 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:50:59,287 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:50:59,294 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:50:59,295 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:50:59,297 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:50:59,301 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:59,306 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:59,307 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:59,308 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:50:59,315 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:50:59,322 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:50:59,329 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:50:59,335 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:50:59,336 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:50:59,339 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:50:59,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:59,348 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:59,349 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:59,350 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:50:59,359 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:50:59,371 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:50:59,455 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:50:59,471 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:50:59,472 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:50:59,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:50:59,478 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:59,483 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:59,484 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:59,484 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:50:59,511 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:50:59,517 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:50:59,523 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:50:59,534 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:50:59,535 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:50:59,538 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:50:59,542 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:59,544 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:59,545 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:50:59,546 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:50:59,553 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:50:59,559 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:50:59,567 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:50:59,572 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:50:59,573 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:50:59,575 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:50:59,577 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:59,579 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:59,579 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:59,580 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:50:59,582 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:50:59,584 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:50:59,586 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:50:59,588 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:50:59,590 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:50:59,592 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:50:59,593 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:50:59,595 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:50:59,596 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:50:59,596 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:50:59,607 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:50:59,617 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:50:59,628 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:50:59,637 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:50:59,639 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:50:59,646 [test.py:40 in test_hf_gen] INFO - for i in range(10):  ( ( (
,,,,,,,,,,,,,,,,, and and and and and and and and and
2023-10-11 12:50:59,647 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:50:59,648 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious???,...   ...                  
2023-10-11 12:50:59,649 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:50:59,650 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?ooooooooooooooooooo's,, and and and and and and and and
2023-10-11 12:50:59,651 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:50:59,651 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?oooooo   ...                  
2023-10-11 12:50:59,652 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:50:59,653 [test.py:40 in test_hf_gen] INFO - for i in range(10):  ( to
:::,,,,,,,,,,,,,,,, and and and and and and and and
2023-10-11 12:50:59,654 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:50:59,656 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious??                             
2023-10-11 12:50:59,656 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:50:59,657 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?oooooooooooooooooo's's, and and and and and and and and and
2023-10-11 12:50:59,658 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:50:59,659 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?ooooo                         
2023-10-11 12:50:59,660 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:50:59,670 [520681597.py:17 in reset_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-11 12:50:59,671 [520681597.py:17 in reset_forward] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-11 12:50:59,672 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-11 12:50:59,673 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-11 12:50:59,674 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-11 12:50:59,675 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-11 12:50:59,676 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-11 12:50:59,676 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-11 12:50:59,678 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-11 12:50:59,678 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-11 12:50:59,679 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-11 12:50:59,681 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-11 12:50:59,682 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-11 12:50:59,683 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-11 12:50:59,683 [520681597.py:17 in reset_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-11 12:50:59,684 [520681597.py:17 in reset_forward] DEBUG - lm_head from flexgen to old.
2023-10-11 12:52:23,209 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:52:23,344 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:52:23,453 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:52:23,496 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:52:23,582 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:52:23,584 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'
2023-10-11 12:52:23,592 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-11 12:52:23,593 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-11 12:52:23,595 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-11 12:52:23,596 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-11 12:52:23,598 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-11 12:52:23,600 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-11 12:52:23,603 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-11 12:52:23,604 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-11 12:52:23,606 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-11 12:52:23,608 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-11 12:52:23,609 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-11 12:52:23,611 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-11 12:52:23,613 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-11 12:52:23,615 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-11 12:52:23,617 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:52:23,618 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:52:23,619 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-11 12:52:23,622 [model.py:148 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-11 12:52:23,624 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-11 12:52:23,655 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-11 12:52:23,655 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-11 12:52:23,657 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-11 12:52:23,657 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-11 12:52:23,658 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-11 12:52:23,659 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-11 12:52:23,660 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-11 12:52:23,661 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-11 12:52:23,662 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-11 12:52:23,663 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-11 12:52:23,664 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-11 12:52:23,665 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-11 12:52:23,666 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-11 12:52:23,667 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-11 12:52:23,668 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-11 12:52:23,669 [520681597.py:42 in to_test_forward] DEBUG - lm_head to test forward
2023-10-11 12:52:23,711 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:52:23,895 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:23,897 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:23,899 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:23,901 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:23,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:23,913 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:23,916 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:23,926 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:23,929 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:23,938 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:23,941 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:23,966 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:23,969 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:23,977 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:23,980 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:23,988 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:23,991 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:23,999 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:24,002 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:24,009 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:24,011 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:24,019 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:24,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:24,030 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:24,032 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:24,041 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:24,044 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:24,052 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:24,054 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:24,056 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:24,057 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:24,070 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:24,076 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-11 12:52:24,077 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:52:24,090 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-11 12:52:24,091 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-11 12:52:24,092 [520681597.py:22 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-11 12:52:24,093 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-11 12:52:24,094 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-11 12:52:24,095 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-11 12:52:24,096 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-11 12:52:24,097 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-11 12:52:24,098 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-11 12:52:24,099 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-11 12:52:24,100 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-11 12:52:24,101 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-11 12:52:24,102 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-11 12:52:24,104 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-11 12:52:24,104 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-11 12:52:24,105 [520681597.py:22 in reset_forward] DEBUG - lm_head from test to old.
2023-10-11 12:52:24,106 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-11 12:52:24,107 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-11 12:52:24,108 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-11 12:52:24,109 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-11 12:52:24,110 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-11 12:52:24,111 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-11 12:52:24,113 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-11 12:52:24,113 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-11 12:52:24,115 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-11 12:52:24,116 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-11 12:52:24,117 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-11 12:52:24,118 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-11 12:52:24,119 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-11 12:52:24,119 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-11 12:52:24,120 [2192271695.py:48 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-11 12:52:24,121 [2192271695.py:48 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-11 12:52:24,163 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:52:24,320 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:24,322 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:24,323 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])",)
2023-10-11 12:52:24,324 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:24,325 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:24,326 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:24,328 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:24,329 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:24,331 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:52:24,332 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:24,333 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:24,335 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:24,339 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])", "<class 'int'>: 0")
2023-10-11 12:52:24,340 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:24,341 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:24,343 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:24,344 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:24,346 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:24,348 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:52:24,349 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:24,354 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:24,358 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:24,363 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:52:24,364 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:24,365 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:24,376 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:24,383 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:24,406 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:24,416 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:52:24,417 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:24,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:24,425 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:24,430 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:52:24,430 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:24,431 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:24,438 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:24,445 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:24,450 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:24,457 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:52:24,458 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:24,461 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:24,466 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:24,471 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:52:24,472 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:24,473 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:24,480 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:24,486 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:24,492 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:24,499 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:52:24,500 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:24,503 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:24,508 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:24,513 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:52:24,514 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:24,515 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:24,524 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:24,536 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:24,548 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:24,556 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:52:24,557 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:24,559 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:24,564 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:24,569 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:52:24,570 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:24,570 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:24,577 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:24,587 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:24,595 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:24,600 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:52:24,601 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:24,604 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:24,609 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:24,614 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:52:24,615 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:24,616 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:24,623 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:24,631 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:24,638 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:24,644 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:52:24,646 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:24,648 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:24,653 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:24,659 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:52:24,660 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:24,660 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:24,668 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:24,677 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:24,686 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:24,699 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:52:24,700 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:24,702 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:24,707 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:24,712 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:52:24,713 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:24,714 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:24,721 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:24,732 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:24,741 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:24,747 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:52:24,748 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:24,750 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:24,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:24,759 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:52:24,760 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:24,761 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:24,780 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:24,787 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:24,796 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:24,813 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:52:24,813 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:24,816 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:24,821 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:24,826 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:52:24,826 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:24,828 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:24,834 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:24,841 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:24,857 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:24,867 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:52:24,868 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:24,871 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:24,876 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:24,881 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:52:24,882 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:24,882 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:24,893 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:24,899 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:24,908 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:24,915 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:52:24,917 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:24,919 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:24,924 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:24,926 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:52:24,927 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': "<class 'NoneType'>: None", 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:24,928 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:24,943 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:24,951 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:24,956 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:24,965 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"))
2023-10-11 12:52:24,966 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:24,969 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:24,971 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:24,972 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:52:24,973 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:24,974 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:24,977 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:24,980 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:24,983 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:24,987 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:52:24,988 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:24,990 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:24,991 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:24,992 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])",)
2023-10-11 12:52:24,993 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:24,994 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:25,016 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:25,031 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:25,047 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:25,070 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 25136])
2023-10-11 12:52:25,071 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:25,082 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:25,083 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:25,085 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:25,085 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:25,086 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:25,088 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:25,089 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:25,091 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:25,092 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:25,093 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:25,095 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:25,096 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:25,102 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 10])", "<class 'int'>: 9")
2023-10-11 12:52:25,103 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:25,104 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:25,106 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:25,108 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:25,109 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:25,111 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:25,112 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:25,116 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:25,121 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:25,126 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,127 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:25,128 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:25,137 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:25,142 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:25,148 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:25,153 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:52:25,154 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:25,157 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:25,161 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:25,167 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,167 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:25,168 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:25,175 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:25,183 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:25,193 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:25,205 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:52:25,206 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:25,208 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:25,212 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:25,217 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,218 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:25,219 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:25,225 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:25,231 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:25,237 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:25,245 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:52:25,245 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:25,248 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:25,252 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:25,256 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,257 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:25,258 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:25,270 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:25,277 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:25,283 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:25,289 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:52:25,289 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:25,292 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:25,297 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:25,303 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,304 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:25,304 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:25,313 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:25,320 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:25,331 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:25,337 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:52:25,338 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:25,340 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:25,344 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:25,349 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,350 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:25,351 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:25,358 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:25,366 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:25,381 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:25,390 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:52:25,391 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:25,394 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:25,399 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:25,404 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,405 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:25,406 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:25,412 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:25,434 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:25,440 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:25,448 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:52:25,449 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:25,452 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:25,456 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:25,461 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,462 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:25,463 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:25,471 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:25,479 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:25,486 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:25,492 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:52:25,493 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:25,495 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:25,500 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:25,506 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,507 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:25,508 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:25,517 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:25,524 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:25,530 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:25,535 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:52:25,536 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:25,539 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:25,543 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:25,548 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,549 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:25,551 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:25,559 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:25,564 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:25,572 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:25,578 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:52:25,579 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:25,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:25,588 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:25,594 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,596 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:25,596 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:25,603 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:25,677 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:25,692 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:25,699 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:52:25,701 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:25,704 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:25,709 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:25,710 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,711 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 10])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 9, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:25,713 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:25,720 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:25,730 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:25,743 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:25,750 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"))
2023-10-11 12:52:25,751 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:25,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:25,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:25,757 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,758 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:25,759 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:25,763 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:25,765 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:25,768 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:25,770 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:25,771 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:25,772 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:25,773 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:25,774 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,775 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:25,776 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:25,787 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:25,799 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:25,824 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:25,838 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:25,840 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:25,852 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:25,854 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:25,856 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:25,857 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:25,858 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:25,860 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:25,862 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:25,864 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:25,866 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:25,867 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:25,869 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:25,870 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:25,878 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 11])", "<class 'int'>: 10")
2023-10-11 12:52:25,879 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:25,880 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:25,883 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:25,885 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:25,888 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:25,890 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:25,891 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:25,897 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:25,905 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:25,913 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,914 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:25,915 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:25,924 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:25,937 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:25,948 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:25,961 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:52:25,962 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:25,966 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:25,971 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:25,977 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:25,978 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:25,979 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:25,986 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:25,991 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:25,996 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:26,002 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:52:26,004 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:26,007 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:26,011 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:26,016 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,017 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,017 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:26,024 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:26,031 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:26,037 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:26,043 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:52:26,043 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:26,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:26,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:26,056 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,056 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,057 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:26,065 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:26,074 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:26,083 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:26,088 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:52:26,088 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:26,091 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:26,095 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:26,100 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,101 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,101 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:26,107 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:26,117 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:26,122 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:26,127 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:52:26,128 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:26,131 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:26,135 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:26,139 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,140 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,141 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:26,149 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:26,155 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:26,162 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:26,167 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:52:26,168 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:26,171 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:26,176 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:26,181 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,182 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,183 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:26,200 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:26,206 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:26,214 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:26,220 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:52:26,221 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:26,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:26,228 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:26,233 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,234 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,235 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:26,260 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:26,266 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:26,272 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:26,280 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:52:26,281 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:26,283 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:26,288 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:26,293 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,294 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,295 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:26,304 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:26,310 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:26,324 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:26,330 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:52:26,331 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:26,334 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:26,339 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:26,344 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,345 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,346 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:26,356 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:26,362 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:26,368 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:26,374 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:52:26,375 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:26,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:26,382 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:26,387 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,388 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,389 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:26,395 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:26,404 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:26,410 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:26,422 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:52:26,423 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:26,425 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:26,430 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:26,431 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,432 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 11])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 10, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,433 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:26,442 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:26,455 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:26,460 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:26,466 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"))
2023-10-11 12:52:26,467 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:26,469 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:26,470 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:26,471 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,472 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:26,473 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:26,476 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:26,478 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:26,480 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:26,481 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:26,482 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:26,484 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:26,485 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:26,486 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,487 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:26,487 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:26,499 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:26,514 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:26,525 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:26,536 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:26,537 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:26,545 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:26,547 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:26,549 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:26,550 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:26,550 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:26,552 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:26,554 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:26,556 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:26,557 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:26,558 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:26,559 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:26,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:26,566 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 12])", "<class 'int'>: 11")
2023-10-11 12:52:26,567 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:26,568 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:26,570 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:26,572 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:26,573 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:26,575 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:26,576 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:26,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:26,586 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:26,591 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,592 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,593 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:26,600 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:26,606 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:26,616 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:26,621 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:52:26,622 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:26,624 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:26,629 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:26,634 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,635 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,636 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:26,642 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:26,650 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:26,654 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:26,663 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:52:26,664 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:26,667 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:26,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:26,676 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,677 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,678 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:26,686 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:26,692 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:26,697 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:26,704 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:52:26,705 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:26,708 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:26,712 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:26,717 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,717 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,718 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:26,725 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:26,731 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:26,743 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:26,754 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:52:26,755 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:26,757 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:26,762 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:26,767 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,768 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,769 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:26,778 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:26,787 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:26,795 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:26,806 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:52:26,807 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:26,810 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:26,814 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:26,819 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,820 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,821 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:26,827 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:26,850 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:26,856 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:26,866 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:52:26,867 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:26,869 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:26,874 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:26,880 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:26,881 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:26,882 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:26,950 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:26,984 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:26,990 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:26,994 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:52:26,995 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:26,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:27,002 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:27,006 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,008 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,009 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:27,015 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:27,024 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:27,030 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:27,039 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:52:27,039 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:27,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:27,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:27,051 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,052 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,053 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:27,059 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:27,070 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:27,078 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:27,089 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:52:27,090 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:27,093 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:27,097 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:27,102 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,102 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,103 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:27,109 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:27,119 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:27,124 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:27,129 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:52:27,130 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:27,132 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:27,136 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:27,142 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,142 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,144 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:27,152 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:27,162 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:27,168 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:27,173 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:52:27,175 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:27,177 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:27,182 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:27,183 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,184 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 12])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 11, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,184 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:27,191 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:27,197 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:27,202 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:27,208 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"))
2023-10-11 12:52:27,209 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:27,211 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:27,212 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:27,213 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,214 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:27,215 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:27,218 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:27,222 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:27,226 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:27,230 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:27,231 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:27,232 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:27,233 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:27,235 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,236 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:27,236 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:27,251 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:27,267 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:27,276 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:27,286 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:27,287 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:27,296 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:27,297 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:27,299 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:27,300 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:27,300 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:27,302 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:27,304 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:27,306 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:27,307 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:27,309 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:27,310 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:27,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:27,317 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 13])", "<class 'int'>: 12")
2023-10-11 12:52:27,318 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:27,319 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:27,321 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:27,323 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:27,324 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:27,326 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:27,327 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:27,331 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:27,336 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:27,341 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,342 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,343 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:27,350 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:27,358 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:27,365 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:27,372 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:52:27,373 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:27,376 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:27,380 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:27,386 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,387 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,387 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:27,395 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:27,402 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:27,407 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:27,415 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:52:27,416 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:27,419 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:27,424 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:27,429 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,430 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,431 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:27,438 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:27,446 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:27,451 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:27,462 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:52:27,463 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:27,465 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:27,470 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:27,474 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,475 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,476 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:27,526 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:27,554 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:27,582 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:27,591 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:52:27,592 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:27,595 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:27,599 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:27,603 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,604 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,605 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:27,612 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:27,617 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:27,622 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:27,627 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:52:27,628 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:27,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:27,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:27,641 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,642 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,642 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:27,649 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:27,655 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:27,661 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:27,671 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:52:27,672 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:27,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:27,678 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:27,683 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,684 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,685 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:27,692 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:27,697 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:27,701 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:27,708 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:52:27,708 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:27,711 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:27,715 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:27,720 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,720 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,721 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:27,728 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:27,733 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:27,738 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:27,751 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:52:27,751 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:27,753 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:27,757 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:27,762 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,763 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,763 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:27,769 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:27,775 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:27,780 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:27,786 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:52:27,787 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:27,789 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:27,795 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:27,799 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,800 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,801 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:27,811 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:27,824 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:27,831 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:27,838 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:52:27,839 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:27,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:27,846 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:27,851 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,852 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,853 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:27,860 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:27,866 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:27,873 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:27,879 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:52:27,880 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:27,882 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:27,887 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:27,889 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,890 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 13])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 12, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:27,891 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:27,901 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:27,907 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:27,913 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:27,919 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"))
2023-10-11 12:52:27,920 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:27,923 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:27,925 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:27,926 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,927 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:27,927 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:27,931 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:27,934 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:27,937 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:27,940 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:27,941 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:27,943 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:27,944 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:27,946 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:27,946 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:27,947 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:27,961 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:27,973 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:27,985 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:27,995 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:27,997 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:28,004 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:28,005 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:28,006 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:28,007 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:28,008 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:28,010 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:28,012 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:28,013 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:28,015 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:28,016 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:28,017 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:28,019 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:28,024 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 14])", "<class 'int'>: 13")
2023-10-11 12:52:28,025 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:28,025 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:28,027 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:28,028 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:28,030 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:28,032 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:28,033 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:28,037 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:28,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:28,046 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,047 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,047 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:28,054 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:28,063 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:28,068 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:28,077 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:52:28,077 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:28,080 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:28,085 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:28,089 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,090 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,091 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:28,099 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:28,110 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:28,115 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:28,122 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:52:28,123 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:28,125 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:28,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:28,135 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,136 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,137 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:28,149 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:28,158 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:28,166 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:28,173 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:52:28,174 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:28,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:28,183 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:28,189 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,190 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,190 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:28,202 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:28,208 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:28,214 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:28,219 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:52:28,221 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:28,223 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:28,228 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:28,233 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,234 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,235 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:28,246 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:28,253 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:28,258 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:28,265 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:52:28,266 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:28,268 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:28,272 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:28,277 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,278 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,279 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:28,288 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:28,296 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:28,304 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:28,310 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:52:28,311 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:28,313 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:28,318 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:28,322 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,323 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,324 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:28,332 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:28,338 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:28,350 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:28,357 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:52:28,358 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:28,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:28,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:28,370 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,371 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,372 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:28,380 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:28,386 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:28,392 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:28,399 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:52:28,400 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:28,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:28,406 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:28,412 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,413 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,414 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:28,476 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:28,500 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:28,507 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:28,525 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:52:28,527 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:28,529 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:28,535 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:28,541 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,542 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,543 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:28,553 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:28,559 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:28,565 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:28,570 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:52:28,571 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:28,574 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:28,578 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:28,583 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,584 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,584 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:28,593 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:28,608 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:28,614 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:28,620 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:52:28,621 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:28,623 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:28,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:28,630 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,631 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 14])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 13, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,632 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:28,638 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:28,644 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:28,649 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:28,656 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"))
2023-10-11 12:52:28,657 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:28,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:28,661 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:28,662 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,663 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:28,664 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:28,666 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:28,668 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:28,670 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:28,672 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:28,672 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:28,674 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:28,676 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:28,677 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,678 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:28,679 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:28,692 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:28,702 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:28,712 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:28,721 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:28,722 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:28,735 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:28,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:28,738 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:28,739 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:28,740 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:28,742 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:28,743 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:28,744 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:28,746 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:28,746 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:28,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:28,749 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:28,754 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 15])", "<class 'int'>: 14")
2023-10-11 12:52:28,755 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:28,756 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:28,758 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:28,760 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:28,762 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:28,764 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:28,765 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:28,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:28,775 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:28,780 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,781 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,781 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:28,788 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:28,794 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:28,805 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:28,810 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:52:28,811 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:28,813 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:28,818 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:28,823 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,823 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,824 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:28,834 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:28,844 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:28,856 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:28,866 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:52:28,867 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:28,869 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:28,874 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:28,879 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,880 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,881 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:28,889 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:28,930 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:28,938 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:28,950 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:52:28,951 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:28,953 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:28,958 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:28,964 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:28,964 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:28,965 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:28,972 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:28,983 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:28,998 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:29,004 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:52:29,005 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:29,007 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:29,011 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:29,016 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,017 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,017 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:29,024 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:29,031 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:29,037 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:29,042 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:52:29,043 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:29,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:29,050 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:29,054 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,055 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,056 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:29,062 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:29,068 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:29,074 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:29,081 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:52:29,082 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:29,084 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:29,088 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:29,093 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,094 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,095 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:29,103 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:29,108 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:29,114 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:29,122 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:52:29,123 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:29,125 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:29,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:29,135 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,136 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,137 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:29,146 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:29,153 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:29,158 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:29,163 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:52:29,164 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:29,167 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:29,171 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:29,176 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,177 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,178 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:29,187 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:29,193 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:29,208 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:29,215 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:52:29,216 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:29,218 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:29,222 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:29,227 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,228 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,229 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:29,238 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:29,246 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:29,256 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:29,266 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:52:29,267 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:29,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:29,274 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:29,279 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,280 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,281 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:29,294 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:29,299 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:29,306 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:29,311 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:52:29,312 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:29,315 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:29,319 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:29,321 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,322 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 15])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 14, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,323 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:29,329 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:29,334 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:29,340 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:29,347 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"))
2023-10-11 12:52:29,347 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:29,350 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:29,351 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:29,353 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,353 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:29,355 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:29,358 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:29,361 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:29,364 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:29,367 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:29,368 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:29,369 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:29,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:29,372 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,373 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:29,374 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:29,385 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:29,398 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:29,408 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:29,417 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:29,419 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:29,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:29,445 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:29,446 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:29,447 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:29,448 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:29,450 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:29,451 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:29,452 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:29,453 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:29,454 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:29,455 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:29,457 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:29,461 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 16])", "<class 'int'>: 15")
2023-10-11 12:52:29,462 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:29,463 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:29,464 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:29,466 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:29,467 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:29,469 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:29,469 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:29,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:29,478 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:29,483 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,483 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,484 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:29,492 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:29,497 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:29,508 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:29,513 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:52:29,514 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:29,516 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:29,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:29,526 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,527 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,528 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:29,570 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:29,594 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:29,628 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:29,685 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:52:29,687 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:29,690 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:29,694 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:29,699 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,700 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,700 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:29,710 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:29,716 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:29,722 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:29,732 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:52:29,733 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:29,735 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:29,740 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:29,744 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,745 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,746 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:29,752 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:29,762 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:29,767 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:29,773 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:52:29,773 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:29,776 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:29,780 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:29,785 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,786 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,787 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:29,793 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:29,799 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:29,804 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:29,810 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:52:29,811 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:29,814 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:29,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:29,823 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,824 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,825 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:29,832 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:29,838 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:29,843 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:29,853 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:52:29,854 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:29,856 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:29,861 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:29,866 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,867 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,868 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:29,875 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:29,880 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:29,886 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:29,908 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:52:29,909 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:29,912 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:29,916 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:29,921 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,922 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,923 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:29,930 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:29,935 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:29,941 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:29,948 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:52:29,949 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:29,952 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:29,957 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:29,963 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:29,963 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:29,964 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:29,972 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:29,977 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:30,039 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:30,058 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:52:30,060 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:30,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:30,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:30,072 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,073 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,073 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:30,081 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:30,086 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:30,093 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:30,102 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:52:30,102 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:30,105 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:30,109 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:30,114 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,115 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,116 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:30,122 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:30,127 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:30,142 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:30,147 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:52:30,148 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:30,151 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:30,155 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:30,157 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,158 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 16])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 15, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,159 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:30,174 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:30,182 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:30,190 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:30,196 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"))
2023-10-11 12:52:30,196 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:30,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:30,200 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:30,201 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,202 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:30,203 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:30,207 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:30,209 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:30,211 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:30,214 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:30,214 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:30,216 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:30,217 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:30,218 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,219 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:30,220 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:30,235 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:30,245 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:30,254 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:30,264 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:30,265 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:30,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:30,274 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:30,276 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:30,277 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:30,277 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:30,279 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:30,281 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:30,282 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:30,284 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:30,285 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:30,286 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:30,288 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:30,293 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 17])", "<class 'int'>: 16")
2023-10-11 12:52:30,294 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:30,294 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:30,296 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:30,298 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:30,299 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:30,301 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:30,302 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:30,306 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:30,311 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:30,315 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,316 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,317 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:30,330 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:30,336 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:30,344 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:30,356 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:52:30,357 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:30,359 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:30,363 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:30,368 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,369 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,369 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:30,379 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:30,385 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:30,390 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:30,396 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:52:30,397 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:30,400 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:30,405 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:30,410 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,411 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,412 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:30,419 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:30,425 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:30,430 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:30,436 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:52:30,437 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:30,440 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:30,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:30,449 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,449 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,450 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:30,457 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:30,463 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:30,471 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:30,486 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:52:30,487 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:30,490 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:30,494 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:30,500 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,501 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,502 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:30,509 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:30,525 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:30,532 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:30,537 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:52:30,538 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:30,542 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:30,547 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:30,551 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,552 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,553 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:30,559 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:30,564 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:30,570 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:30,576 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:52:30,577 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:30,580 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:30,584 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:30,589 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,590 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,591 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:30,599 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:30,616 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:30,622 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:30,627 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:52:30,628 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:30,631 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:30,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:30,641 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,642 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,643 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:30,650 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:30,657 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:30,670 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:30,675 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:52:30,676 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:30,678 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:30,682 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:30,692 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,693 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,694 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:30,751 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:30,795 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:30,813 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:30,818 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:52:30,819 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:30,822 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:30,827 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:30,832 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,833 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,834 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:30,847 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:30,855 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:30,860 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:30,870 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:52:30,871 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:30,873 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:30,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:30,882 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,883 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,884 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:30,892 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:30,897 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:30,902 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:30,909 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:52:30,910 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:30,913 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:30,918 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:30,920 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,921 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 17])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 16, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:30,922 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:30,928 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:30,933 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:30,938 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:30,943 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"))
2023-10-11 12:52:30,945 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:30,947 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:30,948 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:30,949 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,950 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:30,951 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:30,952 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:30,954 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:30,956 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:30,958 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:30,959 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:30,960 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:30,962 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:30,964 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:30,965 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:30,965 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:30,976 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:30,986 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:30,997 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:31,007 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:31,008 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:31,016 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:31,017 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:31,019 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:31,020 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:31,021 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:31,022 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:31,024 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:31,025 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:31,027 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:31,027 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:31,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:31,031 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:31,036 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 18])", "<class 'int'>: 17")
2023-10-11 12:52:31,036 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:31,037 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:31,039 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:31,041 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:31,042 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:31,044 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:31,045 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:31,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:31,054 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:31,059 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,060 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,060 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:31,067 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:31,074 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:31,099 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:31,106 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:52:31,107 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:31,110 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:31,114 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:31,119 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,120 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,121 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:31,192 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:31,208 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:31,215 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:31,220 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:52:31,221 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:31,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:31,229 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:31,234 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,235 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,236 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:31,243 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:31,248 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:31,256 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:31,262 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:52:31,263 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:31,265 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:31,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:31,274 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,275 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,276 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:31,282 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:31,288 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:31,306 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:31,314 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:52:31,315 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:31,317 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:31,322 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:31,326 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,327 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,328 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:31,335 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:31,342 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:31,347 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:31,352 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:52:31,353 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:31,356 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:31,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:31,364 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,365 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,366 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:31,373 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:31,378 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:31,383 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:31,388 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:52:31,389 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:31,392 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:31,396 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:31,401 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,401 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,402 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:31,411 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:31,417 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:31,421 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:31,426 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:52:31,427 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:31,429 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:31,434 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:31,438 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,439 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,440 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:31,449 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:31,454 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:31,459 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:31,470 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:52:31,472 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:31,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:31,479 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:31,484 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,485 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,485 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:31,492 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:31,497 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:31,502 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:31,507 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:52:31,508 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:31,511 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:31,516 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:31,521 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,521 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,522 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:31,534 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:31,540 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:31,555 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:31,560 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:52:31,561 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:31,563 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:31,568 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:31,573 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,574 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,575 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:31,588 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:31,594 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:31,606 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:31,674 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:52:31,676 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:31,679 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:31,684 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:31,685 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,686 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 18])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 17, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,687 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:31,707 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:31,713 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:31,720 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:31,727 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"))
2023-10-11 12:52:31,727 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:31,730 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:31,732 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:31,733 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,734 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:31,735 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:31,738 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:31,740 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:31,741 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:31,743 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:31,744 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:31,745 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:31,747 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:31,748 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,749 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:31,749 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:31,760 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:31,770 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:31,781 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:31,796 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:31,798 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:31,809 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:31,810 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:31,812 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:31,813 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:31,813 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:31,815 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:31,817 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:31,818 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:31,820 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:31,820 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:31,822 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:31,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:31,828 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 19])", "<class 'int'>: 18")
2023-10-11 12:52:31,829 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:31,830 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:31,831 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:31,833 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:31,834 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:31,836 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:31,837 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:31,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:31,845 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:31,850 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,850 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,851 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:31,858 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:31,864 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:31,869 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:31,881 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:52:31,882 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:31,885 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:31,890 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:31,895 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,896 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,896 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:31,904 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:31,915 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:31,921 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:31,926 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:52:31,927 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:31,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:31,935 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:31,939 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:31,940 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:31,941 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:31,948 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:31,953 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:31,965 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:31,978 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:52:31,980 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:31,984 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:31,992 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:32,003 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,004 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,005 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:32,012 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:32,018 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:32,042 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:32,052 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:52:32,053 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:32,056 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:32,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:32,066 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,067 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,068 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:32,074 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:32,087 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:32,093 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:32,099 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:52:32,100 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:32,103 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:32,108 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:32,113 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,114 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,115 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:32,122 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:32,127 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:32,133 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:32,142 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:52:32,143 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:32,146 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:32,150 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:32,156 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,156 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,157 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:32,168 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:32,173 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:32,178 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:32,184 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:52:32,185 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:32,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:32,192 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:32,197 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,198 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,198 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:32,206 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:32,219 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:32,223 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:32,231 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:52:32,232 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:32,234 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:32,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:32,243 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,244 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,245 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:32,252 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:32,258 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:32,265 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:32,271 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:52:32,272 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:32,274 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:32,279 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:32,284 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,285 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,286 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:32,294 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:32,315 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:32,321 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:32,330 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:52:32,331 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:32,333 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:32,337 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:32,342 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,343 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,344 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:32,350 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:32,356 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:32,361 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:32,370 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:52:32,371 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:32,374 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:32,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:32,380 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,381 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 19])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 18, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,382 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:32,389 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:32,398 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:32,403 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:32,419 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"))
2023-10-11 12:52:32,420 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:32,422 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:32,424 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:32,425 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,426 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:32,427 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:32,429 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:32,432 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:32,434 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:32,436 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:32,437 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:32,439 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:32,440 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:32,441 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,442 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:32,443 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:32,455 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:32,469 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:32,479 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:32,493 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:32,494 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:32,504 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:32,505 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:32,507 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:32,508 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:32,509 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:32,511 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:32,513 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:32,514 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:32,516 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:32,517 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:32,518 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:32,520 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:32,524 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 20])", "<class 'int'>: 19")
2023-10-11 12:52:32,525 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:32,526 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:32,530 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:32,531 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:32,533 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:32,535 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:32,536 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:32,542 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:32,549 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:32,556 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,557 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,558 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:32,567 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:32,575 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:32,582 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:32,589 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:52:32,590 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:32,592 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:32,598 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:32,605 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,606 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,608 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:32,615 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:32,620 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:32,626 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:32,631 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:52:32,631 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:32,633 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:32,638 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:32,643 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,644 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,645 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:32,654 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:32,662 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:32,668 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:32,674 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:52:32,675 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:32,678 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:32,683 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:32,687 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,688 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,689 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:32,696 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:32,717 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:32,725 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:32,731 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:52:32,733 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:32,735 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:32,740 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:32,745 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,746 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,747 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:32,754 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:32,767 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:32,786 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:32,796 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:52:32,797 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:32,800 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:32,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:32,812 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,813 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,814 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:32,820 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:32,827 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:32,832 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:32,838 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:52:32,839 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:32,841 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:32,846 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:32,852 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,853 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,854 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:32,863 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:32,870 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:32,876 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:32,883 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:52:32,884 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:32,887 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:32,891 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:32,896 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,897 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,897 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:32,906 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:32,911 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:32,917 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:32,923 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:52:32,924 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:32,926 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:32,930 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:32,936 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,937 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,937 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:32,945 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:32,955 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:32,962 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:32,968 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:52:32,969 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:32,971 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:32,976 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:32,980 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:32,981 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:32,982 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:32,988 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:32,994 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:33,000 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:33,035 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:52:33,036 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:33,038 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:33,043 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:33,049 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,050 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:33,051 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:33,057 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:33,062 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:33,068 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:33,090 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:52:33,091 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:33,093 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:33,098 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:33,099 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,100 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 20])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 19, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:33,101 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:33,116 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:33,148 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:33,154 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:33,159 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"))
2023-10-11 12:52:33,160 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:33,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:33,163 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:33,165 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,166 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:33,166 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:33,168 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:33,170 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:33,172 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:33,175 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:33,176 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:33,177 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:33,178 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:33,179 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,180 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:33,181 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:33,192 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:33,204 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:33,215 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:33,225 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:33,226 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:33,233 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:33,234 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:33,235 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:33,236 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:33,238 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:33,240 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:33,242 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:33,243 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:33,245 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:33,246 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:33,248 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:33,249 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:33,254 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 21])", "<class 'int'>: 20")
2023-10-11 12:52:33,255 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:33,256 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:33,258 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:33,259 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:33,261 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:33,262 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:33,263 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:33,267 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:33,272 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:33,277 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,278 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:33,279 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:33,289 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:33,298 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:33,303 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:33,309 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:52:33,310 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:33,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:33,316 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:33,322 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,323 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:33,324 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:33,333 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:33,343 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:33,350 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:33,357 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:52:33,358 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:33,361 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:33,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:33,370 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,371 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:33,372 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:33,380 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:33,387 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:33,399 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:33,404 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:52:33,405 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:33,408 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:33,412 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:33,417 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,418 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:33,419 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:33,426 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:33,431 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:33,438 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:33,443 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:52:33,444 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:33,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:33,452 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:33,457 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,458 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:33,459 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:33,466 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:33,474 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:33,480 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:33,485 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:52:33,486 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:33,488 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:33,493 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:33,497 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,498 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:33,499 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:33,507 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:33,517 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:33,523 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:33,530 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:52:33,530 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:33,532 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:33,537 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:33,542 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,543 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:33,544 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:33,551 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:33,566 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:33,572 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:33,579 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:52:33,580 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:33,582 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:33,587 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:33,591 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,592 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:33,593 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:33,600 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:33,605 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:33,612 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:33,617 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:52:33,618 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:33,620 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:33,625 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:33,630 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,631 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:33,631 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:33,638 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:33,645 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:33,653 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:33,670 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:52:33,671 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:33,673 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:33,677 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:33,682 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,683 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:33,683 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:33,696 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:33,706 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:33,716 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:33,722 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:52:33,723 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:33,725 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:33,730 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:33,735 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,735 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:33,736 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:33,746 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:33,753 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:33,759 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:33,765 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:52:33,766 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:33,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:33,773 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:33,774 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,775 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 21])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 20, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:33,776 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:33,783 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:33,798 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:33,872 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:33,891 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"))
2023-10-11 12:52:33,892 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:33,894 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:33,896 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:33,897 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,898 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:33,899 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:33,904 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:33,909 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:33,911 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:33,913 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:33,914 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:33,916 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:33,917 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:33,918 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:33,919 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:33,920 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:33,931 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:33,944 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:33,956 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:33,968 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:33,971 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:33,979 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:33,980 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:33,982 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:33,983 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:33,984 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:33,985 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:33,987 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:33,988 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:33,990 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:33,991 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:33,992 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:33,994 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:33,998 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 22])", "<class 'int'>: 21")
2023-10-11 12:52:33,999 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:34,000 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:34,002 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:34,003 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:34,005 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:34,007 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:34,007 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:34,012 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:34,016 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:34,021 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,021 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,022 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:34,029 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:34,035 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:34,040 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:34,045 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:52:34,045 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:34,048 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:34,053 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:34,057 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,058 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,059 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:34,066 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:34,082 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:34,092 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:34,099 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:52:34,099 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:34,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:34,107 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:34,112 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,113 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,114 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:34,124 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:34,133 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:34,141 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:34,149 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:52:34,150 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:34,153 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:34,158 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:34,163 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,164 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,164 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:34,174 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:34,180 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:34,185 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:34,191 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:52:34,191 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:34,194 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:34,199 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:34,204 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,205 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,205 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:34,212 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:34,220 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:34,226 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:34,249 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:52:34,250 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:34,252 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:34,257 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:34,262 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,263 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,264 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:34,279 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:34,319 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:34,369 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:34,398 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:52:34,399 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:34,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:34,407 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:34,412 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,412 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,413 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:34,420 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:34,427 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:34,432 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:34,437 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:52:34,438 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:34,441 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:34,445 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:34,450 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,450 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,451 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:34,458 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:34,467 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:34,475 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:34,480 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:52:34,481 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:34,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:34,488 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:34,493 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,494 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,495 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:34,503 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:34,509 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:34,514 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:34,519 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:52:34,520 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:34,523 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:34,527 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:34,532 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,532 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,533 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:34,540 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:34,547 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:34,553 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:34,558 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:52:34,559 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:34,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:34,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:34,571 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,572 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,573 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:34,580 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:34,587 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:34,598 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:34,604 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:52:34,605 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:34,607 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:34,612 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:34,613 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,614 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 22])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 21, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,615 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:34,626 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:34,633 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:34,638 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:34,644 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"))
2023-10-11 12:52:34,645 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:34,647 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:34,649 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:34,650 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,651 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:34,652 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:34,655 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:34,658 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:34,664 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:34,680 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:34,685 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:34,688 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:34,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:34,693 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,695 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:34,698 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:34,719 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:34,735 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:34,750 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:34,767 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:34,769 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:34,793 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:34,795 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:34,797 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:34,798 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:34,799 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:34,802 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:34,804 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:34,806 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:34,808 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:34,809 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:34,810 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:34,812 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:34,817 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 23])", "<class 'int'>: 22")
2023-10-11 12:52:34,818 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:34,819 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:34,821 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:34,823 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:34,825 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:34,827 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:34,828 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:34,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:34,837 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:34,842 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,843 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,844 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:34,852 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:34,858 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:34,870 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:34,875 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:52:34,876 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:34,880 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:34,885 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:34,891 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,891 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,892 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:34,899 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:34,904 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:34,916 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:34,925 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:52:34,927 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:34,929 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:34,934 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:34,939 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,940 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,941 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:34,951 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:34,958 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:34,963 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:34,969 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:52:34,970 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:34,972 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:34,977 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:34,982 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:34,983 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:34,984 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:34,991 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:34,998 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:35,013 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:35,025 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:52:35,025 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:35,028 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:35,033 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:35,038 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,039 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,039 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:35,046 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:35,054 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:35,060 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:35,066 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:52:35,066 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:35,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:35,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:35,080 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,081 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,082 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:35,103 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:35,109 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:35,118 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:35,123 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:52:35,124 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:35,126 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:35,130 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:35,135 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,136 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,137 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:35,158 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:35,168 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:35,177 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:35,182 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:52:35,183 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:35,186 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:35,191 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:35,195 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,196 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,197 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:35,205 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:35,213 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:35,220 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:35,229 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:52:35,230 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:35,233 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:35,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:35,246 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,247 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,248 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:35,257 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:35,267 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:35,275 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:35,280 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:52:35,281 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:35,285 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:35,292 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:35,300 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,301 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,301 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:35,317 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:35,399 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:35,410 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:35,416 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:52:35,417 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:35,419 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:35,425 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:35,430 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,431 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,432 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:35,438 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:35,443 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:35,452 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:35,458 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:52:35,460 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:35,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:35,467 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:35,469 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,470 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 23])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 22, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,471 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:35,477 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:35,484 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:35,490 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:35,495 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"))
2023-10-11 12:52:35,496 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:35,498 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:35,500 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:35,501 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,502 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:35,502 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:35,505 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:35,506 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:35,508 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:35,509 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:35,510 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:35,512 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:35,514 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:35,515 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,516 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:35,518 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:35,536 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:35,548 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:35,558 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:35,570 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:35,571 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:35,578 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:35,579 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:35,581 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:35,582 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:35,582 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:35,584 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:35,586 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:35,587 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:35,589 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:35,589 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:35,591 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:35,593 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:35,597 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 24])", "<class 'int'>: 23")
2023-10-11 12:52:35,598 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:35,599 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:35,601 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:35,603 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:35,604 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:35,606 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:35,607 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:35,611 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:35,616 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:35,622 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,623 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,624 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:35,632 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:35,641 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:35,646 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:35,652 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:52:35,653 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:35,655 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:35,660 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:35,666 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,667 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,668 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:35,681 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:35,689 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:35,694 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:35,699 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:52:35,700 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:35,703 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:35,707 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:35,712 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,713 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,714 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:35,721 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:35,726 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:35,733 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:35,747 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:52:35,747 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:35,751 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:35,755 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:35,760 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,761 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,761 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:35,770 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:35,775 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:35,782 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:35,796 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:52:35,797 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:35,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:35,804 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:35,810 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,810 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,811 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:35,819 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:35,825 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:35,831 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:35,844 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:52:35,845 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:35,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:35,852 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:35,857 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,858 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,859 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:35,867 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:35,878 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:35,891 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:35,903 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:52:35,904 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:35,906 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:35,911 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:35,916 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,917 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,918 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:35,928 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:35,934 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:35,940 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:35,945 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:52:35,946 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:35,949 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:35,954 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:35,959 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:35,959 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:35,960 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:35,969 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:35,979 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:35,984 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:35,994 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:52:35,995 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:35,997 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:36,002 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:36,007 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,008 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,009 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:36,016 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:36,027 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:36,033 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:36,038 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:52:36,039 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:36,042 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:36,046 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:36,051 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,052 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,053 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:36,059 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:36,066 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:36,078 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:36,084 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:52:36,086 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:36,089 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:36,094 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:36,099 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,100 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,101 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:36,110 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:36,126 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:36,133 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:36,140 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:52:36,141 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:36,144 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:36,149 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:36,151 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,152 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 24])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 23, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,153 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:36,160 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:36,171 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:36,183 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:36,188 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"))
2023-10-11 12:52:36,189 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:36,192 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:36,193 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:36,195 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,195 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:36,196 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:36,200 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:36,202 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:36,204 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:36,205 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:36,206 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:36,208 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:36,209 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:36,211 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,211 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:36,212 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:36,223 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:36,234 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:36,249 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:36,261 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:36,262 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:36,270 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:36,272 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:36,273 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:36,274 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:36,275 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:36,277 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:36,279 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:36,281 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:36,282 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:36,283 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:36,284 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:36,286 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:36,291 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 25])", "<class 'int'>: 24")
2023-10-11 12:52:36,292 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:36,293 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:36,295 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:36,296 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:36,298 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:36,300 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:36,300 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:36,305 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:36,310 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:36,315 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,315 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,316 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:36,323 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:36,329 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:36,334 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:36,366 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:52:36,367 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:36,369 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:36,387 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:36,392 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,394 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,395 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:36,401 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:36,407 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:36,413 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:36,440 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:52:36,442 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:36,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:36,449 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:36,454 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,455 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,456 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:36,464 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:36,471 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:36,477 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:36,486 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:52:36,487 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:36,490 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:36,494 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:36,499 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,500 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,501 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:36,507 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:36,513 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:36,558 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:36,608 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:52:36,610 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:36,612 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:36,616 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:36,621 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,622 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,623 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:36,631 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:36,637 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:36,642 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:36,653 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:52:36,653 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:36,656 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:36,661 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:36,666 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,667 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,668 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:36,675 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:36,692 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:36,698 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:36,704 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:52:36,705 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:36,707 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:36,712 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:36,717 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,718 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,719 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:36,730 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:36,736 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:36,745 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:36,750 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:52:36,751 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:36,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:36,759 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:36,764 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,765 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,765 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:36,772 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:36,777 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:36,782 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:36,791 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:52:36,792 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:36,795 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:36,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:36,804 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,805 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,806 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:36,813 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:36,819 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:36,832 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:36,843 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:52:36,844 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:36,846 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:36,851 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:36,855 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,856 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,857 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:36,864 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:36,870 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:36,876 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:36,887 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:52:36,889 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:36,891 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:36,895 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:36,900 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,900 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,901 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:36,909 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:36,915 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:36,923 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:36,929 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:52:36,930 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:36,933 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:36,937 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:36,939 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,940 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 25])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 24, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:36,941 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:36,947 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:36,953 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:36,958 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:36,964 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"))
2023-10-11 12:52:36,965 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:36,968 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:36,969 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:36,971 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,971 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:36,972 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:36,975 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:36,979 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:36,982 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:36,990 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:36,991 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:36,992 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:36,993 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:36,995 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:36,996 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:36,997 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:37,008 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:37,021 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:37,031 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:37,041 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:37,042 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:37,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:37,050 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:37,051 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:37,052 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:37,053 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:37,055 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:37,057 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:37,058 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:37,060 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:37,061 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:37,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:37,063 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:37,068 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 26])", "<class 'int'>: 25")
2023-10-11 12:52:37,069 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:37,069 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:37,071 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:37,073 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:37,075 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:37,077 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:37,078 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:37,083 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:37,088 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:37,099 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,100 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:37,102 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:37,161 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:37,179 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:37,191 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:37,197 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:52:37,198 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:37,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:37,205 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:37,210 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,211 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:37,212 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:37,219 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:37,225 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:37,231 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:37,236 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:52:37,237 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:37,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:37,244 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:37,249 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,250 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:37,250 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:37,257 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:37,263 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:37,269 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:37,275 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:52:37,276 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:37,278 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:37,283 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:37,288 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,289 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:37,290 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:37,299 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:37,307 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:37,313 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:37,319 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:52:37,320 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:37,322 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:37,327 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:37,332 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,332 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:37,333 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:37,340 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:37,346 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:37,352 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:37,359 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:52:37,360 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:37,363 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:37,368 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:37,373 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,374 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:37,375 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:37,408 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:37,414 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:37,420 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:37,426 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:52:37,427 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:37,430 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:37,434 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:37,439 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,440 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:37,441 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:37,448 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:37,459 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:37,464 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:37,469 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:52:37,470 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:37,472 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:37,477 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:37,482 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,482 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:37,483 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:37,490 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:37,496 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:37,502 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:37,508 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:52:37,508 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:37,511 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:37,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:37,520 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,521 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:37,522 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:37,535 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:37,567 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:37,577 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:37,584 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:52:37,585 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:37,589 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:37,593 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:37,599 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,600 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:37,601 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:37,614 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:37,621 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:37,627 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:37,634 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:52:37,635 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:37,637 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:37,642 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:37,647 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,648 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:37,648 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:37,657 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:37,665 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:37,748 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:37,764 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:52:37,765 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:37,768 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:37,773 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:37,775 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,776 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 26])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 25, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:37,777 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:37,784 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:37,790 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:37,796 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:37,802 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"))
2023-10-11 12:52:37,803 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:37,806 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:37,807 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:37,808 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,809 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:37,810 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:37,812 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:37,815 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:37,817 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:37,819 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:37,820 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:37,822 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:37,823 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:37,824 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,825 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:37,826 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:37,840 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:37,851 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:37,862 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:37,877 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:37,878 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:37,889 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:37,890 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:37,892 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:37,893 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:37,894 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:37,896 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:37,897 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:37,899 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:37,901 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:37,901 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:37,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:37,904 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:37,909 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 27])", "<class 'int'>: 26")
2023-10-11 12:52:37,910 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:37,911 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:37,913 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:37,914 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:37,916 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:37,917 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:37,918 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:37,922 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:37,927 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:37,931 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,932 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:37,933 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:37,942 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:37,947 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:37,953 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:37,959 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:52:37,960 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:37,963 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:37,969 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:37,974 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:37,975 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:37,976 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:37,983 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:38,001 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:38,015 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:38,021 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:52:38,022 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:38,024 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:38,029 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:38,034 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,035 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,035 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:38,044 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:38,061 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:38,067 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:38,073 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:52:38,074 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:38,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:38,081 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:38,085 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,086 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,087 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:38,093 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:38,098 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:38,108 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:38,114 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:52:38,115 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:38,118 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:38,122 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:38,127 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,127 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,128 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:38,135 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:38,141 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:38,147 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:38,153 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:52:38,154 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:38,157 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:38,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:38,167 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,168 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,169 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:38,176 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:38,181 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:38,187 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:38,192 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:52:38,193 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:38,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:38,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:38,206 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,207 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,208 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:38,217 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:38,223 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:38,228 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:38,233 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:52:38,234 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:38,237 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:38,241 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:38,246 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,247 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,248 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:38,254 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:38,292 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:38,299 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:38,311 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:52:38,311 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:38,314 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:38,318 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:38,323 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,324 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,325 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:38,332 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:38,339 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:38,345 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:38,351 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:52:38,352 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:38,355 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:38,359 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:38,363 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,364 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,365 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:38,372 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:38,378 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:38,383 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:38,389 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:52:38,389 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:38,392 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:38,396 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:38,401 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,402 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,403 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:38,411 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:38,417 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:38,423 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:38,430 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:52:38,431 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:38,433 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:38,438 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:38,440 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,441 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 27])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 26, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,441 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:38,449 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:38,455 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:38,460 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:38,466 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"))
2023-10-11 12:52:38,467 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:38,469 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:38,471 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:38,472 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,473 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:38,474 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:38,476 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:38,478 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:38,480 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:38,483 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:38,484 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:38,485 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:38,486 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:38,488 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,488 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:38,489 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:38,502 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:38,515 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:38,525 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:38,534 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:38,536 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:38,543 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:38,544 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:38,546 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:38,547 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:38,548 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:38,549 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:38,551 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:38,552 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:38,554 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:38,554 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:38,556 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:38,558 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:38,562 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 28])", "<class 'int'>: 27")
2023-10-11 12:52:38,563 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:38,564 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:38,566 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:38,568 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:38,569 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:38,571 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:38,572 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:38,576 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:38,581 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:38,585 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,586 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,587 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:38,662 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:38,697 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:38,703 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:38,710 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:52:38,711 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:38,713 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:38,718 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:38,723 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,724 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,725 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:38,736 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:38,743 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:38,774 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:38,784 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:52:38,785 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:38,788 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:38,793 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:38,798 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,799 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,801 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:38,812 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:38,819 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:38,825 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:38,831 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:52:38,831 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:38,834 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:38,839 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:38,843 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,844 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,845 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:38,857 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:38,863 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:38,869 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:38,875 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:52:38,876 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:38,878 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:38,883 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:38,888 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,889 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,890 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:38,899 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:38,907 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:38,915 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:38,922 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:52:38,923 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:38,926 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:38,931 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:38,936 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:38,936 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:38,937 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:39,005 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:39,045 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:39,057 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:39,066 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:52:39,067 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:39,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:39,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:39,084 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,085 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,087 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:39,099 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:39,106 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:39,112 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:39,119 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:52:39,121 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:39,124 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:39,129 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:39,135 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,136 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,137 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:39,144 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:39,151 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:39,160 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:39,166 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:52:39,167 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:39,169 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:39,174 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:39,179 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,180 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,181 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:39,189 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:39,195 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:39,201 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:39,206 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:52:39,207 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:39,210 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:39,214 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:39,219 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,220 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,221 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:39,228 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:39,235 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:39,241 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:39,246 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:52:39,247 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:39,250 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:39,254 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:39,259 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,260 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,261 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:39,287 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:39,294 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:39,307 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:39,314 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:52:39,316 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:39,318 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:39,323 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:39,324 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,325 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 28])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 27, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,326 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:39,333 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:39,343 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:39,352 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:39,367 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"))
2023-10-11 12:52:39,367 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:39,370 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:39,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:39,373 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,374 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:39,374 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:39,376 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:39,379 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:39,381 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:39,383 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:39,384 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:39,385 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:39,387 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:39,388 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,389 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:39,389 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:39,401 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:39,414 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:39,423 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:39,433 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:39,434 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:39,442 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:39,443 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:39,445 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:39,446 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:39,447 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:39,449 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:39,451 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:39,452 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:39,453 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:39,454 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:39,456 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:39,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:39,462 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 29])", "<class 'int'>: 28")
2023-10-11 12:52:39,463 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:39,464 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:39,466 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:39,467 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:39,469 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:39,471 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:39,472 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:39,476 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:39,480 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:39,485 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,486 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,487 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:39,494 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:39,499 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:39,506 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:39,512 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:52:39,513 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:39,515 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:39,520 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:39,525 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,526 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,527 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:39,534 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:39,539 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:39,552 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:39,558 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:52:39,558 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:39,561 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:39,565 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:39,570 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,572 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,572 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:39,581 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:39,587 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:39,593 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:39,598 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:52:39,599 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:39,602 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:39,606 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:39,611 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,612 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,612 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:39,622 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:39,630 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:39,635 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:39,642 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:52:39,643 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:39,645 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:39,649 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:39,654 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,655 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,656 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:39,667 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:39,673 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:39,682 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:39,688 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:52:39,688 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:39,692 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:39,696 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:39,701 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,702 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,703 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:39,714 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:39,726 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:39,733 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:39,740 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:52:39,741 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:39,743 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:39,747 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:39,752 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,753 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,754 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:39,761 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:39,767 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:39,783 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:39,794 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:52:39,795 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:39,797 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:39,802 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:39,807 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,807 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,808 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:39,815 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:39,824 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:39,830 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:39,835 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:52:39,836 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:39,839 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:39,843 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:39,849 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,850 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,852 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:39,860 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:39,867 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:39,873 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:39,879 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:52:39,880 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:39,889 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:39,899 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:39,912 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,913 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,914 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:39,921 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:39,929 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:39,939 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:39,948 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:52:39,949 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:39,952 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:39,958 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:39,963 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:39,964 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:39,965 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:39,975 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:39,983 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:39,993 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:40,000 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:52:40,001 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:40,004 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:40,009 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:40,011 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,012 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 29])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 28, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:40,013 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:40,021 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:40,029 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:40,037 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:40,045 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"))
2023-10-11 12:52:40,046 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:40,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:40,051 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:40,053 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,054 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:40,055 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:40,060 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:40,063 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:40,067 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:40,070 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:40,071 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:40,073 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:40,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:40,076 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,076 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:40,077 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:40,102 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:40,123 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:40,141 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:40,157 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:40,158 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:40,185 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:40,186 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:40,188 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:40,189 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:40,189 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:40,191 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:40,192 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:40,194 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:40,195 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:40,196 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:40,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:40,199 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:40,203 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 30])", "<class 'int'>: 29")
2023-10-11 12:52:40,204 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:40,205 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:40,207 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:40,208 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:40,210 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:40,211 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:40,212 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:40,216 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:40,221 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:40,226 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,227 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:40,227 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:40,235 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:40,245 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:40,251 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:40,258 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:52:40,259 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:40,261 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:40,265 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:40,270 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,271 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:40,271 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:40,280 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:40,294 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:40,301 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:40,306 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:52:40,307 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:40,310 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:40,315 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:40,319 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,320 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:40,321 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:40,339 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:40,348 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:40,355 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:40,360 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:52:40,361 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:40,364 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:40,368 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:40,373 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,374 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:40,374 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:40,382 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:40,388 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:40,400 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:40,406 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:52:40,407 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:40,409 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:40,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:40,419 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,419 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:40,420 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:40,431 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:40,437 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:40,442 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:40,450 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:52:40,451 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:40,453 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:40,458 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:40,462 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,463 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:40,464 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:40,472 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:40,480 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:40,486 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:40,492 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:52:40,493 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:40,495 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:40,500 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:40,505 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,506 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:40,507 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:40,514 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:40,601 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:40,615 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:40,622 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:52:40,623 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:40,625 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:40,630 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:40,634 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,635 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:40,636 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:40,643 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:40,648 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:40,655 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:40,660 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:52:40,661 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:40,663 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:40,667 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:40,672 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,673 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:40,674 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:40,687 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:40,696 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:40,702 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:40,756 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:52:40,757 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:40,761 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:40,766 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:40,770 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,771 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:40,772 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:40,814 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:40,833 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:40,840 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:40,845 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:52:40,846 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:40,848 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:40,853 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:40,858 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,859 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:40,860 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:40,873 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:40,878 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:40,888 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:40,895 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:52:40,896 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:40,899 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:40,903 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:40,905 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,906 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 30])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 29, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:40,907 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:40,915 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:40,921 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:40,928 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:40,936 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"))
2023-10-11 12:52:40,937 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:40,939 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:40,940 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:40,942 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,942 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:40,943 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:40,946 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:40,947 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:40,949 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:40,951 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:40,952 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:40,953 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:40,955 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:40,956 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:40,957 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:40,957 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:40,969 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:40,980 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:40,990 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:41,000 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:41,001 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:41,008 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:41,010 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:41,011 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:41,012 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:41,012 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:41,014 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:41,016 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:41,017 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:41,019 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:41,020 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:41,021 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:41,022 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:41,027 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 31])", "<class 'int'>: 30")
2023-10-11 12:52:41,028 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:41,029 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:41,031 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:41,032 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:41,034 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:41,035 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:41,036 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:41,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:41,045 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:41,050 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,051 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,052 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:41,058 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:41,066 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:41,090 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:41,098 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:52:41,099 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:41,102 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:41,107 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:41,112 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,113 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,114 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:41,123 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:41,135 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:41,140 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:41,147 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:52:41,148 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:41,150 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:41,155 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:41,159 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,160 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,161 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:41,168 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:41,178 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:41,184 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:41,191 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:52:41,192 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:41,194 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:41,198 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:41,203 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,204 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,205 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:41,213 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:41,219 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:41,229 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:41,235 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:52:41,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:41,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:41,244 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:41,249 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,250 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,251 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:41,261 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:41,270 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:41,302 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:41,309 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:52:41,310 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:41,312 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:41,317 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:41,321 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,322 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,323 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:41,347 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:41,356 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:41,361 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:41,370 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:52:41,371 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:41,373 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:41,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:41,383 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,383 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,384 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:41,391 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:41,404 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:41,409 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:41,417 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:52:41,418 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:41,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:41,425 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:41,430 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,430 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,431 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:41,440 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:41,447 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:41,452 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:41,458 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:52:41,458 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:41,461 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:41,465 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:41,470 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,471 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,471 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:41,478 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:41,484 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:41,495 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:41,502 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:52:41,503 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:41,505 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:41,509 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:41,514 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,515 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,516 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:41,524 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:41,533 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:41,540 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:41,545 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:52:41,547 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:41,550 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:41,554 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:41,559 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,560 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,561 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:41,570 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:41,593 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:41,602 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:41,612 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:52:41,613 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:41,615 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:41,619 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:41,621 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,622 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 31])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 30, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,623 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:41,629 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:41,638 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:41,644 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:41,655 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"))
2023-10-11 12:52:41,656 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:41,658 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:41,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:41,660 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,661 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:41,662 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:41,664 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:41,666 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:41,668 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:41,670 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:41,670 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:41,672 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:41,673 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:41,674 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,675 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:41,676 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:41,687 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:41,699 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:41,712 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:41,728 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:41,729 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:41,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:41,739 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:41,740 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:41,741 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:41,742 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:41,744 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:41,745 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:41,746 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:41,748 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:41,749 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:41,750 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:41,752 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:41,756 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 32])", "<class 'int'>: 31")
2023-10-11 12:52:41,757 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:41,758 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:41,760 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:41,761 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:41,763 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:41,765 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:41,765 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:41,770 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:41,775 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:41,780 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,781 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,782 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:41,789 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:41,879 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:41,894 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:41,910 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:52:41,911 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:41,914 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:41,919 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:41,923 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,924 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,925 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:41,934 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:41,943 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:41,949 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:41,955 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:52:41,956 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:41,958 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:41,963 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:41,968 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:41,968 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:41,969 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:41,977 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:41,985 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:42,000 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:42,025 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:52:42,026 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:42,029 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:42,033 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:42,038 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,038 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,039 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:42,046 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:42,056 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:42,061 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:42,067 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:52:42,068 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:42,070 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:42,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:42,079 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,080 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,080 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:42,088 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:42,093 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:42,099 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:42,105 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:52:42,106 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:42,108 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:42,113 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:42,117 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,118 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,119 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:42,133 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:42,140 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:42,151 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:42,158 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:52:42,159 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:42,161 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:42,166 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:42,171 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,172 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,173 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:42,180 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:42,187 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:42,192 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:42,198 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:52:42,199 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:42,202 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:42,206 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:42,211 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,212 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,213 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:42,223 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:42,228 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:42,233 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:42,239 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:52:42,239 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:42,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:42,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:42,252 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,253 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,254 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:42,261 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:42,267 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:42,272 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:42,278 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:52:42,279 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:42,282 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:42,292 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:42,297 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,298 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,299 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:42,307 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:42,313 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:42,319 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:42,373 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:52:42,374 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:42,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:42,381 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:42,386 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,387 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,388 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:42,419 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:42,428 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:42,434 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:42,440 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:52:42,441 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:42,444 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:42,448 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:42,450 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,451 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 32])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 31, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,452 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:42,503 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:42,519 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:42,526 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:42,532 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"))
2023-10-11 12:52:42,532 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:42,535 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:42,537 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:42,538 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,539 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:42,539 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:42,542 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:42,545 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:42,555 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:42,562 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:42,564 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:42,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:42,567 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:42,569 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,570 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:42,571 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:42,600 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:42,618 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:42,634 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:42,644 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:42,646 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:42,653 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:42,654 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:42,656 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:42,657 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:42,658 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:42,660 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:42,661 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:42,663 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:42,665 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:42,666 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:42,667 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:42,669 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:42,674 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 33])", "<class 'int'>: 32")
2023-10-11 12:52:42,674 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:42,675 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:42,677 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:42,679 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:42,680 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:42,682 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:42,683 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:42,687 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:42,691 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:42,696 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,698 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,698 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:42,720 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:42,728 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:42,735 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:42,744 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:52:42,745 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:42,747 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:42,752 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:42,756 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,757 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,758 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:42,770 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:42,777 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:42,783 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:42,789 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:52:42,790 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:42,793 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:42,797 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:42,801 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,802 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,803 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:42,811 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:42,817 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:42,823 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:42,829 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:52:42,830 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:42,832 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:42,837 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:42,841 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,842 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,843 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:42,850 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:42,857 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:42,865 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:42,873 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:52:42,874 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:42,876 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:42,881 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:42,887 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,888 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,888 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:42,896 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:42,905 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:42,913 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:42,920 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:52:42,920 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:42,923 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:42,927 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:42,932 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,933 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,933 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:42,948 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:42,954 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:42,973 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:42,982 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:52:42,983 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:42,986 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:42,990 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:42,995 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:42,996 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:42,997 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:43,004 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:43,012 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:43,018 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:43,027 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:52:43,027 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:43,030 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:43,034 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:43,039 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,040 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:43,040 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:43,047 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:43,055 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:43,062 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:43,068 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:52:43,069 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:43,071 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:43,076 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:43,081 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,082 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:43,083 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:43,089 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:43,098 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:43,107 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:43,115 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:52:43,116 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:43,119 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:43,123 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:43,128 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,129 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:43,130 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:43,140 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:43,147 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:43,153 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:43,159 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:52:43,159 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:43,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:43,166 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:43,171 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,172 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:43,173 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:43,180 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:43,186 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:43,193 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:43,199 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:52:43,200 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:43,202 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:43,207 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:43,208 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,209 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 33])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 32, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:43,210 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:43,235 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:43,249 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:43,255 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:43,263 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"))
2023-10-11 12:52:43,264 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:43,267 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:43,269 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:43,270 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,271 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:43,272 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:43,274 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:43,276 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:43,281 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:43,284 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:43,291 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:43,293 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:43,294 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:43,296 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,297 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:43,298 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:43,317 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:43,335 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:43,349 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:43,362 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:43,364 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:43,389 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:43,390 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:43,391 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:43,392 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:43,393 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:43,396 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:43,398 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:43,399 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:43,400 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:43,401 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:43,403 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:43,404 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:43,409 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 34])", "<class 'int'>: 33")
2023-10-11 12:52:43,410 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:43,411 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:43,413 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:43,414 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:43,416 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:43,418 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:43,418 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:43,423 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:43,427 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:43,431 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,432 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:43,433 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:43,440 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:43,454 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:43,461 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:43,475 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:52:43,476 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:43,478 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:43,483 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:43,487 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,488 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:43,489 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:43,498 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:43,509 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:43,518 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:43,524 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:52:43,525 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:43,527 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:43,531 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:43,536 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,537 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:43,538 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:43,552 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:43,558 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:43,563 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:43,570 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:52:43,571 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:43,573 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:43,578 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:43,583 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,584 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:43,585 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:43,592 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:43,598 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:43,607 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:43,619 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:52:43,620 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:43,622 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:43,627 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:43,632 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,632 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:43,633 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:43,641 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:43,648 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:43,737 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:43,789 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:52:43,792 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:43,794 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:43,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:43,804 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,805 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:43,806 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:43,831 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:43,839 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:43,844 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:43,850 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:52:43,851 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:43,853 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:43,858 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:43,863 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,864 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:43,865 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:43,872 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:43,897 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:43,903 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:43,908 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:52:43,909 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:43,912 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:43,916 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:43,921 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,922 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:43,923 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:43,933 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:43,941 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:43,946 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:43,959 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:52:43,960 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:43,962 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:43,966 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:43,971 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:43,972 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:43,972 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:44,003 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:44,012 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:44,018 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:44,025 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:52:44,026 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:44,029 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:44,033 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:44,038 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,038 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,039 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:44,046 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:44,053 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:44,058 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:44,064 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:52:44,065 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:44,067 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:44,072 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:44,076 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,077 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,078 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:44,086 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:44,094 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:44,100 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:44,106 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:52:44,107 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:44,109 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:44,114 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:44,115 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,116 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 34])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 33, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,117 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:44,128 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:44,136 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:44,142 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:44,148 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"))
2023-10-11 12:52:44,149 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:44,151 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:44,153 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:44,154 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,155 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:44,156 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:44,161 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:44,165 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:44,168 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:44,171 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:44,173 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:44,175 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:44,176 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:44,177 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,178 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:44,179 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:44,193 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:44,208 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:44,218 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:44,229 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:44,231 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:44,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:44,240 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:44,241 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:44,242 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:44,243 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:44,245 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:44,246 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:44,248 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:44,249 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:44,250 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:44,252 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:44,254 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:44,259 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 35])", "<class 'int'>: 34")
2023-10-11 12:52:44,261 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:44,262 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:44,264 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:44,265 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:44,267 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:44,269 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:44,269 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:44,274 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:44,278 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:44,283 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,284 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,285 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:44,293 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:44,299 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:44,307 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:44,318 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:52:44,319 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:44,322 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:44,327 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:44,331 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,332 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,333 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:44,342 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:44,349 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:44,357 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:44,364 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:52:44,365 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:44,367 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:44,372 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:44,376 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,377 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,379 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:44,387 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:44,392 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:44,406 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:44,412 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:52:44,413 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:44,415 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:44,420 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:44,424 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,425 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,426 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:44,432 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:44,445 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:44,450 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:44,460 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:52:44,461 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:44,463 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:44,468 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:44,473 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,474 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,475 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:44,483 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:44,497 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:44,503 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:44,508 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:52:44,509 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:44,511 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:44,516 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:44,521 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,522 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,523 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:44,538 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:44,632 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:44,648 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:44,654 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:52:44,655 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:44,657 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:44,662 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:44,667 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,668 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,669 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:44,677 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:44,684 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:44,690 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:44,695 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:52:44,696 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:44,699 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:44,704 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:44,709 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,710 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,711 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:44,724 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:44,729 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:44,735 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:44,740 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:52:44,741 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:44,743 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:44,748 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:44,753 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,753 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,754 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:44,761 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:44,771 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:44,777 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:44,786 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:52:44,787 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:44,789 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:44,794 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:44,799 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,800 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,801 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:44,808 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:44,837 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:44,849 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:44,856 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:52:44,858 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:44,860 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:44,865 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:44,871 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,872 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,873 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:44,882 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:44,902 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:44,910 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:44,917 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:52:44,919 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:44,922 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:44,926 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:44,928 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,929 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 35])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 34, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:44,930 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:44,937 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:44,946 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:44,952 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:44,960 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"))
2023-10-11 12:52:44,961 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:44,963 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:44,965 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:44,966 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,967 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:44,968 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:44,972 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:44,975 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:44,979 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:44,983 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:44,984 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:44,986 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:44,987 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:44,988 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:44,989 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:44,990 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:45,004 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:45,013 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:45,025 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:45,037 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:45,039 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:45,047 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:45,049 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:45,050 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:45,051 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:45,052 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:45,053 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:45,055 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:45,057 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:45,058 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:45,059 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:45,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:45,062 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:45,067 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 36])", "<class 'int'>: 35")
2023-10-11 12:52:45,068 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:45,069 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:45,071 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:45,072 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:45,074 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:45,075 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:45,076 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:45,080 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:45,085 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:45,090 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,091 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:45,091 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:45,101 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:45,114 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:45,120 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:45,137 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:52:45,137 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:45,140 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:45,144 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:45,149 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,150 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:45,151 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:45,158 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:45,167 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:45,173 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:45,179 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:52:45,180 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:45,183 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:45,187 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:45,192 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,193 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:45,194 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:45,201 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:45,209 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:45,214 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:45,222 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:52:45,222 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:45,225 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:45,229 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:45,234 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,234 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:45,235 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:45,245 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:45,255 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:45,264 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:45,272 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:52:45,273 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:45,275 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:45,280 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:45,285 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,286 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:45,287 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:45,361 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:45,402 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:45,413 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:45,420 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:52:45,421 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:45,423 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:45,428 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:45,433 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,433 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:45,434 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:45,443 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:45,451 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:45,456 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:45,463 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:52:45,464 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:45,467 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:45,471 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:45,477 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,478 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:45,479 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:45,542 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:45,553 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:45,564 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:45,570 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:52:45,571 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:45,573 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:45,578 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:45,582 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,583 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:45,584 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:45,591 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:45,597 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:45,602 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:45,609 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:52:45,610 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:45,613 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:45,618 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:45,624 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,625 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:45,626 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:45,636 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:45,648 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:45,653 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:45,659 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:52:45,660 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:45,663 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:45,668 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:45,673 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,674 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:45,675 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:45,688 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:45,694 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:45,703 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:45,710 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:52:45,711 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:45,713 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:45,717 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:45,722 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,723 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:45,724 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:45,731 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:45,737 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:45,745 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:45,751 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:52:45,752 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:45,754 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:45,758 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:45,760 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,761 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 36])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 35, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:45,762 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:45,770 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:45,776 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:45,784 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:45,791 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"))
2023-10-11 12:52:45,792 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:45,795 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:45,796 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:45,797 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,798 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:45,799 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:45,803 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:45,806 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:45,808 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:45,810 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:45,810 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:45,812 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:45,814 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:45,815 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,816 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:45,817 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:45,831 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:45,844 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:45,854 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:45,867 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:45,868 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:45,875 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:45,877 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:45,878 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:45,879 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:45,880 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:45,882 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:45,884 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:45,886 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:45,887 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:45,888 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:45,890 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:45,891 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:45,896 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 37])", "<class 'int'>: 36")
2023-10-11 12:52:45,897 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:45,898 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:45,900 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:45,901 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:45,903 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:45,905 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:45,906 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:45,910 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:45,915 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:45,920 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:45,920 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:45,921 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:45,933 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:45,940 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:45,952 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:46,051 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:52:46,053 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:46,056 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:46,061 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:46,066 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:46,067 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:46,068 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:46,095 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:46,103 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:46,180 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:46,194 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:52:46,195 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:46,197 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:46,202 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:46,206 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:46,208 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:46,209 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:46,221 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:46,226 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:46,233 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:46,239 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:52:46,240 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:46,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:46,246 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:46,251 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:46,252 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:46,253 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:46,260 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:46,266 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:46,272 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:46,279 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:52:46,280 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:46,282 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:46,286 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:46,291 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:46,292 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:46,293 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:46,302 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:46,314 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:46,320 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:46,326 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:52:46,327 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:46,329 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:46,334 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:46,338 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:46,339 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:46,340 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:46,346 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:46,362 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:46,367 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:46,373 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:52:46,374 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:46,376 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:46,381 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:46,386 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:46,387 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:46,387 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:46,469 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:46,487 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:46,499 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:46,506 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:52:46,506 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:46,509 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:46,513 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:46,519 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:46,520 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:46,520 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:46,575 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:46,608 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:46,625 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:46,631 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:52:46,632 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:46,634 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:46,639 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:46,643 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:46,644 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:46,645 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:46,652 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:46,659 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:46,664 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:46,673 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:52:46,674 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:46,676 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:46,680 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:46,685 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:46,686 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:46,687 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:46,694 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:46,699 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:46,719 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:46,735 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:52:46,737 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:46,739 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:46,744 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:46,749 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:46,750 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:46,751 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:46,758 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:46,763 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:46,769 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:46,775 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:52:46,776 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:46,778 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:46,784 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:46,786 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:46,787 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 37])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 36, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:46,787 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:46,794 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:46,801 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:46,807 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:46,814 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"))
2023-10-11 12:52:46,815 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:46,817 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:46,819 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:46,820 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:46,821 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:46,822 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:46,826 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:46,828 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:46,830 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:46,832 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:46,833 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:46,835 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:46,836 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:46,837 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:46,838 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:46,839 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:46,850 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:46,863 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:46,873 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:46,884 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:46,885 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:46,892 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:46,893 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:46,895 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 1])",)
2023-10-11 12:52:46,896 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:46,896 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:52:46,898 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:52:46,900 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:52:46,901 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:52:46,903 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:46,904 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:52:46,905 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:52:46,907 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:46,912 [2192271695.py:22 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 38])", "<class 'int'>: 37")
2023-10-11 12:52:46,920 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:46,921 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 0
2023-10-11 12:52:46,923 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 1
2023-10-11 12:52:46,924 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 2
2023-10-11 12:52:46,925 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.embed_positions, batch: 3
2023-10-11 12:52:46,927 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:46,928 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:52:46,933 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:52:46,937 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:46,942 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:46,943 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:46,943 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 0
2023-10-11 12:52:46,981 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 1
2023-10-11 12:52:47,019 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 2
2023-10-11 12:52:47,027 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.0, batch: 3
2023-10-11 12:52:47,033 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:52:47,034 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:52:47,037 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:52:47,041 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:47,046 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:47,046 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:47,047 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 0
2023-10-11 12:52:47,054 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 1
2023-10-11 12:52:47,060 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 2
2023-10-11 12:52:47,065 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.1, batch: 3
2023-10-11 12:52:47,071 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:52:47,072 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:52:47,074 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:52:47,078 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:47,083 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:47,084 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:47,085 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 0
2023-10-11 12:52:47,092 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 1
2023-10-11 12:52:47,165 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 2
2023-10-11 12:52:47,187 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.2, batch: 3
2023-10-11 12:52:47,193 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:52:47,194 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:52:47,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:52:47,200 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:47,205 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:47,205 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:47,206 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 0
2023-10-11 12:52:47,215 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 1
2023-10-11 12:52:47,220 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 2
2023-10-11 12:52:47,227 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.3, batch: 3
2023-10-11 12:52:47,237 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:52:47,238 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:52:47,241 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:52:47,245 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:47,250 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:47,251 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:47,252 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 0
2023-10-11 12:52:47,258 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 1
2023-10-11 12:52:47,267 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 2
2023-10-11 12:52:47,275 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.4, batch: 3
2023-10-11 12:52:47,282 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:52:47,282 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:52:47,285 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:52:47,290 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:47,295 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:47,296 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:47,297 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 0
2023-10-11 12:52:47,313 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 1
2023-10-11 12:52:47,343 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 2
2023-10-11 12:52:47,350 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.5, batch: 3
2023-10-11 12:52:47,357 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:52:47,358 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:52:47,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:52:47,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:47,370 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:47,371 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:47,372 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 0
2023-10-11 12:52:47,379 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 1
2023-10-11 12:52:47,386 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 2
2023-10-11 12:52:47,392 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.6, batch: 3
2023-10-11 12:52:47,399 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:52:47,399 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:52:47,402 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:52:47,406 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:47,411 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:47,412 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:47,413 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 0
2023-10-11 12:52:47,423 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 1
2023-10-11 12:52:47,430 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 2
2023-10-11 12:52:47,438 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.7, batch: 3
2023-10-11 12:52:47,444 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:52:47,445 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:52:47,447 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:52:47,452 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:47,456 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:47,457 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:47,458 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 0
2023-10-11 12:52:47,465 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 1
2023-10-11 12:52:47,472 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 2
2023-10-11 12:52:47,479 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.8, batch: 3
2023-10-11 12:52:47,492 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:52:47,493 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:52:47,496 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:52:47,501 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:47,506 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:47,507 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:47,508 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 0
2023-10-11 12:52:47,514 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 1
2023-10-11 12:52:47,520 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 2
2023-10-11 12:52:47,528 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.9, batch: 3
2023-10-11 12:52:47,534 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:52:47,535 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:52:47,537 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:52:47,541 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:47,546 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:47,547 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:47,548 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 0
2023-10-11 12:52:47,554 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 1
2023-10-11 12:52:47,562 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 2
2023-10-11 12:52:47,571 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.10, batch: 3
2023-10-11 12:52:47,592 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:52:47,593 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:52:47,595 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:52:47,599 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:47,601 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:47,602 [2192271695.py:23 in new_forward] DEBUG - kwargs: {'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 1, 38])", 'layer_head_mask': "<class 'NoneType'>: None", 'past_key_value': ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 37, 52])"), 'output_attentions': "<class 'bool'>: False", 'use_cache': "<class 'bool'>: True"}
2023-10-11 12:52:47,603 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 0
2023-10-11 12:52:47,610 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 1
2023-10-11 12:52:47,694 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 2
2023-10-11 12:52:47,709 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.layers.11, batch: 3
2023-10-11 12:52:47,714 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])", ("<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])", "<class '__main__.BatchMixTensor'>: torch.Size([8, 12, 38, 52])"))
2023-10-11 12:52:47,715 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:52:47,717 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:52:47,719 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:47,720 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:47,721 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:47,721 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 0
2023-10-11 12:52:47,725 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 1
2023-10-11 12:52:47,728 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 2
2023-10-11 12:52:47,731 [2192271695.py:27 in new_forward] DEBUG - layer: model.decoder.final_layer_norm, batch: 3
2023-10-11 12:52:47,735 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])
2023-10-11 12:52:47,736 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:52:47,737 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:52:47,738 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:52:47,740 [2192271695.py:22 in new_forward] DEBUG - args: ("<class '__main__.BatchMixTensor'>: torch.Size([8, 1, 384])",)
2023-10-11 12:52:47,740 [2192271695.py:23 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:52:47,741 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 0
2023-10-11 12:52:47,753 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 1
2023-10-11 12:52:47,763 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 2
2023-10-11 12:52:47,776 [2192271695.py:27 in new_forward] DEBUG - layer: lm_head, batch: 3
2023-10-11 12:52:47,787 [2192271695.py:41 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 1, 25136])
2023-10-11 12:52:47,788 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:52:47,798 [test.py:40 in test_hf_gen] INFO - for i in range(10):  ( ( (
,,,,,,,,,,,,,,,,, and and and and and and and and and
2023-10-11 12:52:47,799 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:52:47,799 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious???,...   ...                  
2023-10-11 12:52:47,801 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:52:47,801 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?ooooooooooooooooooo's,, and and and and and and and and
2023-10-11 12:52:47,802 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:52:47,803 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?oooooo   ...                  
2023-10-11 12:52:47,804 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:52:47,805 [test.py:40 in test_hf_gen] INFO - for i in range(10):  ( to
:::,,,,,,,,,,,,,,,, and and and and and and and and
2023-10-11 12:52:47,806 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:52:47,807 [test.py:40 in test_hf_gen] INFO - Who are you? Are you conscious??                             
2023-10-11 12:52:47,808 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:52:47,809 [test.py:40 in test_hf_gen] INFO - Where is Deutschland?oooooooooooooooooo's's, and and and and and and and and and
2023-10-11 12:52:47,810 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:52:47,811 [test.py:40 in test_hf_gen] INFO - How is Huawei Mate 60 Pro?ooooo                         
2023-10-11 12:52:47,811 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:52:47,825 [520681597.py:17 in reset_forward] DEBUG - model.decoder.embed_tokens from flexgen to old.
2023-10-11 12:52:47,825 [520681597.py:17 in reset_forward] DEBUG - model.decoder.embed_positions from flexgen to old.
2023-10-11 12:52:47,826 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.0 from flexgen to old.
2023-10-11 12:52:47,828 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.1 from flexgen to old.
2023-10-11 12:52:47,828 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.2 from flexgen to old.
2023-10-11 12:52:47,829 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.3 from flexgen to old.
2023-10-11 12:52:47,830 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.4 from flexgen to old.
2023-10-11 12:52:47,831 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.5 from flexgen to old.
2023-10-11 12:52:47,832 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.6 from flexgen to old.
2023-10-11 12:52:47,833 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.7 from flexgen to old.
2023-10-11 12:52:47,834 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.8 from flexgen to old.
2023-10-11 12:52:47,836 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.9 from flexgen to old.
2023-10-11 12:52:47,837 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.10 from flexgen to old.
2023-10-11 12:52:47,837 [520681597.py:17 in reset_forward] DEBUG - model.decoder.layers.11 from flexgen to old.
2023-10-11 12:52:47,838 [520681597.py:17 in reset_forward] DEBUG - model.decoder.final_layer_norm from flexgen to old.
2023-10-11 12:52:47,839 [520681597.py:17 in reset_forward] DEBUG - lm_head from flexgen to old.
2023-10-11 12:55:42,575 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:55:42,709 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:55:42,800 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:55:42,899 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:55:42,992 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:55:42,994 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'
2023-10-11 12:55:43,002 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-11 12:55:43,003 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-11 12:55:43,004 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-11 12:55:43,006 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-11 12:55:43,008 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-11 12:55:43,010 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-11 12:55:43,011 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-11 12:55:43,013 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-11 12:55:43,015 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-11 12:55:43,017 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-11 12:55:43,018 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-11 12:55:43,020 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-11 12:55:43,021 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-11 12:55:43,024 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-11 12:55:43,025 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:55:43,027 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:55:43,028 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-11 12:55:43,031 [model.py:148 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-11 12:55:43,033 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-11 12:55:43,060 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-11 12:55:43,061 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-11 12:55:43,061 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-11 12:55:43,062 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-11 12:55:43,064 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-11 12:55:43,065 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-11 12:55:43,066 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-11 12:55:43,067 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-11 12:55:43,068 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-11 12:55:43,069 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-11 12:55:43,070 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-11 12:55:43,071 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-11 12:55:43,072 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-11 12:55:43,073 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-11 12:55:43,074 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-11 12:55:43,074 [520681597.py:42 in to_test_forward] DEBUG - lm_head to test forward
2023-10-11 12:55:43,118 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:55:43,315 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:55:43,317 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:55:43,319 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:55:43,321 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:55:43,322 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:55:43,333 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:55:43,337 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:55:43,346 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:55:43,349 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:55:43,358 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:55:43,360 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:55:43,367 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:55:43,370 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:55:43,377 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:55:43,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:55:43,386 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:55:43,388 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:55:43,396 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:55:43,398 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:55:43,405 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:55:43,408 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:55:43,415 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:55:43,417 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:55:43,425 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:55:43,427 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:55:43,434 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:55:43,437 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:55:43,444 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:55:43,446 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:55:43,450 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:55:43,451 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:55:43,461 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:55:43,467 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-11 12:55:43,468 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:55:43,479 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-11 12:55:43,480 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-11 12:55:43,480 [520681597.py:22 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-11 12:55:43,482 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-11 12:55:43,483 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-11 12:55:43,484 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-11 12:55:43,484 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-11 12:55:43,485 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-11 12:55:43,486 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-11 12:55:43,487 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-11 12:55:43,488 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-11 12:55:43,489 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-11 12:55:43,490 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-11 12:55:43,491 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-11 12:55:43,492 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-11 12:55:43,493 [520681597.py:22 in reset_forward] DEBUG - lm_head from test to old.
2023-10-11 12:55:43,493 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-11 12:55:43,495 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-11 12:55:43,496 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-11 12:55:43,497 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-11 12:55:43,498 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-11 12:55:43,499 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-11 12:55:43,500 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-11 12:55:43,501 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-11 12:55:43,502 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-11 12:55:43,503 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-11 12:55:43,504 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-11 12:55:43,505 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-11 12:55:43,506 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-11 12:55:43,507 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-11 12:55:43,507 [189968365.py:53 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-11 12:55:43,508 [189968365.py:53 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-11 12:55:43,625 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:55:43,797 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:55:43,799 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:55:43,803 [189968365.py:24 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])",)
2023-10-11 12:55:43,806 [189968365.py:25 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:55:43,807 [189968365.py:29 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:55:43,809 [189968365.py:29 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:55:43,811 [189968365.py:29 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:55:43,813 [189968365.py:29 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:55:43,815 [189968365.py:43 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])
2023-10-11 12:56:28,529 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:56:28,688 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:56:28,775 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:56:28,843 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 12:56:28,931 [model.py:159 in is_on_disk] INFO - [], ['lm_head.weight']
2023-10-11 12:56:28,933 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/facebook.opt-125m'
2023-10-11 12:56:28,942 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_tokens, [0. 0. 1.], size_todo: 86630400
2023-10-11 12:56:28,943 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.embed_positions, [0. 0. 1.], size_todo: 85056000
2023-10-11 12:56:28,945 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.final_layer_norm, [0.00000000e+00 1.91116887e-05 9.99980888e-01], size_todo: 85054464
2023-10-11 12:56:28,948 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.0, [0.         0.05002193 0.94997807], size_todo: 77966592
2023-10-11 12:56:28,949 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.1, [0.         0.08698539 0.91301461], size_todo: 70878720
2023-10-11 12:56:28,951 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.2, [0.         0.11542163 0.88457837], size_todo: 63790848
2023-10-11 12:56:28,953 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.3, [0.         0.13797624 0.86202376], size_todo: 56702976
2023-10-11 12:56:28,955 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.4, [0.       0.156303 0.843697], size_todo: 49615104
2023-10-11 12:56:28,958 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.5, [0.       0.200013 0.799987], size_todo: 42527232
2023-10-11 12:56:28,959 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.6, [0.         0.21055017 0.78944983], size_todo: 35439360
2023-10-11 12:56:28,961 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.7, [0.         0.24389645 0.75610355], size_todo: 28351488
2023-10-11 12:56:28,964 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.8, [0.         0.25000554 0.74999446], size_todo: 21263616
2023-10-11 12:56:28,966 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.9, [0.         0.27657765 0.72342235], size_todo: 14175744
2023-10-11 12:56:28,968 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.10, [0.         0.27999324 0.72000676], size_todo: 7087872
2023-10-11 12:56:28,970 [model.py:138 in get_policy_weight_map] DEBUG - model.decoder.layers.11, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:56:28,971 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30186053 0.69813947], size_todo: 0
2023-10-11 12:56:28,972 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-11 12:56:28,975 [model.py:148 in get_policy_weight_map] INFO - CausalLM facebook/opt-125m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.07 GiB (30.19%), Disk Mem 0.16 Gib (69.81%)
2023-10-11 12:56:28,979 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-11 12:56:29,008 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_tokens to test forward
2023-10-11 12:56:29,009 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.embed_positions to test forward
2023-10-11 12:56:29,010 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.final_layer_norm to test forward
2023-10-11 12:56:29,010 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.0 to test forward
2023-10-11 12:56:29,011 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.1 to test forward
2023-10-11 12:56:29,013 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.2 to test forward
2023-10-11 12:56:29,014 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.3 to test forward
2023-10-11 12:56:29,015 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.4 to test forward
2023-10-11 12:56:29,016 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.5 to test forward
2023-10-11 12:56:29,017 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.6 to test forward
2023-10-11 12:56:29,018 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.7 to test forward
2023-10-11 12:56:29,019 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.8 to test forward
2023-10-11 12:56:29,020 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.9 to test forward
2023-10-11 12:56:29,021 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.10 to test forward
2023-10-11 12:56:29,023 [520681597.py:42 in to_test_forward] DEBUG - model.decoder.layers.11 to test forward
2023-10-11 12:56:29,024 [520681597.py:42 in to_test_forward] DEBUG - lm_head to test forward
2023-10-11 12:56:29,068 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:56:29,224 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:56:29,226 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_tokens


2023-10-11 12:56:29,228 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:56:29,230 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.embed_positions


2023-10-11 12:56:29,231 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.0 to cpu
2023-10-11 12:56:29,243 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.0


2023-10-11 12:56:29,247 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.1 to cpu
2023-10-11 12:56:29,257 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.1


2023-10-11 12:56:29,260 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.2 to cpu
2023-10-11 12:56:29,270 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.2


2023-10-11 12:56:29,272 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.3 to cpu
2023-10-11 12:56:29,281 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.3


2023-10-11 12:56:29,284 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.4 to cpu
2023-10-11 12:56:29,292 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.4


2023-10-11 12:56:29,294 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.5 to cpu
2023-10-11 12:56:29,302 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.5


2023-10-11 12:56:29,304 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.6 to cpu
2023-10-11 12:56:29,312 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.6


2023-10-11 12:56:29,314 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.7 to cpu
2023-10-11 12:56:29,322 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.7


2023-10-11 12:56:29,324 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.8 to cpu
2023-10-11 12:56:29,332 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.8


2023-10-11 12:56:29,335 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.9 to cpu
2023-10-11 12:56:29,342 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.9


2023-10-11 12:56:29,345 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.10 to cpu
2023-10-11 12:56:29,354 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.10


2023-10-11 12:56:29,357 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.layers.11 to cpu
2023-10-11 12:56:29,365 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.layers.11


2023-10-11 12:56:29,367 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.final_layer_norm to cpu
2023-10-11 12:56:29,369 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: model.decoder.final_layer_norm


2023-10-11 12:56:29,371 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 12:56:29,386 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 12:56:29,393 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-11 12:56:29,394 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 12:56:29,405 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_tokens from test to old.
2023-10-11 12:56:29,405 [520681597.py:22 in reset_forward] DEBUG - model.decoder.embed_positions from test to old.
2023-10-11 12:56:29,407 [520681597.py:22 in reset_forward] DEBUG - model.decoder.final_layer_norm from test to old.
2023-10-11 12:56:29,408 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.0 from test to old.
2023-10-11 12:56:29,409 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.1 from test to old.
2023-10-11 12:56:29,410 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.2 from test to old.
2023-10-11 12:56:29,411 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.3 from test to old.
2023-10-11 12:56:29,412 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.4 from test to old.
2023-10-11 12:56:29,413 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.5 from test to old.
2023-10-11 12:56:29,414 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.6 from test to old.
2023-10-11 12:56:29,414 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.7 from test to old.
2023-10-11 12:56:29,415 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.8 from test to old.
2023-10-11 12:56:29,416 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.9 from test to old.
2023-10-11 12:56:29,417 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.10 from test to old.
2023-10-11 12:56:29,418 [520681597.py:22 in reset_forward] DEBUG - model.decoder.layers.11 from test to old.
2023-10-11 12:56:29,418 [520681597.py:22 in reset_forward] DEBUG - lm_head from test to old.
2023-10-11 12:56:29,419 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.embed_tokens to flexgen forward
2023-10-11 12:56:29,420 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.embed_positions to flexgen forward
2023-10-11 12:56:29,421 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.0 to flexgen forward
2023-10-11 12:56:29,422 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.1 to flexgen forward
2023-10-11 12:56:29,423 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.2 to flexgen forward
2023-10-11 12:56:29,424 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.3 to flexgen forward
2023-10-11 12:56:29,425 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.4 to flexgen forward
2023-10-11 12:56:29,427 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.5 to flexgen forward
2023-10-11 12:56:29,428 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.6 to flexgen forward
2023-10-11 12:56:29,428 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.7 to flexgen forward
2023-10-11 12:56:29,430 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.8 to flexgen forward
2023-10-11 12:56:29,431 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.9 to flexgen forward
2023-10-11 12:56:29,432 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.10 to flexgen forward
2023-10-11 12:56:29,433 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.layers.11 to flexgen forward
2023-10-11 12:56:29,434 [4253403418.py:53 in to_flexgen_forward] DEBUG - model.decoder.final_layer_norm to flexgen forward
2023-10-11 12:56:29,435 [4253403418.py:53 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-11 12:56:29,472 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /facebook/opt-125m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 12:56:29,618 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_tokens to cpu
2023-10-11 12:56:29,620 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: model.decoder.embed_positions to cpu
2023-10-11 12:56:29,623 [4253403418.py:24 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])",)
2023-10-11 12:56:29,624 [4253403418.py:25 in new_forward] DEBUG - kwargs: {}
2023-10-11 12:56:29,625 [4253403418.py:29 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 0
2023-10-11 12:56:29,628 [4253403418.py:29 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 1
2023-10-11 12:56:29,631 [4253403418.py:29 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 2
2023-10-11 12:56:29,632 [4253403418.py:29 in new_forward] DEBUG - layer: model.decoder.embed_tokens, batch: 3
2023-10-11 12:56:29,635 [4253403418.py:43 in new_forward] DEBUG - outputs after concat: <class '__main__.BatchMixTensor'>: torch.Size([8, 9, 384])
