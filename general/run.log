2023-10-11 15:36:31,530 [instantiator.py:21 in <module>] INFO - Created a temporary directory at /tmp/tmpscaa5xq0
2023-10-11 15:36:31,530 [instantiator.py:76 in _write] INFO - Writing /tmp/tmpscaa5xq0/_remote_module_non_scriptable.py
2023-10-11 15:36:31,971 [connectionpool.py:1003 in _new_conn] DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2023-10-11 15:36:32,071 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 15:36:33,453 [tpu_cluster_resolver.py:32 in <module>] DEBUG - Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
2023-10-11 15:36:33,768 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-11 15:36:33,768 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-11 15:36:33,769 [__init__.py:47 in <module>] DEBUG - Creating converter from 7 to 5
2023-10-11 15:36:33,769 [__init__.py:47 in <module>] DEBUG - Creating converter from 5 to 7
2023-10-11 15:36:34,519 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 15:36:34,625 [model.py:159 in is_on_disk] INFO - [], []
2023-10-11 15:36:34,663 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/config.json HTTP/1.1" 200 0
2023-10-11 15:36:34,772 [model.py:159 in is_on_disk] INFO - [], []
2023-10-11 15:36:34,772 [model.py:182 in download] INFO - The whole model has been downloaded an processed to offload_folder: 'offload_dir/bigscience.bloom-560m'
2023-10-11 15:36:34,782 [model.py:138 in get_policy_weight_map] DEBUG - transformer.word_embeddings, [0. 0. 1.], size_todo: 302313472
2023-10-11 15:36:34,782 [model.py:138 in get_policy_weight_map] DEBUG - transformer.word_embeddings_layernorm, [0.00000000e+00 3.98593761e-06 9.99996014e-01], size_todo: 302311424
2023-10-11 15:36:34,783 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.0, [0.         0.03116083 0.96883917], size_todo: 289715200
2023-10-11 15:36:34,784 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.1, [0.         0.05953522 0.94046478], size_todo: 277118976
2023-10-11 15:36:34,785 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.2, [0.         0.08548396 0.91451604], size_todo: 264522752
2023-10-11 15:36:34,786 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.3, [0.         0.10930533 0.89069467], size_todo: 251926528
2023-10-11 15:36:34,786 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.4, [0.         0.13125066 0.86874934], size_todo: 239330304
2023-10-11 15:36:34,787 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.5, [0.         0.15153316 0.84846684], size_todo: 226734080
2023-10-11 15:36:34,788 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.6, [0.         0.17033494 0.82966506], size_todo: 214137856
2023-10-11 15:36:34,789 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.7, [0.         0.18781242 0.81218758], size_todo: 201541632
2023-10-11 15:36:34,790 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.8, [0.         0.19277305 0.80722695], size_todo: 188945408
2023-10-11 15:36:34,791 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.9, [0.         0.20836231 0.79163769], size_todo: 176349184
2023-10-11 15:36:34,791 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.10, [0.         0.21235237 0.78764763], size_todo: 163752960
2023-10-11 15:36:34,792 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.11, [0.        0.2263748 0.7736252], size_todo: 151156736
2023-10-11 15:36:34,793 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.12, [0.         0.22958653 0.77041347], size_todo: 138560512
2023-10-11 15:36:34,794 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.13, [0.         0.24229253 0.75770747], size_todo: 125964288
2023-10-11 15:36:34,795 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.14, [0.         0.24487307 0.75512693], size_todo: 113368064
2023-10-11 15:36:34,796 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.15, [0.         0.25646083 0.74353917], size_todo: 100771840
2023-10-11 15:36:34,796 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.16, [0.         0.25852448 0.74147552], size_todo: 88175616
2023-10-11 15:36:34,797 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.17, [0.         0.26915308 0.73084692], size_todo: 75579392
2023-10-11 15:36:34,798 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.18, [0.         0.27078978 0.72921022], size_todo: 62983168
2023-10-11 15:36:34,799 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.19, [0.         0.28058853 0.71941147], size_todo: 50386944
2023-10-11 15:36:34,800 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.20, [0.        0.2818699 0.7181301], size_todo: 37790720
2023-10-11 15:36:34,801 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.21, [0.         0.29094504 0.70905496], size_todo: 25194496
2023-10-11 15:36:34,801 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.22, [0.        0.2919287 0.7080713], size_todo: 12598272
2023-10-11 15:36:34,802 [model.py:138 in get_policy_weight_map] DEBUG - transformer.h.23, [0.         0.30036843 0.69963157], size_todo: 2048
2023-10-11 15:36:34,803 [model.py:138 in get_policy_weight_map] DEBUG - transformer.ln_f, [0.         0.30036733 0.69963267], size_todo: 0
2023-10-11 15:36:34,803 [model.py:138 in get_policy_weight_map] DEBUG - lm_head, [0.         0.30036733 0.69963267], size_todo: 0
2023-10-11 15:36:34,803 [model.py:142 in get_policy_weight_map] INFO - device_map is prepared!
2023-10-11 15:36:34,806 [model.py:148 in get_policy_weight_map] INFO - CausalLM bigscience/bloom-560m is to be loaded on: 
GPU Mem 0.00 GiB (0.00%), CPU Mem 0.31 GiB (30.04%), Disk Mem 0.73 Gib (69.96%)
2023-10-11 15:36:34,808 [model.py:241 in init_all_weights] DEBUG - init all weights...
2023-10-11 15:36:34,845 [forward.py:48 in to_test_forward] DEBUG - transformer.word_embeddings to test forward
2023-10-11 15:36:34,845 [forward.py:48 in to_test_forward] DEBUG - transformer.word_embeddings_layernorm to test forward
2023-10-11 15:36:34,845 [forward.py:48 in to_test_forward] DEBUG - transformer.h.0 to test forward
2023-10-11 15:36:34,846 [forward.py:48 in to_test_forward] DEBUG - transformer.h.1 to test forward
2023-10-11 15:36:34,846 [forward.py:48 in to_test_forward] DEBUG - transformer.h.2 to test forward
2023-10-11 15:36:34,846 [forward.py:48 in to_test_forward] DEBUG - transformer.h.3 to test forward
2023-10-11 15:36:34,846 [forward.py:48 in to_test_forward] DEBUG - transformer.h.4 to test forward
2023-10-11 15:36:34,846 [forward.py:48 in to_test_forward] DEBUG - transformer.h.5 to test forward
2023-10-11 15:36:34,846 [forward.py:48 in to_test_forward] DEBUG - transformer.h.6 to test forward
2023-10-11 15:36:34,846 [forward.py:48 in to_test_forward] DEBUG - transformer.h.7 to test forward
2023-10-11 15:36:34,846 [forward.py:48 in to_test_forward] DEBUG - transformer.h.8 to test forward
2023-10-11 15:36:34,847 [forward.py:48 in to_test_forward] DEBUG - transformer.h.9 to test forward
2023-10-11 15:36:34,847 [forward.py:48 in to_test_forward] DEBUG - transformer.h.10 to test forward
2023-10-11 15:36:34,847 [forward.py:48 in to_test_forward] DEBUG - transformer.h.11 to test forward
2023-10-11 15:36:34,847 [forward.py:48 in to_test_forward] DEBUG - transformer.h.12 to test forward
2023-10-11 15:36:34,847 [forward.py:48 in to_test_forward] DEBUG - transformer.h.13 to test forward
2023-10-11 15:36:34,847 [forward.py:48 in to_test_forward] DEBUG - transformer.h.14 to test forward
2023-10-11 15:36:34,847 [forward.py:48 in to_test_forward] DEBUG - transformer.h.15 to test forward
2023-10-11 15:36:34,847 [forward.py:48 in to_test_forward] DEBUG - transformer.h.16 to test forward
2023-10-11 15:36:34,847 [forward.py:48 in to_test_forward] DEBUG - transformer.h.17 to test forward
2023-10-11 15:36:34,848 [forward.py:48 in to_test_forward] DEBUG - transformer.h.18 to test forward
2023-10-11 15:36:34,848 [forward.py:48 in to_test_forward] DEBUG - transformer.h.19 to test forward
2023-10-11 15:36:34,848 [forward.py:48 in to_test_forward] DEBUG - transformer.h.20 to test forward
2023-10-11 15:36:34,848 [forward.py:48 in to_test_forward] DEBUG - transformer.h.21 to test forward
2023-10-11 15:36:34,848 [forward.py:48 in to_test_forward] DEBUG - transformer.h.22 to test forward
2023-10-11 15:36:34,848 [forward.py:48 in to_test_forward] DEBUG - transformer.h.23 to test forward
2023-10-11 15:36:34,848 [forward.py:48 in to_test_forward] DEBUG - transformer.ln_f to test forward
2023-10-11 15:36:34,848 [forward.py:48 in to_test_forward] DEBUG - lm_head to test forward
2023-10-11 15:36:34,887 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 15:36:35,474 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-11 15:36:35,477 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-11 15:36:35,478 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-11 15:36:35,479 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-11 15:36:35,481 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-11 15:36:35,564 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-11 15:36:35,566 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-11 15:36:35,634 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-11 15:36:35,636 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-11 15:36:35,716 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-11 15:36:35,717 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-11 15:36:35,788 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-11 15:36:35,789 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-11 15:36:35,868 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-11 15:36:35,870 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-11 15:36:35,947 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-11 15:36:35,949 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-11 15:36:36,025 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-11 15:36:36,027 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-11 15:36:36,115 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-11 15:36:36,117 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-11 15:36:36,194 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-11 15:36:36,196 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-11 15:36:36,282 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-11 15:36:36,284 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-11 15:36:36,379 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-11 15:36:36,381 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-11 15:36:36,449 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-11 15:36:36,451 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-11 15:36:36,529 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-11 15:36:36,532 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-11 15:36:36,620 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-11 15:36:36,622 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-11 15:36:36,693 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-11 15:36:36,696 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-11 15:36:36,768 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-11 15:36:36,769 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-11 15:36:36,856 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-11 15:36:36,859 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-11 15:36:36,978 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-11 15:36:36,979 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-11 15:36:37,058 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-11 15:36:37,060 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-11 15:36:37,155 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-11 15:36:37,156 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-11 15:36:37,226 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-11 15:36:37,228 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-11 15:36:37,293 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-11 15:36:37,295 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-11 15:36:37,363 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-11 15:36:37,365 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-11 15:36:37,431 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-11 15:36:37,433 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-11 15:36:37,434 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-11 15:36:37,435 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:36:38,382 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


2023-10-11 15:36:38,407 [test.py:40 in test_hf_gen] INFO - 0.
2023-10-11 15:36:38,407 [test.py:41 in test_hf_gen] INFO - ----------
2023-10-11 15:36:38,490 [forward.py:28 in reset_forward] DEBUG - transformer.word_embeddings from test to old.
2023-10-11 15:36:38,490 [forward.py:28 in reset_forward] DEBUG - transformer.word_embeddings_layernorm from test to old.
2023-10-11 15:36:38,490 [forward.py:28 in reset_forward] DEBUG - transformer.h.0 from test to old.
2023-10-11 15:36:38,490 [forward.py:28 in reset_forward] DEBUG - transformer.h.1 from test to old.
2023-10-11 15:36:38,490 [forward.py:28 in reset_forward] DEBUG - transformer.h.2 from test to old.
2023-10-11 15:36:38,491 [forward.py:28 in reset_forward] DEBUG - transformer.h.3 from test to old.
2023-10-11 15:36:38,491 [forward.py:28 in reset_forward] DEBUG - transformer.h.4 from test to old.
2023-10-11 15:36:38,491 [forward.py:28 in reset_forward] DEBUG - transformer.h.5 from test to old.
2023-10-11 15:36:38,491 [forward.py:28 in reset_forward] DEBUG - transformer.h.6 from test to old.
2023-10-11 15:36:38,491 [forward.py:28 in reset_forward] DEBUG - transformer.h.7 from test to old.
2023-10-11 15:36:38,491 [forward.py:28 in reset_forward] DEBUG - transformer.h.8 from test to old.
2023-10-11 15:36:38,491 [forward.py:28 in reset_forward] DEBUG - transformer.h.9 from test to old.
2023-10-11 15:36:38,491 [forward.py:28 in reset_forward] DEBUG - transformer.h.10 from test to old.
2023-10-11 15:36:38,491 [forward.py:28 in reset_forward] DEBUG - transformer.h.11 from test to old.
2023-10-11 15:36:38,492 [forward.py:28 in reset_forward] DEBUG - transformer.h.12 from test to old.
2023-10-11 15:36:38,492 [forward.py:28 in reset_forward] DEBUG - transformer.h.13 from test to old.
2023-10-11 15:36:38,492 [forward.py:28 in reset_forward] DEBUG - transformer.h.14 from test to old.
2023-10-11 15:36:38,492 [forward.py:28 in reset_forward] DEBUG - transformer.h.15 from test to old.
2023-10-11 15:36:38,492 [forward.py:28 in reset_forward] DEBUG - transformer.h.16 from test to old.
2023-10-11 15:36:38,492 [forward.py:28 in reset_forward] DEBUG - transformer.h.17 from test to old.
2023-10-11 15:36:38,492 [forward.py:28 in reset_forward] DEBUG - transformer.h.18 from test to old.
2023-10-11 15:36:38,492 [forward.py:28 in reset_forward] DEBUG - transformer.h.19 from test to old.
2023-10-11 15:36:38,492 [forward.py:28 in reset_forward] DEBUG - transformer.h.20 from test to old.
2023-10-11 15:36:38,492 [forward.py:28 in reset_forward] DEBUG - transformer.h.21 from test to old.
2023-10-11 15:36:38,493 [forward.py:28 in reset_forward] DEBUG - transformer.h.22 from test to old.
2023-10-11 15:36:38,493 [forward.py:28 in reset_forward] DEBUG - transformer.h.23 from test to old.
2023-10-11 15:36:38,493 [forward.py:28 in reset_forward] DEBUG - transformer.ln_f from test to old.
2023-10-11 15:36:38,493 [forward.py:28 in reset_forward] DEBUG - lm_head from test to old.
2023-10-11 15:36:38,493 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.word_embeddings to flexgen forward
2023-10-11 15:36:38,493 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.word_embeddings_layernorm to flexgen forward
2023-10-11 15:36:38,493 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.0 to flexgen forward
2023-10-11 15:36:38,493 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.1 to flexgen forward
2023-10-11 15:36:38,494 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.2 to flexgen forward
2023-10-11 15:36:38,494 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.3 to flexgen forward
2023-10-11 15:36:38,494 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.4 to flexgen forward
2023-10-11 15:36:38,494 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.5 to flexgen forward
2023-10-11 15:36:38,494 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.6 to flexgen forward
2023-10-11 15:36:38,494 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.7 to flexgen forward
2023-10-11 15:36:38,494 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.8 to flexgen forward
2023-10-11 15:36:38,494 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.9 to flexgen forward
2023-10-11 15:36:38,494 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.10 to flexgen forward
2023-10-11 15:36:38,495 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.11 to flexgen forward
2023-10-11 15:36:38,495 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.12 to flexgen forward
2023-10-11 15:36:38,495 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.13 to flexgen forward
2023-10-11 15:36:38,495 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.14 to flexgen forward
2023-10-11 15:36:38,495 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.15 to flexgen forward
2023-10-11 15:36:38,495 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.16 to flexgen forward
2023-10-11 15:36:38,495 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.17 to flexgen forward
2023-10-11 15:36:38,495 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.18 to flexgen forward
2023-10-11 15:36:38,496 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.19 to flexgen forward
2023-10-11 15:36:38,496 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.20 to flexgen forward
2023-10-11 15:36:38,496 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.21 to flexgen forward
2023-10-11 15:36:38,496 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.22 to flexgen forward
2023-10-11 15:36:38,496 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.h.23 to flexgen forward
2023-10-11 15:36:38,496 [forward.py:120 in to_flexgen_forward] DEBUG - transformer.ln_f to flexgen forward
2023-10-11 15:36:38,496 [forward.py:120 in to_flexgen_forward] DEBUG - lm_head to flexgen forward
2023-10-11 15:36:38,536 [connectionpool.py:456 in _make_request] DEBUG - https://huggingface.co:443 "HEAD /bigscience/bloom-560m/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
2023-10-11 15:36:39,059 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-11 15:36:39,059 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-11 15:36:39,060 [forward.py:84 in new_forward] DEBUG - args: ("<class 'torch.Tensor'>: torch.Size([8, 9])",)
2023-10-11 15:36:39,060 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:36:39,061 [forward.py:104 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])
2023-10-11 15:36:39,062 [forward.py:104 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])
2023-10-11 15:36:39,062 [forward.py:104 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])
2023-10-11 15:36:39,063 [forward.py:104 in new_forward] DEBUG - layer: transformer.word_embeddings, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])
2023-10-11 15:36:39,063 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])
2023-10-11 15:36:39,063 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings


2023-10-11 15:36:39,064 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings_layernorm to cpu
2023-10-11 15:36:39,064 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-11 15:36:39,068 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,069 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:36:39,071 [forward.py:104 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])
2023-10-11 15:36:39,072 [forward.py:104 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])
2023-10-11 15:36:39,073 [forward.py:104 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])
2023-10-11 15:36:39,073 [forward.py:104 in new_forward] DEBUG - layer: transformer.word_embeddings_layernorm, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])
2023-10-11 15:36:39,074 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])
2023-10-11 15:36:39,074 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.word_embeddings_layernorm


2023-10-11 15:36:39,075 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.0 to cpu
2023-10-11 15:36:39,077 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-11 15:36:39,081 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,082 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,093 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,101 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,109 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,117 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.0, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,117 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,118 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.0


2023-10-11 15:36:39,120 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.1 to cpu
2023-10-11 15:36:39,122 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-11 15:36:39,126 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,126 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,134 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,142 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,149 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,157 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.1, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,157 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,157 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.1


2023-10-11 15:36:39,159 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.2 to cpu
2023-10-11 15:36:39,162 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-11 15:36:39,166 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,166 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,176 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,184 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,191 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,199 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.2, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,199 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,199 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.2


2023-10-11 15:36:39,201 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.3 to cpu
2023-10-11 15:36:39,204 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-11 15:36:39,207 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,207 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,216 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,223 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,231 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,237 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.3, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,237 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,237 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.3


2023-10-11 15:36:39,239 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.4 to cpu
2023-10-11 15:36:39,242 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-11 15:36:39,245 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,246 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,252 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,258 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,264 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,270 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.4, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,271 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,271 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.4


2023-10-11 15:36:39,273 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.5 to cpu
2023-10-11 15:36:39,276 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-11 15:36:39,279 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,280 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,288 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,294 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,300 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,305 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.5, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,306 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,306 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.5


2023-10-11 15:36:39,308 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.6 to cpu
2023-10-11 15:36:39,311 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-11 15:36:39,314 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,314 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,320 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,329 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,335 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,341 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.6, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,341 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,341 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.6


2023-10-11 15:36:39,343 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.7 to cpu
2023-10-11 15:36:39,346 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-11 15:36:39,349 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,349 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,356 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,362 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,368 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,374 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.7, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,375 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,375 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.7


2023-10-11 15:36:39,377 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.8 to cpu
2023-10-11 15:36:39,379 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-11 15:36:39,383 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,383 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,390 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,396 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,403 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,410 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.8, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,410 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,410 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.8


2023-10-11 15:36:39,413 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.9 to cpu
2023-10-11 15:36:39,416 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-11 15:36:39,419 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,419 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,426 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,432 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,441 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,447 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.9, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,447 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,447 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.9


2023-10-11 15:36:39,449 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.10 to cpu
2023-10-11 15:36:39,452 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-11 15:36:39,456 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,456 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,463 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,468 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,474 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,481 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.10, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,481 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,481 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.10


2023-10-11 15:36:39,484 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.11 to cpu
2023-10-11 15:36:39,487 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-11 15:36:39,490 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,490 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,498 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,504 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,509 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,515 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.11, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,516 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,516 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.11


2023-10-11 15:36:39,518 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.12 to cpu
2023-10-11 15:36:39,521 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-11 15:36:39,524 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,524 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,532 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,539 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,544 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,552 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.12, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,553 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,553 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.12


2023-10-11 15:36:39,556 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.13 to cpu
2023-10-11 15:36:39,559 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-11 15:36:39,562 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,562 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,569 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,575 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,580 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,587 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.13, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,588 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,588 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.13


2023-10-11 15:36:39,590 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.14 to cpu
2023-10-11 15:36:39,592 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-11 15:36:39,596 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,596 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,602 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,610 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,616 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,621 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.14, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,622 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,622 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.14


2023-10-11 15:36:39,625 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.15 to cpu
2023-10-11 15:36:39,628 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-11 15:36:39,631 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,631 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,638 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,644 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,650 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,656 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.15, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,656 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,657 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.15


2023-10-11 15:36:39,659 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.16 to cpu
2023-10-11 15:36:39,661 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-11 15:36:39,665 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,665 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,673 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,679 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,685 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,691 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.16, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,691 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,691 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.16


2023-10-11 15:36:39,694 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.17 to cpu
2023-10-11 15:36:39,697 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-11 15:36:39,700 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,700 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,707 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,713 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,719 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,726 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.17, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,726 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,727 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.17


2023-10-11 15:36:39,729 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.18 to cpu
2023-10-11 15:36:39,731 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-11 15:36:39,735 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,735 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,756 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,762 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,768 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,774 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.18, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,774 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,774 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.18


2023-10-11 15:36:39,777 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.19 to cpu
2023-10-11 15:36:39,780 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-11 15:36:39,783 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,783 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,790 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,796 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,802 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,807 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.19, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,808 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,808 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.19


2023-10-11 15:36:39,810 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.20 to cpu
2023-10-11 15:36:39,813 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-11 15:36:39,816 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,816 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,822 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.20, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,829 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.20, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,835 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.20, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,840 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.20, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,841 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,841 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.20


2023-10-11 15:36:39,844 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.21 to cpu
2023-10-11 15:36:39,847 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-11 15:36:39,850 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,850 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,858 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.21, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,864 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.21, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,872 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.21, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,900 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.21, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,900 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,900 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.21


2023-10-11 15:36:39,902 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.22 to cpu
2023-10-11 15:36:39,905 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-11 15:36:39,908 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,908 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,915 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.22, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,921 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.22, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,927 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.22, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,934 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.22, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,935 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,935 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.22


2023-10-11 15:36:39,938 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.h.23 to cpu
2023-10-11 15:36:39,941 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-11 15:36:39,941 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,942 [forward.py:85 in new_forward] DEBUG - kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([8, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([128, 1, 9])"}
2023-10-11 15:36:39,949 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.23, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,956 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.23, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,962 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.23, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,968 [forward.py:104 in new_forward] DEBUG - layer: transformer.h.23, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {'layer_past': "<class 'NoneType'>: None", 'attention_mask': "<class 'torch.Tensor'>: torch.Size([2, 1, 9, 9])", 'head_mask': "<class 'NoneType'>: None", 'use_cache': "<class 'bool'>: True", 'output_attentions': "<class 'bool'>: False", 'alibi': "<class 'torch.Tensor'>: torch.Size([32, 1, 9])"}, output: ("<class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])", ("<class 'mixtensor.MixTensor'>: torch.Size([32, 64, 9])", "<class 'mixtensor.MixTensor'>: torch.Size([32, 9, 64])"))
2023-10-11 15:36:39,968 [forward.py:113 in new_forward] DEBUG - outputs after concat: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])", ("<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 64, 9])", "<class 'mixtensor.BatchMixTensor'>: torch.Size([128, 9, 64])"))
2023-10-11 15:36:39,969 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.h.23


2023-10-11 15:36:39,971 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.ln_f to cpu
2023-10-11 15:36:39,971 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:36:39,972 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,972 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:36:39,973 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])
2023-10-11 15:36:39,974 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])
2023-10-11 15:36:39,975 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])
2023-10-11 15:36:39,976 [forward.py:104 in new_forward] DEBUG - layer: transformer.ln_f, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 1024])
2023-10-11 15:36:39,976 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])
2023-10-11 15:36:39,976 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: transformer.ln_f


2023-10-11 15:36:39,977 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: lm_head to cpu
2023-10-11 15:36:39,977 [model.py:251 in load_layer_weights] DEBUG - load_layer_weights: transformer.word_embeddings to cpu
2023-10-11 15:36:39,978 [forward.py:84 in new_forward] DEBUG - args: ("<class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 1024])",)
2023-10-11 15:36:39,978 [forward.py:85 in new_forward] DEBUG - kwargs: {}
2023-10-11 15:36:40,050 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 0, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 250880])
2023-10-11 15:36:40,109 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 1, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 250880])
2023-10-11 15:36:40,174 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 2, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 250880])
2023-10-11 15:36:40,236 [forward.py:104 in new_forward] DEBUG - layer: lm_head, batch: 3, args: ("<class 'torch.Tensor'>: torch.Size([2, 9, 1024])",), kwargs: {}, output: <class 'mixtensor.MixTensor'>: torch.Size([2, 9, 250880])
2023-10-11 15:36:40,236 [forward.py:113 in new_forward] DEBUG - outputs after concat: <class 'mixtensor.BatchMixTensor'>: torch.Size([8, 9, 250880])
2023-10-11 15:36:40,236 [model.py:261 in offload_layer_weights] DEBUG - offload_layer_weights: lm_head


